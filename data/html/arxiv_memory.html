
Line wrapï¿¼
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.2.1/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.2.1/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.2.1/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.2.1/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.2.1/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.2.1/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.2.1/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.2.1/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.2.1/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

    <!-- Pendo -->
    <script>
     (function(apiKey){
         (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=[];
             v=['initialize','identify','updateOptions','pageLoad'];for(w=0,x=v.length;w<x;++w)(function(m){
                 o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
             y=e.createElement(n);y.async=!0;y.src='https://content.analytics.arxiv.org/agent/static/'+apiKey+'/pendo.js';
             z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

         // Call this whenever information about your visitors becomes available
         // Please use Strings, Numbers, or Bools for value types.
         pendo.initialize({
             visitor: {
                 id:              'VISITOR-UNIQUE-ID'   // Required if user is logged in
                 // email:        // Recommended if using Pendo Feedback, or NPS Email
                 // full_name:    // Recommended if using Pendo Feedback
                 // role:         // Optional

                 // You can add any additional visitor level key-values here,
                 // as long as it's not one of the above reserved names.
             },

             account: {
                 id:           'ACCOUNT-UNIQUE-ID' // Highly recommended
                 // name:         // Optional
                 // is_paying:    // Recommended if using Pendo Feedback
                 // monthly_value:// Recommended if using Pendo Feedback
                 // planLevel:    // Optional
                 // planPrice:    // Optional
                 // creationDate: // Optional

                 // You can add any additional account level key-values here,
                 // as long as it's not one of the above reserved names.
             }
         });
     })('d6494389-b427-4103-7c76-03182ecc8e60');
    </script>
    <!-- End Pendo -->


  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.2.1/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo"><img src="https://static.arxiv.org/static/base/0.17.2.1/images/arxiv-logo-web.svg" alt="arXiv" aria-label="logo" width="85" /></a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;200 of 273 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28memory%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first">order: -announced_date_first; size: 200; classification: Computer Science (cs); include_cross_list: True; terms: AND all=(code OR program OR software OR application) AND (optimize OR optimizing OR optimization OR improve OR improving OR improvement OR automated OR automatically OR reduce OR reducing) AND (memory)</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28memory%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=200">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="(code OR program OR software OR application) AND (optimize OR optimizing OR optimization OR improve OR improving OR improvement OR automated OR automatically OR reduce OR reducing) AND (memory)"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option value="50">50</option><option value="100">100</option><option selected value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28memory%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=200"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28memory%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28memory%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=200"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.01093">arXiv:2107.01093</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.01093">pdf</a>, <a href="https://arxiv.org/format/2107.01093">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Model Checking C++ <span class="search-hit mathjax">Programs</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Monteiro%2C+F+R">Felipe R. Monteiro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gadelha%2C+M+R">Mikhail R. Gadelha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cordeiro%2C+L+C">Lucas C. Cordeiro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.01093v1-abstract-short" style="display: inline;">
        In the last three decades, <span class="search-hit mathjax">memory</span> safety issues in system&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.01093v1-abstract-full').style.display = 'inline'; document.getElementById('2107.01093v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.01093v1-abstract-full" style="display: none;">
        In the last three decades, <span class="search-hit mathjax">memory</span> safety issues in system <span class="search-hit mathjax">programming</span> languages such as C or C++ have been one of the significant sources of security vulnerabilities. However, there exist only a few attempts with limited success to cope with the complexity of C++ <span class="search-hit mathjax">program</span> verification. Here we describe and evaluate a novel verification approach based on bounded model checking (BMC) and satisfiability modulo theories (SMT) to verify C++ <span class="search-hit mathjax">programs</span> formally. Our verification approach analyzes bounded C++ <span class="search-hit mathjax">programs</span> by encoding into SMT various sophisticated features that the C++ <span class="search-hit mathjax">programming</span> language offers, such as templates, inheritance, polymorphism, exception handling, and the Standard C++ Libraries. We formalize these features within our formal verification framework using a decidable fragment of first-order logic and then show how state-of-the-art SMT solvers can efficiently handle that. We implemented our verification approach on top of ESBMC. We compare ESBMC to LLBMC and DIVINE, which are state-of-the-art verifiers to check C++ <span class="search-hit mathjax">programs</span> directly from the LLVM bitcode. Experimental results show that ESBMC can handle a wide range of C++ <span class="search-hit mathjax">programs</span>, presenting a higher number of correct verification results. At the same time, it <span class="search-hit mathjax">reduces</span> the verification time if compared to LLBMC and DIVINE tools. Additionally, ESBMC has been applied to a commercial C++ <span class="search-hit mathjax">application</span> in the telecommunication domain and successfully detected arithmetic overflow errors, potentially leading to security vulnerabilities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.01093v1-abstract-full').style.display = 'none'; document.getElementById('2107.01093v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">30 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.14830">arXiv:2106.14830</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.14830">pdf</a>, <a href="https://arxiv.org/format/2106.14830">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        THUE: Discovering Top-K High Utility Episodes
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+S">Shicheng Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jiahui Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gan%2C+W">Wensheng Gan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+G">Guoting Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goyal%2C+V">Vikram Goyal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.14830v1-abstract-short" style="display: inline;">
        Episode discovery from an event is a popular framework for data mining tasks and has many real-world <span class="search-hit mathjax">applications</span>. An episode is a partially ordered set of objects (e.g., item, node), and each object is associated with an event type. This episode can also be considered as a complex event sub-sequence. High-utility episode mining is an interesting utility-dri&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14830v1-abstract-full').style.display = 'inline'; document.getElementById('2106.14830v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.14830v1-abstract-full" style="display: none;">
        Episode discovery from an event is a popular framework for data mining tasks and has many real-world <span class="search-hit mathjax">applications</span>. An episode is a partially ordered set of objects (e.g., item, node), and each object is associated with an event type. This episode can also be considered as a complex event sub-sequence. High-utility episode mining is an interesting utility-driven mining task in the real world. Traditional episode mining algorithms, by setting a threshold, usually return a huge episode that is neither intuitive nor saves time. In general, finding a suitable threshold in a pattern-mining algorithm is a trivial and time-consuming task. In this paper, we propose a novel algorithm, called Top-K High Utility Episode (THUE) mining within the complex event sequence, which redefines the previous mining task by obtaining the K highest episodes. We introduce several threshold-raising strategies and <span class="search-hit mathjax">optimize</span> the episode-weighted utilization upper bounds to speed up the mining process and effectively <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">memory</span> cost. Finally, the experimental results on both real-life and synthetic datasets reveal that the THUE algorithm can offer six to eight orders of magnitude running time performance <span class="search-hit mathjax">improvement</span> over the state-of-the-art algorithm and has low <span class="search-hit mathjax">memory</span> consumption.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14830v1-abstract-full').style.display = 'none'; document.getElementById('2106.14830v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint. 6 figures, 9 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.14577">arXiv:2106.14577</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.14577">pdf</a>, <a href="https://arxiv.org/format/2106.14577">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Privacy-Preserving Image Acquisition Using Trainable Optical Kernel
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sepehri%2C+Y">Yamin Sepehri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pad%2C+P">Pedram Pad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frossard%2C+P">Pascal Frossard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dunbar%2C+L+A">L. Andrea Dunbar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.14577v1-abstract-short" style="display: inline;">
        &hellip;that are all vulnerable to direct access attack. Also, in contrast with the previous optical privacy-preserving methods that cannot be trained, our method is data-driven and <span class="search-hit mathjax">optimized</span> for the specific&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14577v1-abstract-full').style.display = 'inline'; document.getElementById('2106.14577v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.14577v1-abstract-full" style="display: none;">
        Preserving privacy is a growing concern in our society where sensors and cameras are ubiquitous. In this work, for the first time, we propose a trainable image acquisition method that removes the sensitive identity revealing information in the optical domain before it reaches the image sensor. The method benefits from a trainable optical convolution kernel which transmits the desired information while filters out the sensitive content. As the sensitive content is suppressed before it reaches the image sensor, it does not enter the digital domain therefore is unretrievable by any sort of privacy attack. This is in contrast with the current digital privacy-preserving methods that are all vulnerable to direct access attack. Also, in contrast with the previous optical privacy-preserving methods that cannot be trained, our method is data-driven and <span class="search-hit mathjax">optimized</span> for the specific <span class="search-hit mathjax">application</span> at hand. Moreover, there is no additional computation, <span class="search-hit mathjax">memory</span>, or power burden on the acquisition system since this processing happens passively in the optical domain and can even be used together and on top of the fully digital privacy-preserving systems. The proposed approach is adaptable to different digital neural networks and content. We demonstrate it for several scenarios such as smile detection as the desired attribute while the gender is filtered out as the sensitive content. We trained the optical kernel in conjunction with two adversarial neural networks where the analysis network tries to detect the desired attribute and the adversarial network tries to detect the sensitive content. We show that this method can <span class="search-hit mathjax">reduce</span> 65.1% of sensitive content when it is selected to be the gender and it only loses 7.3% of the desired content. Moreover, we reconstruct the original faces using the deep reconstruction method that confirms the ineffectiveness of reconstruction attacks to obtain the sensitive content.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14577v1-abstract-full').style.display = 'none'; document.getElementById('2106.14577v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 9 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.10; I.5.0
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.14156">arXiv:2106.14156</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.14156">pdf</a>, <a href="https://arxiv.org/format/2106.14156">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Post-Training Quantization for Vision Transformer
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhenhua Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yunhe Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+K">Kai Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+S">Siwei Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+W">Wen Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.14156v1-abstract-short" style="display: inline;">
        Recently, transformer has achieved remarkable performance on a variety of computer vision <span class="search-hit mathjax">applications</span>. Compared with mainstream convolutional neural networks, vision transformers are often of sophisticated architectures for extracting powerful feature representations, which are more difficult to be developed on mobile devices. In this paper, we present an e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14156v1-abstract-full').style.display = 'inline'; document.getElementById('2106.14156v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.14156v1-abstract-full" style="display: none;">
        Recently, transformer has achieved remarkable performance on a variety of computer vision <span class="search-hit mathjax">applications</span>. Compared with mainstream convolutional neural networks, vision transformers are often of sophisticated architectures for extracting powerful feature representations, which are more difficult to be developed on mobile devices. In this paper, we present an effective post-training quantization algorithm for <span class="search-hit mathjax">reducing</span> the <span class="search-hit mathjax">memory</span> storage and computational costs of vision transformers. Basically, the quantization task can be regarded as finding the <span class="search-hit mathjax">optimal</span> low-bit quantization intervals for weights and inputs, respectively. To preserve the functionality of the attention mechanism, we introduce a ranking loss into the conventional quantization objective that aims to keep the relative order of the self-attention results after quantization. Moreover, we thoroughly analyze the relationship between quantization loss of different layers and the feature diversity, and explore a mixed-precision quantization scheme by exploiting the nuclear norm of each attention map and output feature. The effectiveness of the proposed method is verified on several benchmark models and datasets, which outperforms the state-of-the-art post-training quantization algorithms. For instance, we can obtain an 81.29\% top-1 accuracy using DeiT-B model on ImageNet dataset with about 8-bit quantization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14156v1-abstract-full').style.display = 'none'; document.getElementById('2106.14156v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.11851">arXiv:2106.11851</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.11851">pdf</a>, <a href="https://arxiv.org/format/2106.11851">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stochastic Polyak Stepsize with a Moving Target
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gower%2C+R+M">Robert M. Gower</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Defazio%2C+A">Aaron Defazio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rabbat%2C+M">Michael Rabbat</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.11851v1-abstract-short" style="display: inline;">
        We propose a new stochastic gradient method that uses recorded past loss values to <span class="search-hit mathjax">reduce</span> the variance. Our method can be interpreted as a new stochastic variant of the Polyak Stepsize that converges globally without assuming interpolation. Our method introduces auxiliary variables, one for each data point, that track the loss value for each data point. We p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.11851v1-abstract-full').style.display = 'inline'; document.getElementById('2106.11851v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.11851v1-abstract-full" style="display: none;">
        We propose a new stochastic gradient method that uses recorded past loss values to <span class="search-hit mathjax">reduce</span> the variance. Our method can be interpreted as a new stochastic variant of the Polyak Stepsize that converges globally without assuming interpolation. Our method introduces auxiliary variables, one for each data point, that track the loss value for each data point. We provide a global convergence theory for our method by showing that it can be interpreted as a special variant of online SGD. The new method only stores a single scalar per data point, opening up new <span class="search-hit mathjax">applications</span> for variance reduction where <span class="search-hit mathjax">memory</span> is the bottleneck.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.11851v1-abstract-full').style.display = 'none'; document.getElementById('2106.11851v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">41 pages, 13 figures, 1 table</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          90C53; 74S60; 90C06; 62L20; 68W20; 15B52; 65Y20; 68W40
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          G.1.6
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.04217">arXiv:2106.04217</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.04217">pdf</a>, <a href="https://arxiv.org/format/2106.04217">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Sparse Training for Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sokar%2C+G">Ghada Sokar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mocanu%2C+E">Elena Mocanu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mocanu%2C+D+C">Decebal Constantin Mocanu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pechenizkiy%2C+M">Mykola Pechenizkiy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stone%2C+P">Peter Stone</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.04217v1-abstract-short" style="display: inline;">
        &hellip;success in many decision-making tasks in various fields. However, it requires a large training time of dense neural networks to obtain a good performance. This hinders its <span class="search-hit mathjax">applicability</span> on low-resource devices where&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04217v1-abstract-full').style.display = 'inline'; document.getElementById('2106.04217v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.04217v1-abstract-full" style="display: none;">
        Deep reinforcement learning has achieved significant success in many decision-making tasks in various fields. However, it requires a large training time of dense neural networks to obtain a good performance. This hinders its <span class="search-hit mathjax">applicability</span> on low-resource devices where <span class="search-hit mathjax">memory</span> and computation are strictly constrained. In a step towards enabling deep reinforcement learning agents to be applied to low-resource devices, in this work, we propose for the first time to dynamically train deep reinforcement learning agents with sparse neural networks from scratch. We adopt the evolution principles of dynamic sparse training in the reinforcement learning paradigm and introduce a training algorithm that <span class="search-hit mathjax">optimizes</span> the sparse topology and the weight values jointly to dynamically fit the incoming data. Our approach is easy to be integrated into existing deep reinforcement learning algorithms and has many favorable advantages. First, it allows for significant compression of the network size which <span class="search-hit mathjax">reduces</span> the <span class="search-hit mathjax">memory</span> and computation costs substantially. This would accelerate not only the agent inference but also its training process. Second, it speeds up the agent learning process and allows for <span class="search-hit mathjax">reducing</span> the number of required training steps. Third, it can achieve higher performance than training the dense counterpart network. We evaluate our approach on OpenAI gym continuous control tasks. The experimental results show the effectiveness of our approach in achieving higher performance than one of the state-of-art baselines with a 50\% reduction in the network size and floating-point operations (FLOPs). Moreover, our proposed approach can reach the same performance achieved by the dense network with a 40-50\% reduction in the number of training steps.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04217v1-abstract-full').style.display = 'none'; document.getElementById('2106.04217v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.02018">arXiv:2106.02018</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.02018">pdf</a>, <a href="https://arxiv.org/format/2106.02018">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Nonlinear Matrix Approximation with Radial Basis Function Components
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rebrova%2C+E">Elizaveta Rebrova</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Y">Yu-Hang Tang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.02018v2-abstract-short" style="display: inline;">
        &hellip;us to compute the decomposition for any real matrix that is not necessarily symmetric or positive definite. We formulate the problem of seeking such a decomposition as an <span class="search-hit mathjax">optimization</span> problem with a nonlinear and non-convex loss function. Several modern versions of the gradient descent method, including their scalable stochastic counterparts, are used to sol&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.02018v2-abstract-full').style.display = 'inline'; document.getElementById('2106.02018v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.02018v2-abstract-full" style="display: none;">
        We introduce and investigate matrix approximation by decomposition into a sum of radial basis function (RBF) components. An RBF component is a generalization of the outer product between a pair of vectors, where an RBF function replaces the scalar multiplication between individual vector elements. Even though the RBF functions are positive definite, the summation across components is not restricted to convex combinations and allows us to compute the decomposition for any real matrix that is not necessarily symmetric or positive definite. We formulate the problem of seeking such a decomposition as an <span class="search-hit mathjax">optimization</span> problem with a nonlinear and non-convex loss function. Several modern versions of the gradient descent method, including their scalable stochastic counterparts, are used to solve this problem. We provide extensive empirical evidence of the effectiveness of the RBF decomposition and that of the gradient-based fitting algorithm. While being conceptually motivated by singular value decomposition (SVD), our proposed nonlinear counterpart outperforms SVD by drastically <span class="search-hit mathjax">reducing</span> the <span class="search-hit mathjax">memory</span> required to approximate a data matrix with the same L2 error for a wide range of matrix types. For example, it leads to 2 to 6 times <span class="search-hit mathjax">memory</span> save for Gaussian noise, graph adjacency matrices, and kernel matrices. Moreover, this proximity-based decomposition can offer additional interpretability in <span class="search-hit mathjax">applications</span> that involve, e.g., capturing the inner low-dimensional structure of the data, retaining graph connectivity structure, and preserving the acutance of images.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.02018v2-abstract-full').style.display = 'none'; document.getElementById('2106.02018v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.01766">arXiv:2106.01766</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.01766">pdf</a>, <a href="https://arxiv.org/ps/2106.01766">ps</a>, <a href="https://arxiv.org/format/2106.01766">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ISPRAS.2018.00009">10.1109/ISPRAS.2018.00009 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Analysis of ARINC 653 RTOS with LLVM
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cheptsov%2C+V">Vitaly Cheptsov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khoroshilov%2C+A">Alexey Khoroshilov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.01766v1-abstract-short" style="display: inline;">
        Existing standards for airborne-embedded <span class="search-hit mathjax">software</span> systems impose a number of requirements&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.01766v1-abstract-full').style.display = 'inline'; document.getElementById('2106.01766v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.01766v1-abstract-full" style="display: none;">
        Existing standards for airborne-embedded <span class="search-hit mathjax">software</span> systems impose a number of requirements <span class="search-hit mathjax">applicable</span> to the <span class="search-hit mathjax">software</span> development cycle of hard real-time operating systems found in modern aircraft. The measures taken are meant to <span class="search-hit mathjax">reduce</span> the risks of undesired consequences, but have strongly varying costs. Dynamic instrumentation and static analysis are common practices used to <span class="search-hit mathjax">automatically</span> find <span class="search-hit mathjax">software</span> defects, from strictly non-conforming <span class="search-hit mathjax">code</span> constructions to <span class="search-hit mathjax">memory</span> corruptions or invalid control flow. LLVM analyser and sanitizer infrastructure, while regularly applied to general-purpose <span class="search-hit mathjax">software</span>, originally was not thought to be introduced to heavily restricted environments. In this paper we discuss the specifics of airborne systems with regards to dynamic instrumentation and provide practical considerations to be taken into account for the effective use of general-purpose instrumentation tools. We bring a complete LLVM stack support to JetOS, a prospective onboard real-time operating system currently being developed at ISP RAS in collaboration with GosNIIAS. As an example, we port AddressSanitizer, MemorySanitizer, and UndefinedBehaviorSanitizer and provide the details against the caveats on all relevant sides: a sanitizer, a compiler, and an operating system. In addition we suggest uninvolved optimisations and enhancements to the runtimes to maximise the effects of the tools.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.01766v1-abstract-full').style.display = 'none'; document.getElementById('2106.01766v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2018 Ivannikov Ispras Open Conference (ISPRAS), 2018, pp. 9-15
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.00606">arXiv:2106.00606</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.00606">pdf</a>, <a href="https://arxiv.org/format/2106.00606">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic-Deep: ECG Task-Aware Compression
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Brosh%2C+E">Eli Brosh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wasserstein%2C+E">Elad Wasserstein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bremler-Barr%2C+A">Anat Bremler-Barr</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.00606v1-abstract-short" style="display: inline;">
        Monitoring medical data, e.g., Electrocardiogram (ECG) signals, is a common <span class="search-hit mathjax">application</span> of Internet of Things (IoT) devices. Compression methods are often applied on the massive amounts of sensor data generated before sending it to the Cloud to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.00606v1-abstract-full').style.display = 'inline'; document.getElementById('2106.00606v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.00606v1-abstract-full" style="display: none;">
        Monitoring medical data, e.g., Electrocardiogram (ECG) signals, is a common <span class="search-hit mathjax">application</span> of Internet of Things (IoT) devices. Compression methods are often applied on the massive amounts of sensor data generated before sending it to the Cloud to <span class="search-hit mathjax">reduce</span> storage and delivery costs. A lossy compression provides high compression gain (CG) but may <span class="search-hit mathjax">reduce</span> the performance of an ECG <span class="search-hit mathjax">application</span> (downstream task) due to information loss. Previous works on ECG monitoring focus either on <span class="search-hit mathjax">optimizing</span> the signal reconstruction or the task&#39;s performance. Instead, we advocate a lossy compression solution that allows configuring a desired performance level on the downstream tasks while maintaining an <span class="search-hit mathjax">optimized</span> CG.
  We propose Dynamic-Deep, a task-aware compression that uses convolutional autoencoders. The compression level is dynamically selected to yield an <span class="search-hit mathjax">optimized</span> compression without violating tasks&#39; performance requirements. We conduct an extensive evaluation of our approach on common ECG datasets using two popular ECG <span class="search-hit mathjax">applications</span>, which includes heart rate (HR) arrhythmia classification. We demonstrate that Dynamic-Deep <span class="search-hit mathjax">improves</span> HR classification F1-score by a factor of 3 and increases CG by up to 83% compared to the previous state-of-the-art (autoencoder-based) compressor. Additionally, Dynamic-Deep has a 67% lower <span class="search-hit mathjax">memory</span> footprint. Analyzing Dynamic-Deep on the Google Cloud Platform, we observe a 97% reduction in cloud costs compared to a no compression solution.
  To the best of our knowledge, Dynamic-Deep is the first proposal to focus on balancing the need for high performance of cloud-based downstream tasks and the desire to achieve <span class="search-hit mathjax">optimized</span> compression in IoT ECG monitoring settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.00606v1-abstract-full').style.display = 'none'; document.getElementById('2106.00606v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted to Globecom2021 SAC MLC</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.14156">arXiv:2105.14156</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.14156">pdf</a>, <a href="https://arxiv.org/format/2105.14156">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.13140/RG.2.2.17515.87840">10.13140/RG.2.2.17515.87840 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SMASH: Sparse Matrix Atomic Scratchpad Hashing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shivdikar%2C+K">Kaustubh Shivdikar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.14156v1-abstract-short" style="display: inline;">
        Sparse matrices, more specifically SpGEMM kernels, are commonly found in a wide range of <span class="search-hit mathjax">applications</span>, spanning graph-based path-finding to machine learning algorithms (e.g., neural networks). A particular challenge in implementing SpGEMM kernels has been the pressure placed on DRAM&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.14156v1-abstract-full').style.display = 'inline'; document.getElementById('2105.14156v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.14156v1-abstract-full" style="display: none;">
        Sparse matrices, more specifically SpGEMM kernels, are commonly found in a wide range of <span class="search-hit mathjax">applications</span>, spanning graph-based path-finding to machine learning algorithms (e.g., neural networks). A particular challenge in implementing SpGEMM kernels has been the pressure placed on DRAM <span class="search-hit mathjax">memory</span>. One approach to tackle this problem is to use an inner product method for the SpGEMM kernel implementation. While the inner product produces fewer intermediate results, it can end up saturating the <span class="search-hit mathjax">memory</span> bandwidth, given the high number of redundant fetches of the input matrix elements. Using an outer product-based SpGEMM kernel can <span class="search-hit mathjax">reduce</span> redundant fetches, but at the cost of increased overhead due to extra computation and <span class="search-hit mathjax">memory</span> accesses for producing/managing partial products.
  In this thesis, we introduce a novel SpGEMM kernel implementation based on the row-wise product approach. We leverage atomic instructions to merge intermediate partial products as they are generated. The use of atomic instructions eliminates the need to create partial product matrices.
  To evaluate our row-wise product approach, we map an <span class="search-hit mathjax">optimized</span> SpGEMM kernel to a custom accelerator designed to accelerate graph-based <span class="search-hit mathjax">applications</span>. The targeted accelerator is an experimental system named PIUMA, being developed by Intel. PIUMA provides several attractive features, including fast context switching, user-configurable caches, globally addressable <span class="search-hit mathjax">memory</span>, non-coherent caches, and asynchronous pipelines. We tailor our SpGEMM kernel to exploit many of the features of the PIUMA fabric.
  This thesis compares our SpGEMM implementation against prior solutions, all mapped to the PIUMA framework. We briefly describe some of the PIUMA architecture features and then delve into the details of our <span class="search-hit mathjax">optimized</span> SpGEMM kernel. Our SpGEMM kernel can achieve 9.4x speedup as compared to competing approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.14156v1-abstract-full').style.display = 'none'; document.getElementById('2105.14156v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.05534">arXiv:2105.05534</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.05534">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Applied Physics">physics.app-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Statistical Temperature Coefficient Distribution in Analog RRAM Array: Impact on Neuromorphic System and Mitigation Method
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Heng Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Y">Yue Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Y">Yangyang Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaohu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+G">Guoxuan Qin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.05534v1-abstract-short" style="display: inline;">
        Emerging analog resistive random access <span class="search-hit mathjax">memory</span> (RRAM) based on HfOx is an attractive device for non-von Neumann neuromorphic computing systems. The differences in temperature dependent conductance drift among cells hamper computing accuracy, characterized by the statistical distribution of temperature coefficient(TÎ±). A compact model was presented in order t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.05534v1-abstract-full').style.display = 'inline'; document.getElementById('2105.05534v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.05534v1-abstract-full" style="display: none;">
        Emerging analog resistive random access <span class="search-hit mathjax">memory</span> (RRAM) based on HfOx is an attractive device for non-von Neumann neuromorphic computing systems. The differences in temperature dependent conductance drift among cells hamper computing accuracy, characterized by the statistical distribution of temperature coefficient(TÎ±). A compact model was presented in order to investigate the statistical distribution of TÎ± under different resistance states. Based on this model, the physical mechanism of thermal instability of cells with a positive TÎ± was elucidated. Furthermore, this model can also effectively evaluate the impact of conductance distribution of different levels under various temperatures in artificial neural networks (ANN). An approach incorporating the <span class="search-hit mathjax">optimized</span> conductance range selection and the current compensation scheme was proposed to <span class="search-hit mathjax">reduce</span> the impacts of the distribution of TÎ±. The simulation results showed that recognition accuracy was <span class="search-hit mathjax">improved</span> from 79.8% to 89.6% for the <span class="search-hit mathjax">application</span> of MNIST handwriting digits classification with a two-layer perceptron at 400K after adopting the proposed <span class="search-hit mathjax">optimization</span> method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.05534v1-abstract-full').style.display = 'none'; document.getElementById('2105.05534v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.05004">arXiv:2105.05004</a>
        <span>&nbsp;&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Smart Name Lookup for NDN Forwarding Plane via Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhuo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jindian Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+L">Liu Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Beichuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+P">Peng Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+K">Kaihua Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.05004v2-abstract-short" style="display: inline;">
        &hellip;the efficient name lookup, what counts is deploying a highperformance index in content routers. So far, the proposed indexes have shown good performance, most of which are <span class="search-hit mathjax">optimized</span> for or evaluated with URLs collected from the current Internet, as the large-scale NDN names are not available yet. Unfortunately, the performance of these indexes is always impa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.05004v2-abstract-full').style.display = 'inline'; document.getElementById('2105.05004v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.05004v2-abstract-full" style="display: none;">
        Name lookup is a key technology for the forwarding plane of content router in Named Data Networking (NDN). To realize the efficient name lookup, what counts is deploying a highperformance index in content routers. So far, the proposed indexes have shown good performance, most of which are <span class="search-hit mathjax">optimized</span> for or evaluated with URLs collected from the current Internet, as the large-scale NDN names are not available yet. Unfortunately, the performance of these indexes is always impacted in terms of lookup speed, <span class="search-hit mathjax">memory</span> consumption and false positive probability, as the distributions of URLs retrieved in <span class="search-hit mathjax">memory</span> may differ from those of real NDN names independently generated by content-centric <span class="search-hit mathjax">applications</span> online. Focusing on this gap, a smart mapping model named Pyramid-NN via neural networks is proposed to build an index called LNI for NDN forwarding plane. Through learning the distributions of the names retrieved in the static <span class="search-hit mathjax">memory</span>, LNI can not only <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">memory</span> consumption and the probability of false positive, but also ensure the performance of real NDN name lookup. Experimental results show that LNI-based FIB can <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">memory</span> consumption to 58.258 MB for 2 million names. Moreover, as it can be deployed on SRAMs, the throughput is about 177 MSPS, which well meets the current network requirement for fast packet processing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.05004v2-abstract-full').style.display = 'none'; document.getElementById('2105.05004v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">We need to refine the paper further including the title and the structure of the paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.03536">arXiv:2105.03536</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.03536">pdf</a>, <a href="https://arxiv.org/format/2105.03536">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Pareto-<span class="search-hit mathjax">Optimal</span> Quantized ResNet Is Mostly 4-bit
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abdolrashidi%2C+A">AmirAli Abdolrashidi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Lisa Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agrawal%2C+S">Shivani Agrawal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Malmaud%2C+J">Jonathan Malmaud</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rybakov%2C+O">Oleg Rybakov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leichner%2C+C">Chas Leichner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lew%2C+L">Lukasz Lew</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.03536v1-abstract-short" style="display: inline;">
        Quantization has become a popular technique to compress neural networks and <span class="search-hit mathjax">reduce</span> compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.03536v1-abstract-full').style.display = 'inline'; document.getElementById('2105.03536v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.03536v1-abstract-full" style="display: none;">
        Quantization has become a popular technique to compress neural networks and <span class="search-hit mathjax">reduce</span> compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world <span class="search-hit mathjax">applications</span> of neural networks have compute cost and <span class="search-hit mathjax">memory</span> budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher accuracy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is <span class="search-hit mathjax">optimized</span> for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into <span class="search-hit mathjax">optimal</span> numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.03536v1-abstract-full').style.display = 'none'; document.getElementById('2105.03536v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages. Accepted at the Efficient Deep Learning for Computer Vision Workshop at CVPR 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.02788">arXiv:2105.02788</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.02788">pdf</a>, <a href="https://arxiv.org/format/2105.02788">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ACORN: Adaptive Coordinate Networks for Neural Scene Representation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Martel%2C+J+N+P">Julien N. P. Martel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lindell%2C+D+B">David B. Lindell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+C+Z">Connor Z. Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chan%2C+E+R">Eric R. Chan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Monteiro%2C+M">Marco Monteiro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wetzstein%2C+G">Gordon Wetzstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.02788v1-abstract-short" style="display: inline;">
        Neural representations have emerged as a new paradigm for <span class="search-hit mathjax">applications</span> in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.02788v1-abstract-full').style.display = 'inline'; document.getElementById('2105.02788v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.02788v1-abstract-full" style="display: none;">
        Neural representations have emerged as a new paradigm for <span class="search-hit mathjax">applications</span> in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent <span class="search-hit mathjax">improvements</span> to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is <span class="search-hit mathjax">optimized</span> during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it <span class="search-hit mathjax">reduces</span> training times from days to hours or minutes and <span class="search-hit mathjax">memory</span> requirements by over an order of magnitude.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.02788v1-abstract-full').style.display = 'none'; document.getElementById('2105.02788v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">J. N. P. Martel and D. B. Lindell equally contributed to this work</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.00619">arXiv:2105.00619</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.00619">pdf</a>, <a href="https://arxiv.org/format/2105.00619">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OpTorch: <span class="search-hit mathjax">Optimized</span> deep learning architectures for resource limited environments
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ahmed%2C+S">Salman Ahmed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Naveed%2C+H">Hammad Naveed</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.00619v2-abstract-short" style="display: inline;">
        Deep learning algorithms have made many breakthroughs and have various <span class="search-hit mathjax">applications</span> in real life. Computational resources become a bottleneck as the data and complexity of the deep learning pipeline increases. In this paper, we propose&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00619v2-abstract-full').style.display = 'inline'; document.getElementById('2105.00619v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.00619v2-abstract-full" style="display: none;">
        Deep learning algorithms have made many breakthroughs and have various <span class="search-hit mathjax">applications</span> in real life. Computational resources become a bottleneck as the data and complexity of the deep learning pipeline increases. In this paper, we propose <span class="search-hit mathjax">optimized</span> deep learning pipelines in multiple aspects of training including time and <span class="search-hit mathjax">memory</span>. OpTorch is a machine learning library designed to overcome weaknesses in existing implementations of neural network training. OpTorch provides features to train complex neural networks with limited computational resources. OpTorch achieved the same accuracy as existing libraries on Cifar-10 and Cifar-100 datasets while <span class="search-hit mathjax">reducing</span> <span class="search-hit mathjax">memory</span> usage to approximately 50%. We also explore the effect of weights on total <span class="search-hit mathjax">memory</span> usage in deep learning pipelines. In our experiments, parallel encoding-decoding along with sequential checkpoints results in much <span class="search-hit mathjax">improved</span> <span class="search-hit mathjax">memory</span> and time usage while keeping the accuracy similar to existing pipelines. OpTorch python package is available at available at https://github.com/cbrl-nuces/optorch
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00619v2-abstract-full').style.display = 'none'; document.getElementById('2105.00619v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.00027">arXiv:2105.00027</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.00027">pdf</a>, <a href="https://arxiv.org/format/2105.00027">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Strongly Correlated Electrons">cond-mat.str-el</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Superconductivity">cond-mat.supr-con</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Memory</span> Reduction using a Ring Abstraction over GPU RDMA for Distributed Quantum Monte Carlo Solver
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+W">Weile Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=D%27Azevedo%2C+E">Eduardo D&#39;Azevedo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huck%2C+K">Kevin Huck</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatterjee%2C+A">Arghya Chatterjee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hernandez%2C+O">Oscar Hernandez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaiser%2C+H">Hartmut Kaiser</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.00027v2-abstract-short" style="display: inline;">
        Scientific <span class="search-hit mathjax">applications</span> that run on leadership computing facilities often face the challenge of being unable to fit leading science cases onto accelerator devices due to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00027v2-abstract-full').style.display = 'inline'; document.getElementById('2105.00027v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.00027v2-abstract-full" style="display: none;">
        Scientific <span class="search-hit mathjax">applications</span> that run on leadership computing facilities often face the challenge of being unable to fit leading science cases onto accelerator devices due to <span class="search-hit mathjax">memory</span> constraints (<span class="search-hit mathjax">memory</span>-bound <span class="search-hit mathjax">applications</span>). In this work, the authors studied one such US Department of Energy mission-critical condensed matter physics <span class="search-hit mathjax">application</span>, Dynamical Cluster Approximation (DCA++), and this paper discusses how device <span class="search-hit mathjax">memory</span>-bound challenges were successfully <span class="search-hit mathjax">reduced</span> by proposing an effective &#34;all-to-all&#34; communication method -- a ring communication algorithm. This implementation takes advantage of acceleration on GPUs and remote direct <span class="search-hit mathjax">memory</span> access (RDMA) for fast data exchange between GPUs.
  Additionally, the ring algorithm was <span class="search-hit mathjax">optimized</span> with sub-ring communicators and multi-threaded support to further <span class="search-hit mathjax">reduce</span> communication overhead and expose more concurrency, respectively. The computation and communication were also analyzed by using the Autonomic Performance Environment for Exascale (APEX) profiling tool, and this paper further discusses the performance trade-off for the ring algorithm implementation. The <span class="search-hit mathjax">memory</span> analysis on the ring algorithm shows that the allocation size for the authors&#39; most <span class="search-hit mathjax">memory</span>-intensive data structure per GPU is now <span class="search-hit mathjax">reduced</span> to 1/p of the original size, where p is the number of GPUs in the ring communicator. The communication analysis suggests that the distributed Quantum Monte Carlo execution time grows linearly as sub-ring size increases, and the cost of messages passing through the network interface connector could be a limiting factor.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00027v2-abstract-full').style.display = 'none'; document.getElementById('2105.00027v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.13209">arXiv:2104.13209</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.13209">pdf</a>, <a href="https://arxiv.org/format/2104.13209">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        K-Clique Counting on GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Almasri%2C+M">Mohammad Almasri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hajj%2C+I+E">Izzat El Hajj</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nagi%2C+R">Rakesh Nagi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+J">Jinjun Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hwu%2C+W">Wen-mei Hwu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.13209v1-abstract-short" style="display: inline;">
        Counting k-cliques in a graph is an important problem in graph analysis with many <span class="search-hit mathjax">applications</span>. Counting k-cliques is typically done by traversing search trees starting at each vertex in the graph. An important&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.13209v1-abstract-full').style.display = 'inline'; document.getElementById('2104.13209v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.13209v1-abstract-full" style="display: none;">
        Counting k-cliques in a graph is an important problem in graph analysis with many <span class="search-hit mathjax">applications</span>. Counting k-cliques is typically done by traversing search trees starting at each vertex in the graph. An important <span class="search-hit mathjax">optimization</span> is to eliminate search tree branches that discover the same clique redundantly. Eliminating redundant clique discovery is typically done via graph orientation or pivoting. Parallel implementations for both of these approaches have demonstrated promising performance on CPUs. In this paper, we present our GPU implementations of k-clique counting for both the graph orientation and pivoting approaches. Our implementations explore both vertex-centric and edge-centric parallelization schemes, and replace recursive search tree traversal with iterative traversal based on an explicitly-managed shared stack. We also apply various <span class="search-hit mathjax">optimizations</span> to <span class="search-hit mathjax">reduce</span> <span class="search-hit mathjax">memory</span> consumption and <span class="search-hit mathjax">improve</span> the utilization of parallel execution resources. Our evaluation shows that our best GPU implementation outperforms the best state-of-the-art parallel CPU implementation by a geometric mean speedup of 12.39x, 6.21x, and 18.99x for k = 4, 7, and 10, respectively. We also evaluate the impact of the choice of parallelization scheme and the incremental speedup of each <span class="search-hit mathjax">optimization</span>. Our <span class="search-hit mathjax">code</span> will be open-sourced to enable further research on parallelizing k-clique counting on GPUs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.13209v1-abstract-full').style.display = 'none'; document.getElementById('2104.13209v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.10939">arXiv:2104.10939</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.10939">pdf</a>, <a href="https://arxiv.org/format/2104.10939">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HINT: A Hierarchical Index for Intervals in Main <span class="search-hit mathjax">Memory</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Christodoulou%2C+G">George Christodoulou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bouros%2C+P">Panagiotis Bouros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mamoulis%2C+N">Nikos Mamoulis</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.10939v1-abstract-short" style="display: inline;">
        Indexing intervals is a fundamental problem, finding a wide range of <span class="search-hit mathjax">applications</span>. Recent work on managing large collections of intervals in main&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10939v1-abstract-full').style.display = 'inline'; document.getElementById('2104.10939v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.10939v1-abstract-full" style="display: none;">
        Indexing intervals is a fundamental problem, finding a wide range of <span class="search-hit mathjax">applications</span>. Recent work on managing large collections of intervals in main <span class="search-hit mathjax">memory</span> focused on overlap joins and temporal aggregation problems. In this paper, we propose novel and efficient in-<span class="search-hit mathjax">memory</span> indexing techniques for intervals, with a focus on interval range queries, which are a basic component of many search and analysis tasks. First, we propose an <span class="search-hit mathjax">optimized</span> version of a single-level (flat) domain-partitioning approach, which may have large space requirements due to excessive replication. Then, we propose a hierarchical partitioning approach, which assigns each interval to at most two partitions per level and has controlled space requirements. Novel elements of our techniques include the division of the intervals at each partition into groups based on whether they begin inside or before the partition boundaries, <span class="search-hit mathjax">reducing</span> the information stored at each partition to the absolutely necessary, and the effective handling of data sparsity and skew. Experimental results on real and synthetic interval sets of different characteristics show that our approaches are typically one order of magnitude faster than the state-of-the-art.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10939v1-abstract-full').style.display = 'none'; document.getElementById('2104.10939v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.10716">arXiv:2104.10716</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.10716">pdf</a>, <a href="https://arxiv.org/format/2104.10716">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Accelerating SpMM Kernel with Cache-First Edge Sampling for Graph Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+C">Chien-Yu Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+L">Liang Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ceze%2C+L">Luis Ceze</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.10716v2-abstract-short" style="display: inline;">
        &hellip;deep learning model class, can extract meaningful representations from highly expressive graph-structured data and are therefore gaining popularity for wider ranges of <span class="search-hit mathjax">applications</span>. However, current GNNs suffer from the poor performance of their sparse-dense matrix multiplication (SpMM) operator, even when using powerful GPUs. Our analysis shows that 95% of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10716v2-abstract-full').style.display = 'inline'; document.getElementById('2104.10716v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.10716v2-abstract-full" style="display: none;">
        Graph neural networks (GNNs), an emerging deep learning model class, can extract meaningful representations from highly expressive graph-structured data and are therefore gaining popularity for wider ranges of <span class="search-hit mathjax">applications</span>. However, current GNNs suffer from the poor performance of their sparse-dense matrix multiplication (SpMM) operator, even when using powerful GPUs. Our analysis shows that 95% of the inference time could be spent on SpMM when running popular GNN models on NVIDIA&#39;s advanced V100 GPU. Such SpMM performance bottleneck hinders GNNs&#39; <span class="search-hit mathjax">applicability</span> to large-scale problems or the development of more sophisticated GNN models. To address this inference time bottleneck, we introduce ES-SpMM, a cache-first edge sampling mechanism and codesigned SpMM kernel. ES-SpMM uses edge sampling to downsize the graph to fit into GPU&#39;s shared <span class="search-hit mathjax">memory</span>. It thus <span class="search-hit mathjax">reduces</span> the computation cost and <span class="search-hit mathjax">improves</span> SpMM&#39;s cache locality. To evaluate ES-SpMM&#39;s performance, we integrated it with a popular GNN framework, DGL, and tested it using representative GNN models and datasets. Our results show that ES-SpMM outperforms the highly <span class="search-hit mathjax">optimized</span> cuSPARSE SpMM kernel by up to 4.35x with no accuracy loss and by 45.3x with less than a 1% accuracy loss.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10716v2-abstract-full').style.display = 'none'; document.getElementById('2104.10716v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.09616">arXiv:2104.09616</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.09616">pdf</a>, <a href="https://arxiv.org/format/2104.09616">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MELOPPR: <span class="search-hit mathjax">Software</span>/Hardware Co-design for <span class="search-hit mathjax">Memory</span>-efficient Low-latency Personalized PageRank
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lixiang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zirnheld%2C+Z">Zacharie Zirnheld</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+P">Pan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+C">Cong Hao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.09616v1-abstract-short" style="display: inline;">
        Personalized PageRank (PPR) is a graph algorithm that evaluates the importance of the surrounding nodes from a source node. Widely used in social network related <span class="search-hit mathjax">applications</span> such as recommender systems, PPR requires real-time responses (latency) for a better user experience. Existing works either focus on algorithmic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.09616v1-abstract-full').style.display = 'inline'; document.getElementById('2104.09616v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.09616v1-abstract-full" style="display: none;">
        Personalized PageRank (PPR) is a graph algorithm that evaluates the importance of the surrounding nodes from a source node. Widely used in social network related <span class="search-hit mathjax">applications</span> such as recommender systems, PPR requires real-time responses (latency) for a better user experience. Existing works either focus on algorithmic <span class="search-hit mathjax">optimization</span> for <span class="search-hit mathjax">improving</span> precision while neglecting hardware implementations or focus on distributed global graph processing on large-scale systems for <span class="search-hit mathjax">improving</span> throughput rather than response time. <span class="search-hit mathjax">Optimizing</span> low-latency local PPR algorithm with a tight <span class="search-hit mathjax">memory</span> budget on edge devices remains unexplored. In this work, we propose a <span class="search-hit mathjax">memory</span>-efficient, low-latency PPR solution, namely MeLoPPR, with largely <span class="search-hit mathjax">reduced</span> <span class="search-hit mathjax">memory</span> requirement and a flexible trade-off between latency and precision. MeLoPPR is composed of stage decomposition and linear decomposition and exploits the node score sparsity: Through stage and linear decomposition, MeLoPPR breaks the computation on a large graph into a set of smaller sub-graphs, that significantly saves the computation <span class="search-hit mathjax">memory</span>; Through sparsity exploitation, MeLoPPR selectively chooses the sub-graphs that contribute the most to the precision to <span class="search-hit mathjax">reduce</span> the required computation. In addition, through <span class="search-hit mathjax">software</span>/hardware co-design, we propose a hardware implementation on a hybrid CPU and FPGA accelerating platform, that further speeds up the sub-graph computation. We evaluate the proposed MeLoPPR on <span class="search-hit mathjax">memory</span>-constrained devices including a personal laptop and Xilinx Kintex-7 KC705 FPGA using six real-world graphs. First, MeLoPPR demonstrates significant <span class="search-hit mathjax">memory</span> saving by 1.5x to 13.4x on CPU and 73x to 8699x on FPGA. Second, MeLoPPR allows flexible trade-offs between precision and execution time: when the precision is 80%, the speedup on CPU is up to 15x and up to 707x on FPGA; when the precision is around 90%, the speedup is up to 70x on FPGA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.09616v1-abstract-full').style.display = 'none'; document.getElementById('2104.09616v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IEEE Design Automation Conference, 2021 (DAC&#39;21). Six pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.09252">arXiv:2104.09252</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.09252">pdf</a>, <a href="https://arxiv.org/format/2104.09252">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning on Hardware: A Tutorial on Neural Network Accelerators and Co-Processors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Baischer%2C+L">Lukas Baischer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wess%2C+M">Matthias Wess</a>, 
      
      <a href="/search/?searchtype=author&amp;query=TaheriNejad%2C+N">Nima TaheriNejad</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.09252v1-abstract-short" style="display: inline;">
        &hellip;accuracy than common algorithms, and in some tasks, they boast an even higher accuracy than human experts. With the progress of DNNs in recent years, many other fields of <span class="search-hit mathjax">application</span> such as diagnosis of diseases and autonomous driving are taking advantage of them. The trend at DNNs is clear: The network size is growing exponentially, which leads to an expon&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.09252v1-abstract-full').style.display = 'inline'; document.getElementById('2104.09252v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.09252v1-abstract-full" style="display: none;">
        Deep neural networks (DNNs) have the advantage that they can take into account a large number of parameters, which enables them to solve complex tasks. In computer vision and speech recognition, they have a better accuracy than common algorithms, and in some tasks, they boast an even higher accuracy than human experts. With the progress of DNNs in recent years, many other fields of <span class="search-hit mathjax">application</span> such as diagnosis of diseases and autonomous driving are taking advantage of them. The trend at DNNs is clear: The network size is growing exponentially, which leads to an exponential increase in computational effort and required <span class="search-hit mathjax">memory</span> size. For this reason, <span class="search-hit mathjax">optimized</span> hardware accelerators are used to increase the performance of the inference of neuronal networks. However, there are various neural network hardware accelerator platforms, such as graphics processing units (GPUs), <span class="search-hit mathjax">application</span> specific integrated circuits (ASICs) and field programmable gate arrays (FPGAs). Each of these platforms offer certain advantages and disadvantages. Also, there are various methods for <span class="search-hit mathjax">reducing</span> the computational effort of DNNs, which are differently suitable for each hardware accelerator. In this article an overview of existing neural network hardware accelerators and acceleration methods is given. Their strengths and weaknesses are shown and a recommendation of suitable <span class="search-hit mathjax">applications</span> is given. In particular, we focus on acceleration of the inference of convolutional neural networks (CNNs) used for image recognition tasks. Given that there exist many different hardware architectures. FPGA-based implementations are well-suited to show the effect of DNN <span class="search-hit mathjax">optimization</span> methods on accuracy and throughput. For this reason, the focus of this work is more on FPGA-based implementations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.09252v1-abstract-full').style.display = 'none'; document.getElementById('2104.09252v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.05158">arXiv:2104.05158</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.05158">pdf</a>, <a href="https://arxiv.org/format/2104.05158">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        High-performance, Distributed Training of Large-scale Deep Learning Recommendation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mudigere%2C+D">Dheevatsa Mudigere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+Y">Yuchen Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+J">Jianyu Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tulloch%2C+A">Andrew Tulloch</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sridharan%2C+S">Srinivas Sridharan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xing Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ozdal%2C+M">Mustafa Ozdal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nie%2C+J">Jade Nie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+J">Jongsoo Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+L">Liang Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J+A">Jie Amy Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+L">Leon Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ivchenko%2C+D">Dmytro Ivchenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Basant%2C+A">Aarti Basant</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Y">Yuxi Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jiyan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ardestani%2C+E+K">Ehsan K. Ardestani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaodong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Komuravelli%2C+R">Rakesh Komuravelli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+C">Ching-Hsiang Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yilmaz%2C+S">Serhat Yilmaz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Huayu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+J">Jiyuan Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+Z">Zhuobo Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Y">Yinbin Ma</a>
      , et al. (26 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.05158v3-abstract-short" style="display: inline;">
        Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI <span class="search-hit mathjax">application</span> in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalabl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.05158v3-abstract-full').style.display = 'inline'; document.getElementById('2104.05158v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.05158v3-abstract-full" style="display: none;">
        Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI <span class="search-hit mathjax">application</span> in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-performance distributed training of large-scale DLRMs. We introduce a high-performance scalable <span class="search-hit mathjax">software</span> stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, <span class="search-hit mathjax">optimal</span> topology and efficient transport (ii) implementing an <span class="search-hit mathjax">optimized</span> PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-performance core operators while retaining flexibility to support <span class="search-hit mathjax">optimizers</span> with fully deterministic updates (v) leveraging <span class="search-hit mathjax">reduced</span> precision communications, multi-level <span class="search-hit mathjax">memory</span> hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and efficient end-to-end training in production environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.05158v3-abstract-full').style.display = 'none'; document.getElementById('2104.05158v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.03058">arXiv:2104.03058</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.03058">pdf</a>, <a href="https://arxiv.org/format/2104.03058">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Optimizing</span> <span class="search-hit mathjax">Memory</span> Efficiency of Graph Neural Networks on Edge Computing Platforms
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+A">Ao Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jianlei Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Y">Yeqi Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiao%2C+T">Tong Qiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qi%2C+Y">Yingjie Qi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaoyi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yunli Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+P">Pengcheng Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+W">Weisheng Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+C">Chunming Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.03058v2-abstract-short" style="display: inline;">
        Graph neural networks (GNN) have achieved state-of-the-art performance on various industrial tasks. However, the poor efficiency of GNN inference and frequent Out-Of-<span class="search-hit mathjax">Memory</span> (OOM) problem limit the successful&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.03058v2-abstract-full').style.display = 'inline'; document.getElementById('2104.03058v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.03058v2-abstract-full" style="display: none;">
        Graph neural networks (GNN) have achieved state-of-the-art performance on various industrial tasks. However, the poor efficiency of GNN inference and frequent Out-Of-<span class="search-hit mathjax">Memory</span> (OOM) problem limit the successful <span class="search-hit mathjax">application</span> of GNN on edge computing platforms. To tackle these problems, a feature decomposition approach is proposed for <span class="search-hit mathjax">memory</span> efficiency <span class="search-hit mathjax">optimization</span> of GNN inference. The proposed approach could achieve outstanding <span class="search-hit mathjax">optimization</span> on various GNN models, covering a wide range of datasets, which speeds up the inference by up to 3x. Furthermore, the proposed feature decomposition could significantly <span class="search-hit mathjax">reduce</span> the peak <span class="search-hit mathjax">memory</span> usage (up to 5x in <span class="search-hit mathjax">memory</span> efficiency <span class="search-hit mathjax">improvement</span>) and mitigate OOM problems during GNN inference.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.03058v2-abstract-full').style.display = 'none'; document.getElementById('2104.03058v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been accepted by RTAS 2021(brief industry track), with link to publicly available <span class="search-hit mathjax">code</span></span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.02188">arXiv:2104.02188</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.02188">pdf</a>, <a href="https://arxiv.org/format/2104.02188">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GPU Domain Specialization via Composable On-Package Architecture
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+Y">Yaosheng Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bolotin%2C+E">Evgeny Bolotin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatterjee%2C+N">Niladrish Chatterjee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nellans%2C+D">David Nellans</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keckler%2C+S+W">Stephen W. Keckler</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.02188v1-abstract-short" style="display: inline;">
        As GPUs scale their low precision matrix math throughput to boost deep learning (DL) performance, they upset the balance between math throughput and <span class="search-hit mathjax">memory</span> system capabilities. We demonstrate that converged GPU design trying to address diverging architectural requirements between FP32 (or larger) based HPC and FP16 (or smaller) based DL workloads results in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02188v1-abstract-full').style.display = 'inline'; document.getElementById('2104.02188v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.02188v1-abstract-full" style="display: none;">
        As GPUs scale their low precision matrix math throughput to boost deep learning (DL) performance, they upset the balance between math throughput and <span class="search-hit mathjax">memory</span> system capabilities. We demonstrate that converged GPU design trying to address diverging architectural requirements between FP32 (or larger) based HPC and FP16 (or smaller) based DL workloads results in sub-<span class="search-hit mathjax">optimal</span> configuration for either of the <span class="search-hit mathjax">application</span> domains. We argue that a Composable On-PAckage GPU (COPAGPU) architecture to provide domain-specialized GPU products is the most practical solution to these diverging requirements. A COPA-GPU leverages multi-chip-module disaggregation to support maximal design reuse, along with <span class="search-hit mathjax">memory</span> system specialization per <span class="search-hit mathjax">application</span> domain. We show how a COPA-GPU enables DL-specialized products by modular augmentation of the baseline GPU architecture with up to 4x higher off-die bandwidth, 32x larger on-package cache, 2.3x higher DRAM bandwidth and capacity, while conveniently supporting scaled-down HPC-oriented designs. This work explores the microarchitectural design necessary to enable composable GPUs and evaluates the benefits composability can provide to HPC, DL training, and DL inference. We show that when compared to a converged GPU design, a DL-<span class="search-hit mathjax">optimized</span> COPA-GPU featuring a combination of 16x larger cache capacity and 1.6x higher DRAM bandwidth scales per-GPU training and inference performance by 31% and 35% respectively and <span class="search-hit mathjax">reduces</span> the number of GPU instances by 50% in scale-out training scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02188v1-abstract-full').style.display = 'none'; document.getElementById('2104.02188v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.02162">arXiv:2104.02162</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.02162">pdf</a>, <a href="https://arxiv.org/format/2104.02162">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Near-Precise Parameter Approximation for Multiple Multiplications on A Single DSP Block
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kalali%2C+E">Ercan Kalali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=van+Leuken%2C+R">Rene van Leuken</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.02162v1-abstract-short" style="display: inline;">
        A multiply-accumulate (MAC) operation is the main computation unit for DSP <span class="search-hit mathjax">applications</span>. DSP blocks are one of the efficient solutions to implement MACs in FPGA&#39;s. However, since the DSP blocks have wide multiplier and adder blocks, MAC operations using low bit-length parameters lead to an underutilization problem. Hence, an efficient approximation techn&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02162v1-abstract-full').style.display = 'inline'; document.getElementById('2104.02162v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.02162v1-abstract-full" style="display: none;">
        A multiply-accumulate (MAC) operation is the main computation unit for DSP <span class="search-hit mathjax">applications</span>. DSP blocks are one of the efficient solutions to implement MACs in FPGA&#39;s. However, since the DSP blocks have wide multiplier and adder blocks, MAC operations using low bit-length parameters lead to an underutilization problem. Hence, an efficient approximation technique is introduced. The technique includes manipulation and approximation of the low bit-length fixed-point parameters based upon a Single DSP - Multiple Multiplication (SDMM) execution. The SDMM changes the traditional MAC implementation in the DSP block by separating multiplication and accumulation operations. While the accumulator hardware available in the DSP block is used for multiple parameter multiplication, parallel LUTs are employed for the accumulation part of the MAC operation. The accuracy of the developed <span class="search-hit mathjax">optimization</span> technique was evaluated for different CNN weight bit precisions using the Alexnet and VGG-16 networks and the Tiny ImageNet dataset. The <span class="search-hit mathjax">optimization</span> can be implemented without loss of accuracy in almost all cases, while it causes slight accuracy losses in a few cases. Through these <span class="search-hit mathjax">optimizations</span>, the SDMM is performed at the cost of a small hardware overhead. For example, a single DSP block executes 3 8-bit fixed-point parameter multiplications. As a result of our <span class="search-hit mathjax">optimizations</span>, the parameters are represented in a different format on off-chip <span class="search-hit mathjax">memory</span>, providing up to 33% compression without any hardware cost. The compression rate can be further increased by up to 97% when used in conjunction with other compression methods for the VGG-16. Reaching this compression rate requires extra hardware cost. A prototype systolic array architecture was implemented employing our <span class="search-hit mathjax">optimizations</span> on a Xilinx Zynq FPGA. It <span class="search-hit mathjax">reduced</span> the number of DSP blocks by 66.6%, 75%, and 83.3% for 8, 6, and 4-bit input variables, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02162v1-abstract-full').style.display = 'none'; document.getElementById('2104.02162v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 10 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.12293">arXiv:2103.12293</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.12293">pdf</a>, <a href="https://arxiv.org/format/2103.12293">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stochastic Reweighted Gradient Descent
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hanchi%2C+A+E">Ayoub El Hanchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stephens%2C+D+A">David A. Stephens</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.12293v1-abstract-short" style="display: inline;">
        Despite the strong theoretical guarantees that variance-<span class="search-hit mathjax">reduced</span> finite-sum&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.12293v1-abstract-full').style.display = 'inline'; document.getElementById('2103.12293v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.12293v1-abstract-full" style="display: none;">
        Despite the strong theoretical guarantees that variance-<span class="search-hit mathjax">reduced</span> finite-sum <span class="search-hit mathjax">optimization</span> algorithms enjoy, their <span class="search-hit mathjax">applicability</span> remains limited to cases where the <span class="search-hit mathjax">memory</span> overhead they introduce (SAG/SAGA), or the periodic full gradient computation they require (SVRG/SARAH) are manageable. A promising approach to achieving variance reduction while avoiding these drawbacks is the use of importance sampling instead of control variates. While many such methods have been proposed in the literature, directly proving that they <span class="search-hit mathjax">improve</span> the convergence of the resulting <span class="search-hit mathjax">optimization</span> algorithm has remained elusive. In this work, we propose an importance-sampling-based algorithm we call SRG (stochastic reweighted gradient). We analyze the convergence of SRG in the strongly-convex case and show that, while it does not recover the linear rate of control variates methods, it provably outperforms SGD. We pay particular attention to the time and <span class="search-hit mathjax">memory</span> overhead of our proposed method, and design a specialized red-black tree allowing its efficient implementation. Finally, we present empirical results to support our findings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.12293v1-abstract-full').style.display = 'none'; document.getElementById('2103.12293v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.10779">arXiv:2103.10779</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.10779">pdf</a>, <a href="https://arxiv.org/format/2103.10779">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Page Table Management for Heterogeneous <span class="search-hit mathjax">Memory</span> Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+S">Sandeep Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prasad%2C+A">Aravinda Prasad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sarangi%2C+S+R">Smruti R. Sarangi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Subramoney%2C+S">Sreenivas Subramoney</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.10779v1-abstract-short" style="display: inline;">
        Modern enterprise servers are increasingly embracing tiered <span class="search-hit mathjax">memory</span> systems with a combination of low latency DRAMs and large capacity but high latency non-volatile main&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.10779v1-abstract-full').style.display = 'inline'; document.getElementById('2103.10779v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.10779v1-abstract-full" style="display: none;">
        Modern enterprise servers are increasingly embracing tiered <span class="search-hit mathjax">memory</span> systems with a combination of low latency DRAMs and large capacity but high latency non-volatile main <span class="search-hit mathjax">memories</span> (NVMMs) such as Intel&#39;s Optane DC PMM. Prior works have focused on efficient placement and migration of data on a tiered <span class="search-hit mathjax">memory</span> system, but have not studied the <span class="search-hit mathjax">optimal</span> placement of page tables.
  Explicit and efficient placement of page tables is crucial for large <span class="search-hit mathjax">memory</span> footprint <span class="search-hit mathjax">applications</span> with high TLB miss rates because they incur dramatically higher page walk latency when page table pages are placed in NVMM. We show that (i) page table pages can end up on NVMM even when enough DRAM <span class="search-hit mathjax">memory</span> is available and (ii) page table pages that spill over to NVMM due to DRAM <span class="search-hit mathjax">memory</span> pressure are not migrated back later when <span class="search-hit mathjax">memory</span> is available in DRAM.
  We study the performance impact of page table placement in a tiered <span class="search-hit mathjax">memory</span> system and propose an efficient and transparent page table management technique that (i) applies different placement policies for data and page table pages, (ii) introduces a differentiating policy for page table pages by placing a small but critical part of the page table in DRAM, and (iii) dynamically and judiciously manages the rest of the page table by transparently migrating the page table pages between DRAM and NVMM. Our implementation on a real system equipped with Intel&#39;s Optane NVMM running Linux <span class="search-hit mathjax">reduces</span> the page table walk cycles by 12% and total cycles by 20% on an average. This <span class="search-hit mathjax">improves</span> the runtime by 20% on an average for a set of synthetic and real-world large <span class="search-hit mathjax">memory</span> footprint <span class="search-hit mathjax">applications</span> when compared with various default Linux kernel techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.10779v1-abstract-full').style.display = 'none'; document.getElementById('2103.10779v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.10381">arXiv:2103.10381</a>
        <span>&nbsp;&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Physics">physics.comp-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Data-driven Coarse-grained Modeling of Non-equilibrium Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Z">Zhan Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+W">Wenxiao Pan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.10381v2-abstract-short" style="display: inline;">
        Modeling a high-dimensional Hamiltonian system in <span class="search-hit mathjax">reduced</span> dimensions with respect to coarse-grained (CG) variables can greatly&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.10381v2-abstract-full').style.display = 'inline'; document.getElementById('2103.10381v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.10381v2-abstract-full" style="display: none;">
        Modeling a high-dimensional Hamiltonian system in <span class="search-hit mathjax">reduced</span> dimensions with respect to coarse-grained (CG) variables can greatly <span class="search-hit mathjax">reduce</span> computational cost and enable efficient bottom-up prediction of main features of the system for many <span class="search-hit mathjax">applications</span>. However, it usually experiences significantly altered dynamics due to loss of degrees of freedom upon coarse-graining. To establish CG models that can faithfully preserve dynamics, previous efforts mainly focused on equilibrium systems. In contrast, various soft matter systems are known out of equilibrium. Therefore, the present work concerns non-equilibrium systems and enables accurate and efficient CG modeling that preserves non-equilibrium dynamics and is generally <span class="search-hit mathjax">applicable</span> to any non-equilibrium process and any observable of interest. To this end, the dynamic equation of a CG variable is built in the form of the non-stationary generalized Langevin equation (nsGLE) to account for the dependence of non-equilibrium processes on the initial conditions, where the two-time <span class="search-hit mathjax">memory</span> kernel is determined from the data of the two-time auto-correlation function of the non-equilibrium trajectory-averaged observable of interest. By embedding the non-stationary non-Markovian process in an extended stochastic framework, an explicit form of the non-stationary random noise in the nsGLE is introduced, and the cost is significantly <span class="search-hit mathjax">reduced</span> for solving the nsGLE to predict the non-equilibrium dynamics of the CG variable. To prove and exploit the equivalence of the nsGLE and extended dynamics, the <span class="search-hit mathjax">memory</span> kernel is parameterized in a two-time exponential expansion. A data-driven hybrid <span class="search-hit mathjax">optimization</span> process is proposed for the parameterization, a non-convex and high-dimensional <span class="search-hit mathjax">optimization</span> problem.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.10381v2-abstract-full').style.display = 'none'; document.getElementById('2103.10381v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Large part of this paper needs to be revised</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.09019">arXiv:2103.09019</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.09019">pdf</a>, <a href="https://arxiv.org/format/2103.09019">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.jpdc.2021.02.010">10.1016/j.jpdc.2021.02.010 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intelligent colocation of HPC workloads
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zacarias%2C+F+V">Felippe V. Zacarias</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petrucci%2C+V">Vinicius Petrucci</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nishtala%2C+R">Rajiv Nishtala</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carpenter%2C+P">Paul Carpenter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moss%C3%A9%2C+D">Daniel MossÃ©</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.09019v1-abstract-short" style="display: inline;">
        Many HPC <span class="search-hit mathjax">applications</span> suffer from a bottleneck in the shared caches, instruction execution units, I/O or&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09019v1-abstract-full').style.display = 'inline'; document.getElementById('2103.09019v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.09019v1-abstract-full" style="display: none;">
        Many HPC <span class="search-hit mathjax">applications</span> suffer from a bottleneck in the shared caches, instruction execution units, I/O or <span class="search-hit mathjax">memory</span> bandwidth, even though the remaining resources may be underutilized. It is hard for developers and runtime systems to ensure that all critical resources are fully exploited by a single <span class="search-hit mathjax">application</span>, so an attractive technique for increasing HPC system utilization is to colocate multiple <span class="search-hit mathjax">applications</span> on the same server. When <span class="search-hit mathjax">applications</span> share critical resources, however, contention on shared resources may lead to <span class="search-hit mathjax">reduced</span> <span class="search-hit mathjax">application</span> performance.
  In this paper, we show that server efficiency can be <span class="search-hit mathjax">improved</span> by first modeling the expected performance degradation of colocated <span class="search-hit mathjax">applications</span> based on measured hardware performance counters, and then exploiting the model to determine an <span class="search-hit mathjax">optimized</span> mix of colocated <span class="search-hit mathjax">applications</span>. This paper presents a new intelligent resource manager and makes the following contributions: (1) a new machine learning model to predict the performance degradation of colocated <span class="search-hit mathjax">applications</span> based on hardware counters and (2) an intelligent scheduling scheme deployed on an existing resource manager to enable <span class="search-hit mathjax">application</span> co-scheduling with minimum performance degradation. Our results show that our approach achieves performance <span class="search-hit mathjax">improvements</span> of 7% (avg) and 12% (max) compared to the standard policy commonly used by existing job managers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09019v1-abstract-full').style.display = 'none'; document.getElementById('2103.09019v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to Journal of Parallel and Distributed Computing</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Volume 151, May 2021, Pages 125-137
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.08457">arXiv:2103.08457</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.08457">pdf</a>, <a href="https://arxiv.org/format/2103.08457">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RANP: Resource Aware Neuron Pruning at Initialization for 3D CNNs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+Z">Zhiwei Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ajanthan%2C+T">Thalaiyasingam Ajanthan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vineet%2C+V">Vibhav Vineet</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hartley%2C+R">Richard Hartley</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.08457v1-abstract-short" style="display: inline;">
        Although 3D Convolutional Neural Networks are essential for most learning based <span class="search-hit mathjax">applications</span> involving dense 3D data, their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.08457v1-abstract-full').style.display = 'inline'; document.getElementById('2103.08457v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.08457v1-abstract-full" style="display: none;">
        Although 3D Convolutional Neural Networks are essential for most learning based <span class="search-hit mathjax">applications</span> involving dense 3D data, their <span class="search-hit mathjax">applicability</span> is limited due to excessive <span class="search-hit mathjax">memory</span> and computational requirements. Compressing such networks by pruning therefore becomes highly desirable. However, pruning 3D CNNs is largely unexplored possibly because of the complex nature of typical pruning algorithms that embeds pruning into an iterative <span class="search-hit mathjax">optimization</span> paradigm. In this work, we introduce a Resource Aware Neuron Pruning (RANP) algorithm that prunes 3D CNNs at initialization to high sparsity levels. Specifically, the core idea is to obtain an importance score for each neuron based on their sensitivity to the loss function. This neuron importance is then reweighted according to the neuron resource consumption related to FLOPs or <span class="search-hit mathjax">memory</span>. We demonstrate the effectiveness of our pruning method on 3D semantic segmentation with widely used 3D-UNets on ShapeNet and BraTS&#39;18 datasets, video classification with MobileNetV2 and I3D on UCF101 dataset, and two-view stereo matching with Pyramid Stereo Matching (PSM) network on SceneFlow dataset. In these experiments, our RANP leads to roughly 50%-95% reduction in FLOPs and 35%-80% reduction in <span class="search-hit mathjax">memory</span> with negligible loss in accuracy compared to the unpruned networks. This significantly <span class="search-hit mathjax">reduces</span> the computational resources required to train 3D CNNs. The pruned network obtained by our algorithm can also be easily scaled up and transferred to another dataset for training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.08457v1-abstract-full').style.display = 'none'; document.getElementById('2103.08457v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">this is an extension of our 3DV2020 conference paper RANP. arXiv admin note: substantial text overlap with arXiv:2010.02488</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.10707">arXiv:2102.10707</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.10707">pdf</a>, <a href="https://arxiv.org/format/2102.10707">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box <span class="search-hit mathjax">Optimization</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">HanQin Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lou%2C+Y">Yuchen Lou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McKenzie%2C+D">Daniel McKenzie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+W">Wotao Yin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.10707v2-abstract-short" style="display: inline;">
        We consider the zeroth-order <span class="search-hit mathjax">optimization</span> problem in the huge-scale setting, where the dimension of the problem is so large that performing even basic vector operations on the decision variables is infeasible. In this paper, we propose a novel algorithm, coined ZO-BCD, that exhibits favorable overall query complexity and has a much smaller per-iteration comp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.10707v2-abstract-full').style.display = 'inline'; document.getElementById('2102.10707v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.10707v2-abstract-full" style="display: none;">
        We consider the zeroth-order <span class="search-hit mathjax">optimization</span> problem in the huge-scale setting, where the dimension of the problem is so large that performing even basic vector operations on the decision variables is infeasible. In this paper, we propose a novel algorithm, coined ZO-BCD, that exhibits favorable overall query complexity and has a much smaller per-iteration computational complexity. In addition, we discuss how the <span class="search-hit mathjax">memory</span> footprint of ZO-BCD can be <span class="search-hit mathjax">reduced</span> even further by the clever use of circulant measurement matrices. As an <span class="search-hit mathjax">application</span> of our new method, we propose the idea of crafting adversarial attacks on neural network based classifiers in a wavelet domain, which can result in problem dimensions of over 1.7 million. In particular, we show that crafting adversarial examples to audio classifiers in a wavelet domain can achieve the state-of-the-art attack success rate of 97.9%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.10707v2-abstract-full').style.display = 'none'; document.getElementById('2102.10707v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICML 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.07725">arXiv:2102.07725</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.07725">pdf</a>, <a href="https://arxiv.org/format/2102.07725">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Neural Network Compression for Noisy Storage Devices
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Isik%2C+B">Berivan Isik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Choi%2C+K">Kristy Choi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+X">Xin Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Weissman%2C+T">Tsachy Weissman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ermon%2C+S">Stefano Ermon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wong%2C+H+-+P">H. -S. Philip Wong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alaghi%2C+A">Armin Alaghi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.07725v1-abstract-short" style="display: inline;">
        Compression and efficient storage of neural network (NN) parameters is critical for <span class="search-hit mathjax">applications</span> that run on resource-constrained devices. Although NN model compression has made significant progress, there has been considerably less investigation in the actual physical storage of NN parameters. Conventionally, model compression and physical storage are decou&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.07725v1-abstract-full').style.display = 'inline'; document.getElementById('2102.07725v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.07725v1-abstract-full" style="display: none;">
        Compression and efficient storage of neural network (NN) parameters is critical for <span class="search-hit mathjax">applications</span> that run on resource-constrained devices. Although NN model compression has made significant progress, there has been considerably less investigation in the actual physical storage of NN parameters. Conventionally, model compression and physical storage are decoupled, as digital storage media with error correcting <span class="search-hit mathjax">codes</span> (ECCs) provide robust error-free storage. This decoupled approach is inefficient, as it forces the storage to treat each bit of the compressed model equally, and to dedicate the same amount of resources to each bit. We propose a radically different approach that: (i) employs analog <span class="search-hit mathjax">memories</span> to maximize the capacity of each <span class="search-hit mathjax">memory</span> cell, and (ii) jointly <span class="search-hit mathjax">optimizes</span> model compression and physical storage to maximize <span class="search-hit mathjax">memory</span> utility. We investigate the challenges of analog storage by studying model storage on phase change <span class="search-hit mathjax">memory</span> (PCM) arrays and develop a variety of robust <span class="search-hit mathjax">coding</span> strategies for NN model storage. We demonstrate the efficacy of our approach on MNIST, CIFAR-10 and ImageNet datasets for both existing and novel compression methods. Compared to conventional error-free digital storage, our method has the potential to <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">memory</span> size by one order of magnitude, without significantly compromising the stored model&#39;s accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.07725v1-abstract-full').style.display = 'none'; document.getElementById('2102.07725v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">19 pages, 9 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.00755">arXiv:2102.00755</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.00755">pdf</a>, <a href="https://arxiv.org/format/2102.00755">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        High Resolution 3D Ultrasonic Breast Imaging by Time-Domain Full Waveform Inversion
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lucka%2C+F">Felix Lucka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=P%C3%A9rez-Liva%2C+M">Mailyn PÃ©rez-Liva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Treeby%2C+B+E">Bradley E. Treeby</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cox%2C+B+T">Ben T. Cox</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.00755v3-abstract-short" style="display: inline;">
        Ultrasound tomography (UST) scanners allow quantitative images of the human breast&#39;s acoustic properties to be derived with potential <span class="search-hit mathjax">applications</span> in screening, diagnosis and therapy planning. Time domain full waveform inversion (TD-FWI) is a promising UST image formation technique that fits the parameter fields of a wave physics model by gradient-based&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.00755v3-abstract-full').style.display = 'inline'; document.getElementById('2102.00755v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.00755v3-abstract-full" style="display: none;">
        Ultrasound tomography (UST) scanners allow quantitative images of the human breast&#39;s acoustic properties to be derived with potential <span class="search-hit mathjax">applications</span> in screening, diagnosis and therapy planning. Time domain full waveform inversion (TD-FWI) is a promising UST image formation technique that fits the parameter fields of a wave physics model by gradient-based <span class="search-hit mathjax">optimization</span>. For high resolution 3D UST, it holds three key challenges: Firstly, its central building block, the computation of the gradient for a single US measurement, has a restrictively large <span class="search-hit mathjax">memory</span> footprint. Secondly, this building block needs to be computed for each of the $10^3-10^4$ measurements, resulting in a massive parallel computation usually performed on large computational clusters for days. Lastly, the structure of the underlying <span class="search-hit mathjax">optimization</span> problem may result in slow progression of the solver and convergence to a local minimum. In this work, we design and evaluate a comprehensive computational strategy to overcome these challenges: Firstly, we introduce a novel gradient computation based on time reversal that dramatically <span class="search-hit mathjax">reduces</span> the <span class="search-hit mathjax">memory</span> footprint at the expense of one additional wave simulation per source. Secondly, we break the dependence on the number of measurements by using source encoding (SE) to compute stochastic gradient estimates. Also we describe a more accurate, TD-specific SE technique with a finer variance control and use a state-of-the-art stochastic LBFGS method. Lastly, we design an efficient TD multi-grid scheme together with preconditioning to speed up the convergence while avoiding local minima. All components are evaluated in extensive numerical proof-of-concept studies simulating a bowl-shaped 3D UST breast scanner prototype. Finally, we demonstrate that their combination allows us to obtain an accurate 442x442x222 voxel image with a resolution of 0.5mm using Matlab on a single GPU within 24h.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.00755v3-abstract-full').style.display = 'none'; document.getElementById('2102.00755v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.10444">arXiv:2101.10444</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.10444">pdf</a>, <a href="https://arxiv.org/ps/2101.10444">ps</a>, <a href="https://arxiv.org/format/2101.10444">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GnetSeg: Semantic Segmentation Model <span class="search-hit mathjax">Optimized</span> on a 224mW CNN Accelerator Chip at the Speed of 318FPS
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+B">Baohua Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+W">Weixiong Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sha%2C+H">Hao Sha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+J">Jiapeng Su</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.10444v1-abstract-short" style="display: inline;">
        Semantic segmentation is the task to cluster pixels on an image belonging to the same class. It is widely used in the real-world <span class="search-hit mathjax">applications</span> including autonomous driving, medical imaging analysis, industrial inspection, smartphone camera for person segmentation and so on. Accelerating the semantic segmentation models on the mobile and edge devices are pract&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.10444v1-abstract-full').style.display = 'inline'; document.getElementById('2101.10444v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.10444v1-abstract-full" style="display: none;">
        Semantic segmentation is the task to cluster pixels on an image belonging to the same class. It is widely used in the real-world <span class="search-hit mathjax">applications</span> including autonomous driving, medical imaging analysis, industrial inspection, smartphone camera for person segmentation and so on. Accelerating the semantic segmentation models on the mobile and edge devices are practical needs for the industry. Recent years have witnessed the wide availability of CNN (Convolutional Neural Networks) accelerators. They have the advantages on power efficiency, inference speed, which are ideal for accelerating the semantic segmentation models on the edge devices. However, the CNN accelerator chips also have the limitations on flexibility and <span class="search-hit mathjax">memory</span>. In addition, the CPU load is very critical because the CNN accelerator chip works as a co-processor with a host CPU. In this paper, we <span class="search-hit mathjax">optimize</span> the semantic segmentation model in order to fully utilize the limited <span class="search-hit mathjax">memory</span> and the supported operators on the CNN accelerator chips, and at the same time <span class="search-hit mathjax">reduce</span> the CPU load of the CNN model to zero. The resulting model is called GnetSeg. Furthermore, we propose the integer encoding for the mask of the GnetSeg model, which minimizes the latency of data transfer between the CNN accelerator and the host CPU. The experimental result shows that the model running on the 224mW chip achieves the speed of 318FPS with excellent accuracy for <span class="search-hit mathjax">applications</span> such as person segmentation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.10444v1-abstract-full').style.display = 'none'; document.getElementById('2101.10444v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 3 figures, and 2 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.08458">arXiv:2101.08458</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.08458">pdf</a>, <a href="https://arxiv.org/format/2101.08458">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/CGO51591.2021.9370330">10.1109/CGO51591.2021.9370330 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        UNIT: Unifying Tensorized Instruction Compilation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Weng%2C+J">Jian Weng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jain%2C+A">Animesh Jain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jie Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Leyuan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yida Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nowatzki%2C+T">Tony Nowatzki</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.08458v3-abstract-short" style="display: inline;">
        Because of the increasing demand for computation in DNN, researchers develope both hardware and <span class="search-hit mathjax">software</span> mechanisms to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.08458v3-abstract-full').style.display = 'inline'; document.getElementById('2101.08458v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.08458v3-abstract-full" style="display: none;">
        Because of the increasing demand for computation in DNN, researchers develope both hardware and <span class="search-hit mathjax">software</span> mechanisms to <span class="search-hit mathjax">reduce</span> the compute and <span class="search-hit mathjax">memory</span> burden. A widely adopted approach is to use mixed precision data types. However, it is hard to leverage mixed precision without hardware support because of the overhead of data casting. Hardware vendors offer tensorized instructions for mixed-precision tensor operations, like Intel VNNI, Tensor Core, and ARM-DOT. These instructions involve a computing idiom that <span class="search-hit mathjax">reduces</span> multiple low precision elements into one high precision element. The lack of compilation techniques for this makes it hard to utilize these instructions: Using vendor-provided libraries for computationally-intensive kernels is inflexible and prevents further <span class="search-hit mathjax">optimizations</span>, and manually writing hardware intrinsics is error-prone and difficult for programmers. Some prior works address this problem by creating compilers for each instruction. This requires excessive effort when it comes to many tensorized instructions. In this work, we develop a compiler framework to unify the compilation for these instructions -- a unified semantics abstraction eases the integration of new instructions, and reuses the analysis and transformations. Tensorized instructions from different platforms can be compiled via UNIT with moderate effort for favorable performance. Given a tensorized instruction and a tensor operation, UNIT <span class="search-hit mathjax">automatically</span> detects the <span class="search-hit mathjax">applicability</span>, transforms the loop organization of the operation,and rewrites the loop body to leverage the tensorized instruction. According to our evaluation, UNIT can target various mainstream hardware platforms. The generated end-to-end inference model achieves 1.3x speedup over Intel oneDNN on an x86 CPU, 1.75x speedup over Nvidia cuDNN on an NvidiaGPU, and 1.13x speedup over a carefully tuned TVM solution for ARM DOT on an ARM CPU.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.08458v3-abstract-full').style.display = 'none'; document.getElementById('2101.08458v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 13 figures, and 1 table</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2021 IEEE/ACM International Symposium on <span class="search-hit mathjax">Code</span> Generation and <span class="search-hit mathjax">Optimization</span> (CGO), Seoul, Korea (South), 2021, pp. 77-89
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.07327">arXiv:2101.07327</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.07327">pdf</a>, <a href="https://arxiv.org/format/2101.07327">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OpenUVR: an Open-Source System Framework for Untethered Virtual Reality <span class="search-hit mathjax">Applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rohloff%2C+A">Alec Rohloff</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Allen%2C+Z">Zackary Allen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+K">Kung-Min Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Okrend%2C+J">Joshua Okrend</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nie%2C+C">Chengyi Nie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yu-Chia Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tseng%2C+H">Hung-Wei Tseng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.07327v1-abstract-short" style="display: inline;">
        Advancements in heterogeneous computing technologies enable the significant potential of virtual reality (VR) <span class="search-hit mathjax">applications</span>. To offer the best user experience (UX), a system should adopt an untethered, wireless-network-based architecture to transfer VR content between the user and the content generator. However, modern wireless network technologies make imple&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07327v1-abstract-full').style.display = 'inline'; document.getElementById('2101.07327v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.07327v1-abstract-full" style="display: none;">
        Advancements in heterogeneous computing technologies enable the significant potential of virtual reality (VR) <span class="search-hit mathjax">applications</span>. To offer the best user experience (UX), a system should adopt an untethered, wireless-network-based architecture to transfer VR content between the user and the content generator. However, modern wireless network technologies make implementing such an architecture challenging, as VR <span class="search-hit mathjax">applications</span> require superior video quality -- with high resolution, high frame rates, and very low latency.
  This paper presents OpenUVR, an open-source framework that uses commodity hardware components to satisfy the demands of interactive, real-time VR <span class="search-hit mathjax">applications</span>. OpenUVR significantly <span class="search-hit mathjax">improves</span> UX through a redesign of the system stack and addresses the most time-sensitive issues associated with redundant <span class="search-hit mathjax">memory</span> copying in modern computing systems. OpenUVR presents a cross-layered VR datapath to avoid redundant data operations and computation among system components, OpenUVR customizes the network stack to eliminate unnecessary <span class="search-hit mathjax">memory</span> operations incurred by mismatching data formats in each layer, and OpenUVR uses feedback from mobile devices to remove <span class="search-hit mathjax">memory</span> buffers.
  Together, these modifications allow OpenUVR to <span class="search-hit mathjax">reduce</span> VR <span class="search-hit mathjax">application</span> delays to 14.32 ms, meeting the 20 ms minimum latency in avoiding motion sickness. As an open-source system that is fully compatible with commodity hardware, OpenUVR offers the research community an opportunity to develop, investigate, and <span class="search-hit mathjax">optimize</span> <span class="search-hit mathjax">applications</span> for untethered, high-performance VR architectures.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07327v1-abstract-full').style.display = 'none'; document.getElementById('2101.07327v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.00745">arXiv:2101.00745</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.00745">pdf</a>, <a href="https://arxiv.org/format/2101.00745">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DSXplore: <span class="search-hit mathjax">Optimizing</span> Convolutional Neural Networks via Sliding-Channel Convolutions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuke Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+B">Boyuan Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+Y">Yufei Ding</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.00745v1-abstract-short" style="display: inline;">
        As the key advancement of the convolutional neural networks (CNNs), depthwise separable convolutions (DSCs) are becoming one of the most popular techniques to <span class="search-hit mathjax">reduce</span> the computations and parameters size of CNNs meanwhile maintaining the model accuracy. It also brings profound impact to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00745v1-abstract-full').style.display = 'inline'; document.getElementById('2101.00745v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.00745v1-abstract-full" style="display: none;">
        As the key advancement of the convolutional neural networks (CNNs), depthwise separable convolutions (DSCs) are becoming one of the most popular techniques to <span class="search-hit mathjax">reduce</span> the computations and parameters size of CNNs meanwhile maintaining the model accuracy. It also brings profound impact to <span class="search-hit mathjax">improve</span> the <span class="search-hit mathjax">applicability</span> of the compute- and <span class="search-hit mathjax">memory</span>-intensive CNNs to a broad range of <span class="search-hit mathjax">applications</span>, such as mobile devices, which are generally short of computation power and <span class="search-hit mathjax">memory</span>. However, previous research in DSCs are largely focusing on compositing the limited existing DSC designs, thus, missing the opportunities to explore more potential designs that can achieve better accuracy and higher computation/parameter reduction. Besides, the off-the-shelf convolution implementations offer limited computing schemes, therefore, lacking support for DSCs with different convolution patterns.
  To this end, we introduce, DSXplore, the first <span class="search-hit mathjax">optimized</span> design for exploring DSCs on CNNs. Specifically, at the algorithm level, DSXplore incorporates a novel factorized kernel -- sliding-channel convolution (SCC), featured with input-channel overlapping to balance the accuracy performance and the reduction of computation and <span class="search-hit mathjax">memory</span> cost. SCC also offers enormous space for design exploration by introducing adjustable kernel parameters. Further, at the implementation level, we carry out an <span class="search-hit mathjax">optimized</span> GPU-implementation tailored for SCC by leveraging several key techniques, such as the input-centric backward design and the channel-cyclic <span class="search-hit mathjax">optimization</span>. Intensive experiments on different datasets across mainstream CNNs show the advantages of DSXplore in balancing accuracy and computation/parameter reduction over the standard convolution and the existing DSCs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00745v1-abstract-full').style.display = 'none'; document.getElementById('2101.00745v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.13600">arXiv:2012.13600</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.13600">pdf</a>, <a href="https://arxiv.org/format/2012.13600">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/JETCAS.2020.3040300">10.1109/JETCAS.2020.3040300 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EdgeDRNN: Recurrent Neural Network Accelerator for Edge Inference
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+C">Chang Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rios-Navarro%2C+A">Antonio Rios-Navarro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xi Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shih-Chii Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delbruck%2C+T">Tobi Delbruck</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.13600v1-abstract-short" style="display: inline;">
        Low-latency, low-power portable recurrent neural network (RNN) accelerators offer powerful inference capabilities for real-time <span class="search-hit mathjax">applications</span> such as IoT, robotics, and human-machine interaction. We propose a lightweight Gated Recurrent Unit (GRU)-based RNN accelerator called EdgeDRNN that is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.13600v1-abstract-full').style.display = 'inline'; document.getElementById('2012.13600v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.13600v1-abstract-full" style="display: none;">
        Low-latency, low-power portable recurrent neural network (RNN) accelerators offer powerful inference capabilities for real-time <span class="search-hit mathjax">applications</span> such as IoT, robotics, and human-machine interaction. We propose a lightweight Gated Recurrent Unit (GRU)-based RNN accelerator called EdgeDRNN that is <span class="search-hit mathjax">optimized</span> for low-latency edge RNN inference with batch size of 1. EdgeDRNN adopts the spiking neural network inspired delta network algorithm to exploit temporal sparsity in RNNs. Weights are stored in inexpensive DRAM which enables EdgeDRNN to compute large multi-layer RNNs on the most inexpensive FPGA. The sparse updates <span class="search-hit mathjax">reduce</span> DRAM weight <span class="search-hit mathjax">memory</span> access by a factor of up to 10x and the delta can be varied dynamically to trade-off between latency and accuracy. EdgeDRNN updates a 5 million parameter 2-layer GRU-RNN in about 0.5ms. It achieves latency comparable with a 92W Nvidia 1080 GPU. It outperforms NVIDIA Jetson Nano, Jetson TX2 and Intel Neural Compute Stick 2 in latency by 5X. For a batch size of 1, EdgeDRNN achieves a mean effective throughput of 20.2GOp/s and a wall plug power efficiency that is over 4X higher than the commercial edge AI platforms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.13600v1-abstract-full').style.display = 'none'; document.getElementById('2012.13600v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        in IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol. 10, no. 4, pp. 419-432, Dec. 2020
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.09852">arXiv:2012.09852</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.09852">pdf</a>, <a href="https://arxiv.org/format/2012.09852">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hanrui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhekai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+S">Song Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.09852v2-abstract-short" style="display: inline;">
        The attention mechanism is becoming increasingly popular in Natural Language Processing (NLP) <span class="search-hit mathjax">applications</span>, showing superior performance than convolutional and recurrent architectures. However, general-purpose platforms such as CPUs and GPUs are inefficient when performing attention inference due to complicated data movement and low arithmetic intensity. Mor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.09852v2-abstract-full').style.display = 'inline'; document.getElementById('2012.09852v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.09852v2-abstract-full" style="display: none;">
        The attention mechanism is becoming increasingly popular in Natural Language Processing (NLP) <span class="search-hit mathjax">applications</span>, showing superior performance than convolutional and recurrent architectures. However, general-purpose platforms such as CPUs and GPUs are inefficient when performing attention inference due to complicated data movement and low arithmetic intensity. Moreover, existing NN accelerators mainly focus on <span class="search-hit mathjax">optimizing</span> convolutional or recurrent models, and cannot efficiently support attention. In this paper, we present SpAtten, an efficient algorithm-architecture co-design that leverages token sparsity, head sparsity, and quantization opportunities to <span class="search-hit mathjax">reduce</span> the attention computation and <span class="search-hit mathjax">memory</span> access. Inspired by the high redundancy of human languages, we propose the novel cascade token pruning to prune away unimportant tokens in the sentence. We also propose cascade head pruning to remove unessential heads. Cascade pruning is fundamentally different from weight pruning since there is no trainable weight in the attention mechanism, and the pruned tokens and heads are selected on the fly. To efficiently support them on hardware, we design a novel top-k engine to rank token and head importance scores with high throughput. Furthermore, we propose progressive quantization that first fetches MSBs only and performs the computation; if the confidence is low, it fetches LSBs and recomputes the attention outputs, trading computation for <span class="search-hit mathjax">memory</span> reduction.
  Extensive experiments on 30 benchmarks show that, on average, SpAtten <span class="search-hit mathjax">reduces</span> DRAM access by 10.0x with no accuracy loss, and achieves 1.6x, 3.0x, 162x, 347x speedup, and 1,4x, 3.2x, 1193x, 4059x energy savings over A3 accelerator, MNNFast accelerator, TITAN Xp GPU, Xeon CPU, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.09852v2-abstract-full').style.display = 'none'; document.getElementById('2012.09852v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 December, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper in HPCA 2021; 15 pages, 23 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.01714">arXiv:2012.01714</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.01714">pdf</a>, <a href="https://arxiv.org/format/2012.01714">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AutoInt: <span class="search-hit mathjax">Automatic</span> Integration for Fast Neural Volume Rendering
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lindell%2C+D+B">David B. Lindell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martel%2C+J+N+P">Julien N. P. Martel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wetzstein%2C+G">Gordon Wetzstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.01714v2-abstract-short" style="display: inline;">
        Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision <span class="search-hit mathjax">applications</span>. Among these&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.01714v2-abstract-full').style.display = 'inline'; document.getElementById('2012.01714v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.01714v2-abstract-full" style="display: none;">
        Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision <span class="search-hit mathjax">applications</span>. Among these <span class="search-hit mathjax">applications</span>, neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and <span class="search-hit mathjax">memory</span> requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose <span class="search-hit mathjax">automatic</span> integration, a new framework for learning efficient, closed-form solutions to integrals using coordinate-based neural networks. For training, we instantiate the computational graph corresponding to the derivative of the network. The graph is fitted to the signal to integrate. After <span class="search-hit mathjax">optimization</span>, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Applying this approach to neural rendering, we <span class="search-hit mathjax">improve</span> a tradeoff between rendering speed and image quality: <span class="search-hit mathjax">improving</span> render times by greater than 10 times with a tradeoff of slightly <span class="search-hit mathjax">reduced</span> image quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.01714v2-abstract-full').style.display = 'none'; document.getElementById('2012.01714v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 December, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.00506">arXiv:2012.00506</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.00506">pdf</a>, <a href="https://arxiv.org/format/2012.00506">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Parallel Direct Eigensolver for Sequences of Hermitian Eigenvalue Problems with No Tridiagonalization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Shengguo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xinzhe Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roman%2C+J+E">Jose E. Roman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+Z">Ziyang Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+L">Lizhi Cheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.00506v1-abstract-short" style="display: inline;">
        &hellip;Eigenvalue Problems with no tridiagonalization is proposed, denoted by \texttt{PDESHEP}, and it combines direct methods with iterative methods. \texttt{PDESHEP} first <span class="search-hit mathjax">reduces</span> a Hermitian matrix to its banded form, then applies a spectrum slicing algorithm to the banded matrix, and finally computes the eigenvectors of the original matrix via backtransform. Th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.00506v1-abstract-full').style.display = 'inline'; document.getElementById('2012.00506v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.00506v1-abstract-full" style="display: none;">
        In this paper, a Parallel Direct Eigensolver for Sequences of Hermitian Eigenvalue Problems with no tridiagonalization is proposed, denoted by \texttt{PDESHEP}, and it combines direct methods with iterative methods. \texttt{PDESHEP} first <span class="search-hit mathjax">reduces</span> a Hermitian matrix to its banded form, then applies a spectrum slicing algorithm to the banded matrix, and finally computes the eigenvectors of the original matrix via backtransform. Therefore, compared with conventional direct eigensolvers, \texttt{PDESHEP} avoids tridiagonalization, which consists of many <span class="search-hit mathjax">memory</span>-bounded operations. In this work, the iterative method in \texttt{PDESHEP} is based on the contour integral method implemented in FEAST. The combination of direct methods with iterative methods for banded matrices requires some efficient data redistribution algorithms both from 2D to 1D and from 1D to 2D data structures. Hence, some two-step data redistribution algorithms are proposed, which can be $10\times$ faster than ScaLAPACK routine \texttt{PXGEMR2D}. For the symmetric self-consistent field (SCF) eigenvalue problems, \texttt{PDESHEP} can be on average $1.25\times$ faster than the state-of-the-art direct solver in ELPA when using $4096$ processes. Numerical results are obtained for dense Hermitian matrices from real <span class="search-hit mathjax">applications</span> and large real sparse matrices from the SuiteSparse collection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.00506v1-abstract-full').style.display = 'none'; document.getElementById('2012.00506v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">24 pages and 14 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.11188">arXiv:2011.11188</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.11188">pdf</a>, <a href="https://arxiv.org/format/2011.11188">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Integrating Deep Learning in Domain Sciences at Exascale
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Archibald%2C+R">Rick Archibald</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chow%2C+E">Edmond Chow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=D%27Azevedo%2C+E">Eduardo D&#39;Azevedo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dongarra%2C+J">Jack Dongarra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Eisenbach%2C+M">Markus Eisenbach</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Febbo%2C+R">Rocco Febbo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lopez%2C+F">Florent Lopez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nichols%2C+D">Daniel Nichols</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tomov%2C+S">Stanimire Tomov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wong%2C+K">Kwai Wong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+J">Junqi Yin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.11188v1-abstract-short" style="display: inline;">
        &hellip;(AI) and integrating it with traditional high-performance computing (HPC) simulations. We evaluate existing packages for their ability to run deep learning models and <span class="search-hit mathjax">applications</span> on large-scale HPC systems efficiently, identify challenges, and propose new asynchronous parallelization and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.11188v1-abstract-full').style.display = 'inline'; document.getElementById('2011.11188v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.11188v1-abstract-full" style="display: none;">
        This paper presents some of the current challenges in designing deep learning artificial intelligence (AI) and integrating it with traditional high-performance computing (HPC) simulations. We evaluate existing packages for their ability to run deep learning models and <span class="search-hit mathjax">applications</span> on large-scale HPC systems efficiently, identify challenges, and propose new asynchronous parallelization and <span class="search-hit mathjax">optimization</span> techniques for current large-scale heterogeneous systems and upcoming exascale systems. These developments, along with existing HPC AI <span class="search-hit mathjax">software</span> capabilities, have been integrated into MagmaDNN, an open-source HPC deep learning framework. Many deep learning frameworks are targeted at data scientists and fall short in providing quality integration into existing HPC workflows. This paper discusses the necessities of an HPC deep learning framework and how those needs can be provided (e.g., as in MagmaDNN) through a deep integration with existing HPC libraries, such as MAGMA and its modular <span class="search-hit mathjax">memory</span> management, MPI, CuBLAS, CuDNN, MKL, and HIP. Advancements are also illustrated through the use of algorithmic enhancements in <span class="search-hit mathjax">reduced</span>- and mixed-precision, as well as asynchronous <span class="search-hit mathjax">optimization</span> methods. Finally, we present illustrations and potential solutions for enhancing traditional compute- and data-intensive <span class="search-hit mathjax">applications</span> at ORNL and UTK with AI. The approaches and future challenges are illustrated in materials science, imaging, and climate <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.11188v1-abstract-full').style.display = 'none'; document.getElementById('2011.11188v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.08353">arXiv:2011.08353</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.08353">pdf</a>, <a href="https://arxiv.org/format/2011.08353">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AXES: Approximation Manager for Emerging <span class="search-hit mathjax">Memory</span> Architectures
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Maity%2C+B">Biswadip Maity</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Donyanavard%2C+B">Bryan Donyanavard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Surhonne%2C+A">Anmol Surhonne</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rahmani%2C+A">Amir Rahmani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Herkersdorf%2C+A">Andreas Herkersdorf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dutt%2C+N">Nikil Dutt</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.08353v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Memory</span> approximation techniques are commonly limited in scope, targeting individual levels of the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.08353v1-abstract-full').style.display = 'inline'; document.getElementById('2011.08353v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.08353v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Memory</span> approximation techniques are commonly limited in scope, targeting individual levels of the <span class="search-hit mathjax">memory</span> hierarchy. Existing approximation techniques for a full <span class="search-hit mathjax">memory</span> hierarchy determine <span class="search-hit mathjax">optimal</span> configurations at design-time provided a goal and <span class="search-hit mathjax">application</span>. Such policies are rigid: they cannot adapt to unknown workloads and must be redesigned for different <span class="search-hit mathjax">memory</span> configurations and technologies. We propose AXES: the first self-<span class="search-hit mathjax">optimizing</span> runtime manager for coordinating configurable approximation knobs across all levels of the <span class="search-hit mathjax">memory</span> hierarchy. AXES continuously updates and <span class="search-hit mathjax">optimizes</span> its approximation management policy throughout runtime for diverse workloads. AXES <span class="search-hit mathjax">optimizes</span> the approximate <span class="search-hit mathjax">memory</span> configuration to minimize power consumption without compromising the quality threshold specified by <span class="search-hit mathjax">application</span> developers. AXES can (1) learn a policy at runtime to manage variable <span class="search-hit mathjax">application</span> quality of service (QoS) constraints, (2) <span class="search-hit mathjax">automatically</span> <span class="search-hit mathjax">optimize</span> for a target metric within those constraints, and (3) coordinate runtime decisions for interdependent knobs and subsystems. We demonstrate AXES&#39; ability to efficiently provide functions 1-3 on a RISC-V Linux platform with approximate <span class="search-hit mathjax">memory</span> segments in the on-chip cache and main <span class="search-hit mathjax">memory</span>. We demonstrate AXES&#39; ability to save up to 37% energy in the <span class="search-hit mathjax">memory</span> subsystem without any design-time overhead. We show AXES&#39; ability to <span class="search-hit mathjax">reduce</span> QoS violations by 75% with $&lt;5\%$ additional energy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.08353v1-abstract-full').style.display = 'none'; document.getElementById('2011.08353v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">21 pages, 13 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.07160">arXiv:2011.07160</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.07160">pdf</a>, <a href="https://arxiv.org/format/2011.07160">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Phoebe: Reuse-Aware Online Caching with Reinforcement Learning for Emerging Storage Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+N">Nan Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+P">Pengcheng Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.07160v1-abstract-short" style="display: inline;">
        &hellip;with high-performance adoption of these technologies is how to properly define intelligent cache layers such that the performance gap between emerging technologies and main <span class="search-hit mathjax">memory</span> can be well bridged. To this end, we propose Phoebe, a reuse-aware reinforcement learning framework for the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.07160v1-abstract-full').style.display = 'inline'; document.getElementById('2011.07160v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.07160v1-abstract-full" style="display: none;">
        With data durability, high access speed, low power efficiency and byte addressability, NVMe and SSD, which are acknowledged representatives of emerging storage technologies, have been applied broadly in many areas. However, one key issue with high-performance adoption of these technologies is how to properly define intelligent cache layers such that the performance gap between emerging technologies and main <span class="search-hit mathjax">memory</span> can be well bridged. To this end, we propose Phoebe, a reuse-aware reinforcement learning framework for the <span class="search-hit mathjax">optimal</span> online caching that is <span class="search-hit mathjax">applicable</span> for a wide range of emerging storage models. By continuous interacting with the cache environment and the data stream, Phoebe is capable to extract critical temporal data dependency and relative positional information from a single trace, becoming ever smarter over time. To <span class="search-hit mathjax">reduce</span> training overhead during online learning, we utilize periodical training to amortize costs. Phoebe is evaluated on a set of Microsoft cloud storage workloads. Experiment results show that Phoebe is able to close the gap of cache miss rate from LRU and a state-of-the-art online learning based cache policy to the Belady&#39;s <span class="search-hit mathjax">optimal</span> policy by 70.3% and 52.6%, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.07160v1-abstract-full').style.display = 'none'; document.getElementById('2011.07160v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.01850">arXiv:2011.01850</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.01850">pdf</a>, <a href="https://arxiv.org/ps/2011.01850">ps</a>, <a href="https://arxiv.org/format/2011.01850">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Improving</span> the Performance of the GMRES Method using Mixed-Precision Techniques
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lindquist%2C+N">Neil Lindquist</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luszczek%2C+P">Piotr Luszczek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dongarra%2C+J">Jack Dongarra</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.01850v1-abstract-short" style="display: inline;">
        The GMRES method is used to solve sparse, non-symmetric systems of linear equations arising from many scientific <span class="search-hit mathjax">applications</span>. The solver performance within a single node is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.01850v1-abstract-full').style.display = 'inline'; document.getElementById('2011.01850v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.01850v1-abstract-full" style="display: none;">
        The GMRES method is used to solve sparse, non-symmetric systems of linear equations arising from many scientific <span class="search-hit mathjax">applications</span>. The solver performance within a single node is <span class="search-hit mathjax">memory</span> bound, due to the low arithmetic intensity of its computational kernels. To <span class="search-hit mathjax">reduce</span> the amount of data movement, and thus, to <span class="search-hit mathjax">improve</span> performance, we investigated the effect of using a mix of single and double precision while retaining double-precision accuracy. Previous efforts have explored <span class="search-hit mathjax">reduced</span> precision in the preconditioner, but the use of <span class="search-hit mathjax">reduced</span> precision in the solver itself has received limited attention. We found that GMRES only needs double precision in computing the residual and updating the approximate solution to achieve double-precision accuracy, although it must restart after each <span class="search-hit mathjax">improvement</span> of single-precision accuracy. This finding holds for the tested orthogonalization schemes: Modified Gram-Schmidt (MGS) and Classical Gram-Schmidt with Re-orthogonalization (CGSR). Furthermore, our mixed-precision GMRES, when restarted at least once, performed 19% and 24% faster on average than double-precision GMRES for MGS and CGSR, respectively. Our implementation uses generic <span class="search-hit mathjax">programming</span> techniques to ease the burden of <span class="search-hit mathjax">coding</span> implementations for different data types. Our use of the Kokkos library allowed us to exploit parallelism and <span class="search-hit mathjax">optimize</span> data management. Additionally, KokkosKernels was used when producing performance results. In conclusion, using a mix of single and double precision in GMRES can <span class="search-hit mathjax">improve</span> performance while retaining double-precision accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.01850v1-abstract-full').style.display = 'none'; document.getElementById('2011.01850v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages. In the 17th Smoky Mountains Computational Sciences and Engineering Conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.01773">arXiv:2011.01773</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.01773">pdf</a>, <a href="https://arxiv.org/format/2011.01773">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Memory</span>-Efficient RkNN Retrieval by Nonlinear k-Distance Approximation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Obermeier%2C+S">Sandra Obermeier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berrendorf%2C+M">Max Berrendorf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kr%C3%B6ger%2C+P">Peer KrÃ¶ger</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.01773v1-abstract-short" style="display: inline;">
        The reverse k-nearest neighbor (RkNN) query is an established query type with various <span class="search-hit mathjax">applications</span> reaching from identifying highly influential objects over incrementally updating kNN graphs to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.01773v1-abstract-full').style.display = 'inline'; document.getElementById('2011.01773v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.01773v1-abstract-full" style="display: none;">
        The reverse k-nearest neighbor (RkNN) query is an established query type with various <span class="search-hit mathjax">applications</span> reaching from identifying highly influential objects over incrementally updating kNN graphs to <span class="search-hit mathjax">optimizing</span> sensor communication and outlier detection. State-of-the-art solutions exploit that the k-distances in real-world datasets often follow the power-law distribution, and bound them with linear lines in log-log space. In this work, we investigate this assumption and uncover that it is violated in regions of changing density, which we show are typical for real-life datasets. Towards a generic solution, we pose the estimation of k-distances as a regression problem. Thereby, we enable harnessing the power of the abundance of available Machine Learning models and profiting from their advancement. We propose a flexible approach which allows steering the performance-<span class="search-hit mathjax">memory</span> consumption trade-off, and in particular to find good solutions with a fixed <span class="search-hit mathjax">memory</span> budget crucial in the context of edge computing. Moreover, we show how to obtain and <span class="search-hit mathjax">improve</span> guaranteed bounds essential to exact query processing. In experiments on real-world datasets, we demonstrate how this framework can significantly <span class="search-hit mathjax">reduce</span> the index <span class="search-hit mathjax">memory</span> consumption, and strongly <span class="search-hit mathjax">reduce</span> the candidate set size. We publish our <span class="search-hit mathjax">code</span> at https://github.com/sobermeier/nonlinear-kdist.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.01773v1-abstract-full').style.display = 'none'; document.getElementById('2011.01773v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.00850">arXiv:2011.00850</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.00850">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICIIS51140.2020.9342717">10.1109/ICIIS51140.2020.9342717 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Impact of Partial Sums on Interconnect Bandwidth and <span class="search-hit mathjax">Memory</span> Accesses in a DNN Accelerator
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chandra%2C+M">Mahesh Chandra</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.00850v1-abstract-short" style="display: inline;">
        Dedicated accelerators are being designed to address the huge resource requirement of the deep neural network (DNN) <span class="search-hit mathjax">applications</span>. The power, performance and area (PPA) constraints limit the number of MACs available in these accelerators. The convolution layers which require huge number of MACs are often partitioned into multiple iterative sub-tasks. This put&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.00850v1-abstract-full').style.display = 'inline'; document.getElementById('2011.00850v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.00850v1-abstract-full" style="display: none;">
        Dedicated accelerators are being designed to address the huge resource requirement of the deep neural network (DNN) <span class="search-hit mathjax">applications</span>. The power, performance and area (PPA) constraints limit the number of MACs available in these accelerators. The convolution layers which require huge number of MACs are often partitioned into multiple iterative sub-tasks. This puts huge pressure on the available system resources such as interconnect and <span class="search-hit mathjax">memory</span> bandwidth. The <span class="search-hit mathjax">optimal</span> partitioning of the feature maps for these sub-tasks can <span class="search-hit mathjax">reduce</span> the bandwidth requirement substantially. Some accelerators avoid off-chip or interconnect transfers by implementing local <span class="search-hit mathjax">memories</span>; however, the <span class="search-hit mathjax">memory</span> accesses are still performed and a <span class="search-hit mathjax">reduced</span> bandwidth can help in saving power in such architectures. In this paper, we propose a first order analytical method to partition the feature maps for <span class="search-hit mathjax">optimal</span> bandwidth and evaluate the impact of such partitioning on the bandwidth. This bandwidth can be saved by designing an active <span class="search-hit mathjax">memory</span> controller which can perform basic arithmetic operations. It is shown that the <span class="search-hit mathjax">optimal</span> partitioning and active <span class="search-hit mathjax">memory</span> controller can achieve up to 40% bandwidth reduction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.00850v1-abstract-full').style.display = 'none'; document.getElementById('2011.00850v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2020 IEEE 15th International Conference on Industrial and Information Systems (ICIIS)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.13887">arXiv:2010.13887</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.13887">pdf</a>, <a href="https://arxiv.org/format/2010.13887">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LightSeq: A High Performance Inference Library for Transformers
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaohui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+Y">Ying Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+Y">Yang Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+M">Mingxuan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lei Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.13887v4-abstract-short" style="display: inline;">
        &hellip;their variants have achieved great success in natural language processing. Since Transformer models are huge in size, serving these models is a challenge for real industrial <span class="search-hit mathjax">applications</span>. In this paper, we propose LightSeq, a highly efficient inference library for models in the Transformer family. LightSeq includes a series of GPU&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.13887v4-abstract-full').style.display = 'inline'; document.getElementById('2010.13887v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.13887v4-abstract-full" style="display: none;">
        Transformer, BERT and their variants have achieved great success in natural language processing. Since Transformer models are huge in size, serving these models is a challenge for real industrial <span class="search-hit mathjax">applications</span>. In this paper, we propose LightSeq, a highly efficient inference library for models in the Transformer family. LightSeq includes a series of GPU <span class="search-hit mathjax">optimization</span> techniques to to streamline the computation of neural layers and to <span class="search-hit mathjax">reduce</span> <span class="search-hit mathjax">memory</span> footprint. LightSeq can easily import models trained using PyTorch and Tensorflow. Experimental results on machine translation benchmarks show that LightSeq achieves up to 14x speedup compared with TensorFlow and 1.4x compared with FasterTransformer, a concurrent CUDA implementation. The <span class="search-hit mathjax">code</span> is available at https://github.com/bytedance/lightseq.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.13887v4-abstract-full').style.display = 'none'; document.getElementById('2010.13887v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 6 figures, accepted by NAACL 2021 Industry Track</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.10248">arXiv:2010.10248</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.10248">pdf</a>, <a href="https://arxiv.org/format/2010.10248">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Temporal blocking of finite-difference stencil operators with sparse &#34;off-the-grid&#34; sources
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bisbas%2C+G">George Bisbas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luporini%2C+F">Fabio Luporini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Louboutin%2C+M">Mathias Louboutin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nelson%2C+R">Rhodri Nelson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gorman%2C+G">Gerard Gorman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kelly%2C+P+H+J">Paul H. J. Kelly</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.10248v2-abstract-short" style="display: inline;">
        Stencil kernels dominate a range of scientific <span class="search-hit mathjax">applications</span>, including seismic and medical imaging, image processing, and neural networks. Temporal blocking is a performance&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.10248v2-abstract-full').style.display = 'inline'; document.getElementById('2010.10248v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.10248v2-abstract-full" style="display: none;">
        Stencil kernels dominate a range of scientific <span class="search-hit mathjax">applications</span>, including seismic and medical imaging, image processing, and neural networks. Temporal blocking is a performance <span class="search-hit mathjax">optimization</span> that aims to <span class="search-hit mathjax">reduce</span> the required <span class="search-hit mathjax">memory</span> bandwidth of stencil computations by re-using data from the cache for multiple time steps. It has already been shown to be beneficial for this class of algorithms. However, applying temporal blocking to practical <span class="search-hit mathjax">applications</span>&#39; stencils remains challenging. These computations often consist of sparsely located operators not aligned with the computational grid (&#34;off-the-grid&#34;). Our work is motivated by modeling problems in which source injections result in wavefields that must then be measured at receivers by interpolation from the grided wavefield. The resulting data dependencies make the adoption of temporal blocking much more challenging. We propose a methodology to inspect these data dependencies and reorder the computation, leading to performance gains in stencil <span class="search-hit mathjax">codes</span> where temporal blocking has not been <span class="search-hit mathjax">applicable</span>. We implement this novel scheme in the Devito domain-specific compiler toolchain. Devito implements a domain-specific language embedded in Python to generate <span class="search-hit mathjax">optimized</span> partial differential equation solvers using the finite-difference method from high-level symbolic problem definitions. We evaluate our scheme using isotropic acoustic, anisotropic acoustic, and isotropic elastic wave propagators of industrial significance. After auto-tuning, performance evaluation shows that this enables substantial performance <span class="search-hit mathjax">improvement</span> through temporal blocking over highly-<span class="search-hit mathjax">optimized</span> vectorized spatially-blocked <span class="search-hit mathjax">code</span> of up to 1.6x.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.10248v2-abstract-full').style.display = 'none'; document.getElementById('2010.10248v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 February, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication at 35th IEEE International Parallel &amp; Distributed Processing Symposium</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.09746">arXiv:2010.09746</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.09746">pdf</a>, <a href="https://arxiv.org/format/2010.09746">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Physics">physics.comp-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fast simulation of quantum algorithms using circuit <span class="search-hit mathjax">optimization</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guerreschi%2C+G+G">Gian Giacomo Guerreschi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.09746v2-abstract-short" style="display: inline;">
        Classical simulators play a major role in the development and benchmark of quantum algorithms and practically any <span class="search-hit mathjax">software</span> framework for quantum computation provides the option of running the algorithms on simulators. However, the development of quantum simulators was substantially separated from the rest of the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.09746v2-abstract-full').style.display = 'inline'; document.getElementById('2010.09746v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.09746v2-abstract-full" style="display: none;">
        Classical simulators play a major role in the development and benchmark of quantum algorithms and practically any <span class="search-hit mathjax">software</span> framework for quantum computation provides the option of running the algorithms on simulators. However, the development of quantum simulators was substantially separated from the rest of the <span class="search-hit mathjax">software</span> frameworks which, instead, focus on usability and compilation. In practice, simulators are considered just one of many possible backends. Here, we demonstrate the advantage of co-developing and integrating simulators and compilers by proposing a specialized compiler pass to <span class="search-hit mathjax">reduce</span> the simulation time for arbitrary circuits. While the concept is broadly <span class="search-hit mathjax">applicable</span>, we present a concrete implementation based on the Intel Quantum Simulator, a high-performance distributed simulator. As part of this work, we extend its implementation with additional functionalities related to the representation of quantum states. The communication overhead is <span class="search-hit mathjax">reduced</span> by changing the order in which state amplitudes are stored in the distributed <span class="search-hit mathjax">memory</span>, a concept analogous to the distinction between local and global qubits for distributed Schroedinger-type simulators. We then implement a compiler pass to exploit the novel functionalities by introducing special instructions governing data movement as part of the quantum circuit. Those instructions target unique capabilities of simulators and have no analogue in actual quantum devices. To quantify the advantage, we compare the time required to simulate random circuits with and without our <span class="search-hit mathjax">optimization</span>. The simulation time is typically halved.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.09746v2-abstract-full').style.display = 'none'; document.getElementById('2010.09746v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 December, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.08695">arXiv:2010.08695</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.08695">pdf</a>, <a href="https://arxiv.org/format/2010.08695">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RECEIPT: REfine CoarsE-grained IndePendent Tasks for Parallel Tip decomposition of Bipartite Graphs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lakhotia%2C+K">Kartik Lakhotia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kannan%2C+R">Rajgopal Kannan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prasanna%2C+V">Viktor Prasanna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=De+Rose%2C+C+A+F">Cesar A. F. De Rose</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.08695v1-abstract-short" style="display: inline;">
        Tip decomposition is a crucial kernel for mining dense subgraphs in bipartite networks, with <span class="search-hit mathjax">applications</span> in spam detection, analysis of affiliation networks etc. It creates a hierarchy of vertex-induced subgraphs with varying densities determined by the participation of vertices in butterflies (2,2-bicliques). To build the hierarchy, existing algorithms ite&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.08695v1-abstract-full').style.display = 'inline'; document.getElementById('2010.08695v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.08695v1-abstract-full" style="display: none;">
        Tip decomposition is a crucial kernel for mining dense subgraphs in bipartite networks, with <span class="search-hit mathjax">applications</span> in spam detection, analysis of affiliation networks etc. It creates a hierarchy of vertex-induced subgraphs with varying densities determined by the participation of vertices in butterflies (2,2-bicliques). To build the hierarchy, existing algorithms iteratively follow a delete-update(peeling) process: deleting vertices with the minimum number of butterflies and correspondingly updating the butterfly count of their 2-hop neighbors. The need to explore 2-hop neighborhood renders tip-decomposition computationally very expensive. Furthermore, the inherent sequentiality in peeling only minimum butterfly vertices makes derived parallel algorithms prone to heavy synchronization.
  In this paper, we propose a novel parallel tip-decomposition algorithm -- REfine CoarsE-grained Independent Tasks (RECEIPT) that relaxes the peeling order restrictions by partitioning the vertices into multiple independent subsets that can be concurrently peeled. This enables RECEIPT to simultaneously achieve a high degree of parallelism and dramatic reduction in synchronizations. Further, RECEIPT employs a hybrid peeling strategy along with other <span class="search-hit mathjax">optimizations</span> that drastically <span class="search-hit mathjax">reduce</span> the amount of wedge exploration and execution time.
  We perform detailed experimental evaluation of RECEIPT on a shared-<span class="search-hit mathjax">memory</span> multicore server. It can process some of the largest publicly available bipartite datasets orders of magnitude faster than the state-of-the-art algorithms -- achieving up to 1100x and 64x reduction in the number of thread synchronizations and traversed wedges, respectively. Using 36 threads, RECEIPT can provide up to 17.1x self-relative speedup. Our implementation of RECEIPT is available at https://github.com/kartiklakhotia/RECEIPT.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.08695v1-abstract-full').style.display = 'none'; document.getElementById('2010.08695v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in Proceedings of VLDB Vol. 14</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.05754">arXiv:2010.05754</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.05754">pdf</a>, <a href="https://arxiv.org/format/2010.05754">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TCAD.2020.3030610">10.1109/TCAD.2020.3030610 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DESCNet: Developing Efficient Scratchpad <span class="search-hit mathjax">Memories</span> for Capsule Network Hardware
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Marchisio%2C+A">Alberto Marchisio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mrazek%2C+V">Vojtech Mrazek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hanif%2C+M+A">Muhammad Abdullah Hanif</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shafique%2C+M">Muhammad Shafique</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.05754v1-abstract-short" style="display: inline;">
        Deep Neural Networks (DNNs) have been established as the state-of-the-art algorithm for advanced machine learning <span class="search-hit mathjax">applications</span>. Recently proposed by the Google Brain&#39;s team, the Capsule Networks (CapsNets) have&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.05754v1-abstract-full').style.display = 'inline'; document.getElementById('2010.05754v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.05754v1-abstract-full" style="display: none;">
        Deep Neural Networks (DNNs) have been established as the state-of-the-art algorithm for advanced machine learning <span class="search-hit mathjax">applications</span>. Recently proposed by the Google Brain&#39;s team, the Capsule Networks (CapsNets) have <span class="search-hit mathjax">improved</span> the generalization ability, as compared to DNNs, due to their multi-dimensional capsules and preserving the spatial relationship between different objects. However, they pose significantly high computational and <span class="search-hit mathjax">memory</span> requirements, making their energy-efficient inference a challenging task. This paper provides, for the first time, an in-depth analysis to highlight the design and management related challenges for the (on-chip) <span class="search-hit mathjax">memories</span> deployed in hardware accelerators executing fast CapsNets inference. To enable an efficient design, we propose an <span class="search-hit mathjax">application</span>-specific <span class="search-hit mathjax">memory</span> hierarchy, which minimizes the off-chip <span class="search-hit mathjax">memory</span> accesses, while efficiently feeding the data to the hardware accelerator. We analyze the corresponding on-chip <span class="search-hit mathjax">memory</span> requirements and leverage it to propose a novel methodology to explore different scratchpad <span class="search-hit mathjax">memory</span> designs and their energy/area trade-offs.
  Afterwards, an <span class="search-hit mathjax">application</span>-specific power-gating technique is proposed to further <span class="search-hit mathjax">reduce</span> the energy consumption, depending upon the utilization across different operations of the CapsNets. Our results for a selected Pareto-<span class="search-hit mathjax">optimal</span> solution demonstrate no performance loss and an energy reduction of 79% for the complete accelerator, including computational units and <span class="search-hit mathjax">memories</span>, when compared to a state-of-the-art design executing Google&#39;s CapsNet model for the MNIST dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.05754v1-abstract-full').style.display = 'none'; document.getElementById('2010.05754v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication at the IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.05337">arXiv:2010.05337</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.05337">pdf</a>, <a href="https://arxiv.org/format/2010.05337">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+D">Da Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+C">Chao Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+M">Minjie Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jinjing Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+Q">Qidong Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+X">Xiang Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gan%2C+Q">Quan Gan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zheng Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Karypis%2C+G">George Karypis</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.05337v2-abstract-short" style="display: inline;">
        Graph neural networks (GNN) have shown great success in learning from graph-structured data. They are widely used in various <span class="search-hit mathjax">applications</span>, such as recommendation, fraud detection, and search. In these domains, the graphs are typically large, containing hundreds of millions of nodes and several billions of edges. To tackle this challenge, we develop DistDGL,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.05337v2-abstract-full').style.display = 'inline'; document.getElementById('2010.05337v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.05337v2-abstract-full" style="display: none;">
        Graph neural networks (GNN) have shown great success in learning from graph-structured data. They are widely used in various <span class="search-hit mathjax">applications</span>, such as recommendation, fraud detection, and search. In these domains, the graphs are typically large, containing hundreds of millions of nodes and several billions of edges. To tackle this challenge, we develop DistDGL, a system for training GNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the Deep Graph Library (DGL), a popular GNN development framework. DistDGL distributes the graph and its associated data (initial features and embeddings) across the machines and uses this distribution to derive a computational decomposition by following an owner-compute rule. DistDGL follows a synchronous training approach and allows ego-networks forming the mini-batches to include non-local nodes. To minimize the overheads associated with distributed computations, DistDGL uses a high-quality and light-weight min-cut graph partitioning algorithm along with multiple balancing constraints. This allows it to <span class="search-hit mathjax">reduce</span> communication overheads and statically balance the computations. It further <span class="search-hit mathjax">reduces</span> the communication by replicating halo nodes and by using sparse embedding updates. The combination of these design choices allows DistDGL to train high-quality models while achieving high parallel efficiency and <span class="search-hit mathjax">memory</span> scalability. We demonstrate our <span class="search-hit mathjax">optimizations</span> on both inductive and transductive GNN models. Our results show that DistDGL achieves linear speedup without compromising model accuracy and requires only 13 seconds to complete a training epoch for a graph with 100 million nodes and 3 billion edges on a cluster with 16 machines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.05337v2-abstract-full').style.display = 'none'; document.getElementById('2010.05337v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.04282">arXiv:2010.04282</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.04282">pdf</a>, <a href="https://arxiv.org/format/2010.04282">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Memory</span>-Limited Model-Based Diagnosis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rodler%2C+P">Patrick Rodler</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.04282v1-abstract-short" style="display: inline;">
        &hellip;order within linear space bounds, without sacrificing the desirable soundness or completeness properties. The idea of HBF-HS is to find a trade-off between runtime <span class="search-hit mathjax">optimization</span> and a restricted space consumption that does not exceed the available&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.04282v1-abstract-full').style.display = 'inline'; document.getElementById('2010.04282v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.04282v1-abstract-full" style="display: none;">
        Various model-based diagnosis scenarios require the computation of most preferred fault explanations. Existing algorithms that are sound (i.e., output only actual fault explanations) and complete (i.e., can return all explanations), however, require exponential space to achieve this task. As a remedy, we propose two novel diagnostic search algorithms, called RBF-HS (Recursive Best-First Hitting Set Search) and HBF-HS (Hybrid Best-First Hitting Set Search), which build upon tried and tested techniques from the heuristic search domain. RBF-HS can enumerate an arbitrary predefined finite number of fault explanations in best-first order within linear space bounds, without sacrificing the desirable soundness or completeness properties. The idea of HBF-HS is to find a trade-off between runtime <span class="search-hit mathjax">optimization</span> and a restricted space consumption that does not exceed the available <span class="search-hit mathjax">memory</span>.
  In extensive experiments on real-world diagnosis cases we compared our approaches to Reiter&#39;s HS-Tree, a state-of-the-art method that gives the same theoretical guarantees and is as general(ly <span class="search-hit mathjax">applicable</span>) as the suggested algorithms. For the computation of minimum-cardinality fault explanations, we find that (1) RBF-HS <span class="search-hit mathjax">reduces</span> <span class="search-hit mathjax">memory</span> requirements substantially in most cases by up to several orders of magnitude, (2) in more than a third of the cases, both <span class="search-hit mathjax">memory</span> savings and runtime savings are achieved, and (3) given the runtime overhead is significant, using HBF-HS instead of RBF-HS <span class="search-hit mathjax">reduces</span> the runtime to values comparable with HS-Tree while keeping the used <span class="search-hit mathjax">memory</span> reasonably bounded. When computing most probable fault explanations, we observe that RBF-HS tends to trade <span class="search-hit mathjax">memory</span> savings more or less one-to-one for runtime overheads. Again, HBF-HS proves to be a reasonable remedy to cut down the runtime while complying with practicable <span class="search-hit mathjax">memory</span> bounds.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.04282v1-abstract-full').style.display = 'none'; document.getElementById('2010.04282v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.02488">arXiv:2010.02488</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.02488">pdf</a>, <a href="https://arxiv.org/ps/2010.02488">ps</a>, <a href="https://arxiv.org/format/2010.02488">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RANP: Resource Aware Neuron Pruning at Initialization for 3D CNNs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+Z">Zhiwei Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ajanthan%2C+T">Thalaiyasingam Ajanthan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vineet%2C+V">Vibhav Vineet</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hartley%2C+R">Richard Hartley</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.02488v3-abstract-short" style="display: inline;">
        Although 3D Convolutional Neural Networks (CNNs) are essential for most learning based <span class="search-hit mathjax">applications</span> involving dense 3D data, their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02488v3-abstract-full').style.display = 'inline'; document.getElementById('2010.02488v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.02488v3-abstract-full" style="display: none;">
        Although 3D Convolutional Neural Networks (CNNs) are essential for most learning based <span class="search-hit mathjax">applications</span> involving dense 3D data, their <span class="search-hit mathjax">applicability</span> is limited due to excessive <span class="search-hit mathjax">memory</span> and computational requirements. Compressing such networks by pruning therefore becomes highly desirable. However, pruning 3D CNNs is largely unexplored possibly because of the complex nature of typical pruning algorithms that embeds pruning into an iterative <span class="search-hit mathjax">optimization</span> paradigm. In this work, we introduce a Resource Aware Neuron Pruning (RANP) algorithm that prunes 3D CNNs at initialization to high sparsity levels. Specifically, the core idea is to obtain an importance score for each neuron based on their sensitivity to the loss function. This neuron importance is then reweighted according to the neuron resource consumption related to FLOPs or <span class="search-hit mathjax">memory</span>. We demonstrate the effectiveness of our pruning method on 3D semantic segmentation with widely used 3D-UNets on ShapeNet and BraTS&#39;18 as well as on video classification with MobileNetV2 and I3D on UCF101 dataset. In these experiments, our RANP leads to roughly 50-95 reduction in FLOPs and 35-80 reduction in <span class="search-hit mathjax">memory</span> with negligible loss in accuracy compared to the unpruned networks. This significantly <span class="search-hit mathjax">reduces</span> the computational resources required to train 3D CNNs. The pruned network obtained by our algorithm can also be easily scaled up and transferred to another dataset for training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02488v3-abstract-full').style.display = 'none'; document.getElementById('2010.02488v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">International Conference on 3D Vision (3DV), 2020 (Oral)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.01921">arXiv:2010.01921</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.01921">pdf</a>, <a href="https://arxiv.org/format/2010.01921">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Physics">physics.comp-ph</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        $Î¾$-torch: differentiable scientific computing library
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kasim%2C+M+F">Muhammad F. Kasim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vinko%2C+S+M">Sam M. Vinko</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.01921v1-abstract-short" style="display: inline;">
        &hellip;-torch are written based on their analytical expression to <span class="search-hit mathjax">improve</span> numerical stability and <span class="search-hit mathjax">reduce</span> <span class="search-hit mathjax">memory</span> requirements.
  $Î¾$-torch also provides second and higher order derivatives of the functionals which are rarely available in existing packages.
  We show two&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.01921v1-abstract-full').style.display = 'inline'; document.getElementById('2010.01921v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.01921v1-abstract-full" style="display: none;">
        Physics-informed learning has shown to have a better generalization than learning without physical priors.
  However, training physics-informed deep neural networks requires some aspect of physical simulations to be written in a differentiable manner.
  Unfortunately, some operations and functionals commonly used in physical simulations are scattered, hard to integrate, and lack higher order derivatives which are needed in physical simulations.
  In this work, we present $Î¾$-torch, a library of differentiable functionals for scientific simulations.
  Example functionals are a root finder and an initial value problem solver, among others.
  The gradient of functionals in $Î¾$-torch are written based on their analytical expression to <span class="search-hit mathjax">improve</span> numerical stability and <span class="search-hit mathjax">reduce</span> <span class="search-hit mathjax">memory</span> requirements.
  $Î¾$-torch also provides second and higher order derivatives of the functionals which are rarely available in existing packages.
  We show two <span class="search-hit mathjax">applications</span> of this library in <span class="search-hit mathjax">optimizing</span> parameters in physics simulations.
  The library and all test cases in this work can be found at https://github.com/xitorch/xitorch/ and the documentation at https://xitorch.readthedocs.io.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.01921v1-abstract-full').style.display = 'none'; document.getElementById('2010.01921v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.12009">arXiv:2009.12009</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.12009">pdf</a>, <a href="https://arxiv.org/format/2009.12009">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AMReX: Block-Structured Adaptive Mesh Refinement for Multiphysics <span class="search-hit mathjax">Applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Weiqun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Myers%2C+A">Andrew Myers</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gott%2C+K">Kevin Gott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Almgren%2C+A">Ann Almgren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bell%2C+J">John Bell</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.12009v1-abstract-short" style="display: inline;">
        Block-structured adaptive mesh refinement (AMR) provides the basis for the temporal and spatial discretization strategy for a number of ECP <span class="search-hit mathjax">applications</span> in the areas of accelerator design, additive manufacturing, astrophysics, combustion, cosmology, multiphase flow, and wind plant modelling. AMReX is a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.12009v1-abstract-full').style.display = 'inline'; document.getElementById('2009.12009v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.12009v1-abstract-full" style="display: none;">
        Block-structured adaptive mesh refinement (AMR) provides the basis for the temporal and spatial discretization strategy for a number of ECP <span class="search-hit mathjax">applications</span> in the areas of accelerator design, additive manufacturing, astrophysics, combustion, cosmology, multiphase flow, and wind plant modelling. AMReX is a <span class="search-hit mathjax">software</span> framework that provides a unified infrastructure with the functionality needed for these and other AMR <span class="search-hit mathjax">applications</span> to be able to effectively and efficiently utilize machines from laptops to exascale architectures. AMR <span class="search-hit mathjax">reduces</span> the computational cost and <span class="search-hit mathjax">memory</span> footprint compared to a uniform mesh while preserving accurate descriptions of different physical processes in complex multi-physics algorithms. AMReX supports algorithms that solve systems of partial differential equations (PDEs) in simple or complex geometries, and those that use particles and/or particle-mesh operations to represent component physical processes. In this paper, we will discuss the core elements of the AMReX framework such as data containers and iterators as well as several specialized operations to meet the needs of the <span class="search-hit mathjax">application</span> projects. In addition we will highlight the strategy that the AMReX team is pursuing to achieve highly performant <span class="search-hit mathjax">code</span> across a range of accelerator-based architectures for a variety of different <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.12009v1-abstract-full').style.display = 'none'; document.getElementById('2009.12009v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 9 figures, submitted to IJHPCA</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.10243">arXiv:2009.10243</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.10243">pdf</a>, <a href="https://arxiv.org/ps/2009.10243">ps</a>, <a href="https://arxiv.org/format/2009.10243">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.4204/EPTCS.325.20">10.4204/EPTCS.325.20 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tabling <span class="search-hit mathjax">Optimization</span> for Contextual Abduction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dewoprabowo%2C+R">Ridhwan Dewoprabowo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saptawijaya%2C+A">Ari Saptawijaya</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.10243v1-abstract-short" style="display: inline;">
        Tabling for contextual abduction in logic <span class="search-hit mathjax">programming</span> has been introduced as a means to store previously obtained abductive solutions in one context to be reused in another context. This paper identifies a number of issues in the existing implementations of tabling in contextual abduction and aims to mitigate the issues. We propose a new&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.10243v1-abstract-full').style.display = 'inline'; document.getElementById('2009.10243v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.10243v1-abstract-full" style="display: none;">
        Tabling for contextual abduction in logic <span class="search-hit mathjax">programming</span> has been introduced as a means to store previously obtained abductive solutions in one context to be reused in another context. This paper identifies a number of issues in the existing implementations of tabling in contextual abduction and aims to mitigate the issues. We propose a new <span class="search-hit mathjax">program</span> transformation for integrity constraints to deal with their proper <span class="search-hit mathjax">application</span> for filtering solutions while also <span class="search-hit mathjax">reducing</span> the table <span class="search-hit mathjax">memory</span> usage. We further <span class="search-hit mathjax">optimize</span> the table <span class="search-hit mathjax">memory</span> usage by selectively picking predicates to table and by pragmatically simplifying the representation of the problem. The evaluation of our proposed approach, on both artificial and real world problems, shows that they <span class="search-hit mathjax">improve</span> the scalability of tabled abduction compared to previous implementations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.10243v1-abstract-full').style.display = 'none'; document.getElementById('2009.10243v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In Proceedings ICLP 2020, arXiv:2009.09158</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        EPTCS 325, 2020, pp. 137-150
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.09523">arXiv:2009.09523</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.09523">pdf</a>, <a href="https://arxiv.org/format/2009.09523">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VirtualFlow: Decoupling Deep Learning Models from the Underlying Hardware
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Or%2C+A">Andrew Or</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Haoyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Freedman%2C+M+J">Michael J. Freedman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.09523v2-abstract-short" style="display: inline;">
        State-of-the-art deep learning systems such as TensorFlow and PyTorch tightly couple the model with the underlying hardware. This coupling requires the user to modify <span class="search-hit mathjax">application</span> logic in order to run the same job across a different set of resources, thereby limiting the choice of hardware for a given workload and potentially forcing the user to forgo more e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.09523v2-abstract-full').style.display = 'inline'; document.getElementById('2009.09523v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.09523v2-abstract-full" style="display: none;">
        State-of-the-art deep learning systems such as TensorFlow and PyTorch tightly couple the model with the underlying hardware. This coupling requires the user to modify <span class="search-hit mathjax">application</span> logic in order to run the same job across a different set of resources, thereby limiting the choice of hardware for a given workload and potentially forcing the user to forgo more efficient hardware configurations.
  We propose VirtualFlow, a system leveraging a novel abstraction called virtual node processing to decouple the model from the hardware. In each step of training or inference, the batch of input data is split across virtual nodes instead of hardware accelerators (e.g. GPUs and TPUs). Mapping multiple virtual nodes to each accelerator and processing them sequentially effectively time slices the batch, thereby allowing users to <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">memory</span> requirement of their workloads and mimic large batch sizes on small clusters.
  Using this technique, VirtualFlow enables many new use cases, such as reproducing training results across different hardware, resource elasticity, and heterogeneous training. In our evaluation, our implementation of VirtualFlow for TensorFlow achieved strong convergence guarantees across different hardware with out-of-the-box hyperparameters, up to 48% lower job completion times with resource elasticity, and up to 42% higher throughput with heterogeneous training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.09523v2-abstract-full').style.display = 'none'; document.getElementById('2009.09523v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 September, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 29 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.09388">arXiv:2009.09388</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.09388">pdf</a>, <a href="https://arxiv.org/ps/2009.09388">ps</a>, <a href="https://arxiv.org/format/2009.09388">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tb/s Polar Successive Cancellation Decoder 16nm ASIC Implementation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=S%C3%BCral%2C+A">AltuÄ SÃ¼ral</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sezer%2C+E+G">E. GÃ¶ksu Sezer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kola%C4%9Fas%C4%B1o%C4%9Flu%2C+E">ErtuÄrul KolaÄasÄ±oÄlu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Derudder%2C+V">Veerle Derudder</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertrand%2C+K">Kaoutar Bertrand</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.09388v1-abstract-short" style="display: inline;">
        This work presents an efficient ASIC implementation of successive cancellation (SC) decoder for polar <span class="search-hit mathjax">codes</span>. SC is a low-complexity depth-first search decoding algorithm, favorable for beyond-5G&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.09388v1-abstract-full').style.display = 'inline'; document.getElementById('2009.09388v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.09388v1-abstract-full" style="display: none;">
        This work presents an efficient ASIC implementation of successive cancellation (SC) decoder for polar <span class="search-hit mathjax">codes</span>. SC is a low-complexity depth-first search decoding algorithm, favorable for beyond-5G <span class="search-hit mathjax">applications</span> that require extremely high throughput and low power. The ASIC implementation of SC in this work exploits many techniques including pipelining and unrolling to achieve Tb/s data throughput without compromising power and area metrics. To <span class="search-hit mathjax">reduce</span> the complexity of the implementation, an adaptive log-likelihood ratio (LLR) quantization scheme is used. This scheme <span class="search-hit mathjax">optimizes</span> bit precision of the internal LLRs within the range of 1-5 bits by considering irregular polarization and entropy of LLR distribution in SC decoder. The performance cost of this scheme is less than 0.2 dB when the <span class="search-hit mathjax">code</span> block length is 1024 bits and the payload is 854 bits. Furthermore, some computations in SC take large space with high degree of parallelization while others take longer time steps. To <span class="search-hit mathjax">optimize</span> these computations and <span class="search-hit mathjax">reduce</span> both <span class="search-hit mathjax">memory</span> and latency, register reduction/balancing (R-RB) method is used. The final decoder architecture is called <span class="search-hit mathjax">optimized</span> polar SC (OPSC). The post-placement-routing results at 16nm FinFet ASIC technology show that OPSC decoder achieves 1.2 Tb/s <span class="search-hit mathjax">coded</span> throughput on 0.79 mm$^2$ area with 0.95 pJ/bit energy efficiency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.09388v1-abstract-full').style.display = 'none'; document.getElementById('2009.09388v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.09682">arXiv:2008.09682</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.09682">pdf</a>, <a href="https://arxiv.org/format/2008.09682">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DwarvesGraph: A High-Performance Graph Mining System with Pattern Decomposition
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jingji Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+X">Xuehai Qian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.09682v3-abstract-short" style="display: inline;">
        &hellip;algorithms used in current GPM systems because the execution time of pattern enumeration drastically increases with pattern size. We define a novel partial-embedding-centric <span class="search-hit mathjax">programming</span> model that supports various&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.09682v3-abstract-full').style.display = 'inline'; document.getElementById('2008.09682v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.09682v3-abstract-full" style="display: none;">
        This paper presents DwarvesGraph, the first compilation-based graph pattern mining (GPM) system based on pattern decomposition algorithms, which decompose a pattern into several subpatterns and find the count of each. Such algorithms can be orders of magnitudes faster than algorithms used in current GPM systems because the execution time of pattern enumeration drastically increases with pattern size. We define a novel partial-embedding-centric <span class="search-hit mathjax">programming</span> model that supports various <span class="search-hit mathjax">applications</span>. We propose an efficient on-the-fly aggregation of subpatterns embeddings to <span class="search-hit mathjax">reduce</span> <span class="search-hit mathjax">memory</span> consumption and random accesses. DwarvesGraph compiler, using abstract syntax tree (AST) as intermediate representation (IR), can apply conventional and a novel pattern-aware loop rewriting <span class="search-hit mathjax">optimization</span> to eliminate redundant computation that cannot be removed with standard methods. To estimate implementation cost based on AST, we propose a simple locality-aware and an advanced approximate-mining-based cost model to accurately capture the characteristics of real-world graphs. DwarvesGraph fully <span class="search-hit mathjax">automates</span> the algorithm generation, <span class="search-hit mathjax">optimization</span>, and selection in the search space. As a general GPM system, DwarvesGraph achieves performance much closer to the best native pattern decomposition algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.09682v3-abstract-full').style.display = 'none'; document.getElementById('2008.09682v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.09589">arXiv:2008.09589</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.09589">pdf</a>, <a href="https://arxiv.org/format/2008.09589">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Instrumentation and Methods for Astrophysics">astro-ph.IM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Analysis, Statistics and Probability">physics.data-an</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation">stat.CO</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ParaDRAM: A Cross-Language Toolbox for Parallel High-Performance Delayed-Rejection Adaptive Metropolis Markov Chain Monte Carlo Simulations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shahmoradi%2C+A">Amir Shahmoradi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bagheri%2C+F">Fatemeh Bagheri</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.09589v1-abstract-short" style="display: inline;">
        We present ParaDRAM, a high-performance Parallel Delayed-Rejection Adaptive Metropolis Markov Chain Monte Carlo <span class="search-hit mathjax">software</span> for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.09589v1-abstract-full').style.display = 'inline'; document.getElementById('2008.09589v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.09589v1-abstract-full" style="display: none;">
        We present ParaDRAM, a high-performance Parallel Delayed-Rejection Adaptive Metropolis Markov Chain Monte Carlo <span class="search-hit mathjax">software</span> for <span class="search-hit mathjax">optimization</span>, sampling, and integration of mathematical objective functions encountered in scientific inference. ParaDRAM is currently accessible from several popular <span class="search-hit mathjax">programming</span> languages including C/C++, Fortran, MATLAB, Python and is part of the ParaMonte open-source project with the following principal design goals: 1. full <span class="search-hit mathjax">automation</span> of Monte Carlo simulations, 2. interoperability of the core library with as many <span class="search-hit mathjax">programming</span> languages as possible, thus, providing a unified <span class="search-hit mathjax">Application</span> <span class="search-hit mathjax">Programming</span> Interface and Monte Carlo simulation environment across all <span class="search-hit mathjax">programming</span> languages, 3. high-performance 4. parallelizability and scalability of simulations from personal laptops to supercomputers, 5. virtually zero-dependence on external libraries, 6. fully-deterministic reproducibility of simulations, 7. <span class="search-hit mathjax">automatic</span> comprehensive reporting and post-processing of the simulation results. We present and discuss several novel techniques implemented in ParaDRAM to <span class="search-hit mathjax">automatically</span> and dynamically ensure the good-mixing and the diminishing-adaptation of the resulting pseudo-Markov chains from ParaDRAM. We also discuss the implementation of an efficient data storage method used in ParaDRAM that <span class="search-hit mathjax">reduces</span> the average <span class="search-hit mathjax">memory</span> and storage requirements of the algorithm by, a factor of 4 for simple simulation problems, to an order of magnitude and more for sampling complex high-dimensional mathematical objective functions. Finally, we discuss how the design goals of ParaDRAM can help users readily and efficiently solve a variety of machine learning and scientific inference problems on a wide range of computing platforms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.09589v1-abstract-full').style.display = 'none'; document.getElementById('2008.09589v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.09072">arXiv:2008.09072</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.09072">pdf</a>, <a href="https://arxiv.org/format/2008.09072">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Utilizing Explainable AI for Quantization and Pruning of Deep Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sabih%2C+M">Muhammad Sabih</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hannig%2C+F">Frank Hannig</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Teich%2C+J">Juergen Teich</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.09072v1-abstract-short" style="display: inline;">
        For many <span class="search-hit mathjax">applications</span>, utilizing DNNs (Deep Neural Networks) requires their implementation on a target architecture in an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.09072v1-abstract-full').style.display = 'inline'; document.getElementById('2008.09072v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.09072v1-abstract-full" style="display: none;">
        For many <span class="search-hit mathjax">applications</span>, utilizing DNNs (Deep Neural Networks) requires their implementation on a target architecture in an <span class="search-hit mathjax">optimized</span> manner concerning energy consumption, <span class="search-hit mathjax">memory</span> requirement, throughput, etc. DNN compression is used to <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">memory</span> footprint and complexity of a DNN before its deployment on hardware. Recent efforts to understand and explain AI (Artificial Intelligence) methods have led to a new research area, termed as explainable AI. Explainable AI methods allow us to understand better the inner working of DNNs, such as the importance of different neurons and features. The concepts from explainable AI provide an opportunity to <span class="search-hit mathjax">improve</span> DNN compression methods such as quantization and pruning in several ways that have not been sufficiently explored so far. In this paper, we utilize explainable AI methods: mainly DeepLIFT method. We use these methods for (1) pruning of DNNs; this includes structured and unstructured pruning of \ac{CNN} filters pruning as well as pruning weights of fully connected layers, (2) non-uniform quantization of DNN weights using clustering algorithm; this is also referred to as Weight Sharing, and (3) integer-based mixed-precision quantization; this is where each layer of a DNN may use a different number of integer bits. We use typical image classification datasets with common deep learning image classification models for evaluation. In all these three cases, we demonstrate significant <span class="search-hit mathjax">improvements</span> as well as new insights and opportunities from the use of explainable AI in DNN compression.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.09072v1-abstract-full').style.display = 'none'; document.getElementById('2008.09072v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.07127">arXiv:2008.07127</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.07127">pdf</a>, <a href="https://arxiv.org/format/2008.07127">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TC.2021.3066883">10.1109/TC.2021.3066883 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DORY: <span class="search-hit mathjax">Automatic</span> End-to-End Deployment of Real-World DNNs on Low-Cost IoT MCUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Burrello%2C+A">Alessio Burrello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garofalo%2C+A">Angelo Garofalo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bruschi%2C+N">Nazareno Bruschi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tagliavini%2C+G">Giuseppe Tagliavini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rossi%2C+D">Davide Rossi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Conti%2C+F">Francesco Conti</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.07127v3-abstract-short" style="display: inline;">
        The deployment of Deep Neural Networks (DNNs) on end-nodes at the extreme edge of the Internet-of-Things is a critical enabler to support pervasive Deep Learning-enhanced <span class="search-hit mathjax">applications</span>. Low-Cost MCU-based end-nodes have limited on-chip&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.07127v3-abstract-full').style.display = 'inline'; document.getElementById('2008.07127v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.07127v3-abstract-full" style="display: none;">
        The deployment of Deep Neural Networks (DNNs) on end-nodes at the extreme edge of the Internet-of-Things is a critical enabler to support pervasive Deep Learning-enhanced <span class="search-hit mathjax">applications</span>. Low-Cost MCU-based end-nodes have limited on-chip <span class="search-hit mathjax">memory</span> and often replace caches with scratchpads, to <span class="search-hit mathjax">reduce</span> area overheads and increase energy efficiency -- requiring explicit DMA-based <span class="search-hit mathjax">memory</span> transfers between different levels of the <span class="search-hit mathjax">memory</span> hierarchy. Mapping modern DNNs on these systems requires aggressive topology-dependent tiling and double-buffering. In this work, we propose DORY (Deployment Oriented to <span class="search-hit mathjax">memoRY</span>) - an <span class="search-hit mathjax">automatic</span> tool to deploy DNNs on low cost MCUs with typically less than 1MB of on-chip SRAM <span class="search-hit mathjax">memory</span>. DORY abstracts tiling as a Constraint <span class="search-hit mathjax">Programming</span> (CP) problem: it maximizes L1 <span class="search-hit mathjax">memory</span> utilization under the topological constraints imposed by each DNN layer. Then, it generates ANSI C <span class="search-hit mathjax">code</span> to orchestrate off- and on-chip transfers and computation phases. Furthermore, to maximize speed, DORY augments the CP formulation with heuristics promoting performance-effective tile sizes. As a case study for DORY, we target GreenWaves Technologies GAP8, one of the most advanced parallel ultra-low power MCU-class devices on the market. On this device, DORY achieves up to 2.5x better MAC/cycle than the GreenWaves proprietary <span class="search-hit mathjax">software</span> solution and 18.1x better than the state-of-the-art result on an STM32-F746 MCU on single layers. Using our tool, GAP-8 can perform end-to-end inference of a 1.0-MobileNet-128 network consuming just 63 pJ/MAC on average @ 4.3 fps - 15.4x better than an STM32-F746. We release all our developments - the DORY framework, the <span class="search-hit mathjax">optimized</span> backend kernels, and the related heuristics - as open-source <span class="search-hit mathjax">software</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.07127v3-abstract-full').style.display = 'none'; document.getElementById('2008.07127v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 12 figures, 4 tables, 2 listings. Accepted for publication in IEEE Transactions on Computers (https://ieeexplore.ieee.org/document/9381618)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.06177">arXiv:2008.06177</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.06177">pdf</a>, <a href="https://arxiv.org/format/2008.06177">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PANDA: Processing-in-MRAM Accelerated De Bruijn Graph based DNA Assembly
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Angizi%2C+S">Shaahin Angizi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fahmi%2C+N+A">Naima Ahmed Fahmi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+D">Deliang Fan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.06177v1-abstract-short" style="display: inline;">
        Spurred by widening gap between data processing speed and data communication speed in Von-Neumann computing architectures, some bioinformatic <span class="search-hit mathjax">applications</span> have harnessed the computational power of Processing-in-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.06177v1-abstract-full').style.display = 'inline'; document.getElementById('2008.06177v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.06177v1-abstract-full" style="display: none;">
        Spurred by widening gap between data processing speed and data communication speed in Von-Neumann computing architectures, some bioinformatic <span class="search-hit mathjax">applications</span> have harnessed the computational power of Processing-in-<span class="search-hit mathjax">Memory</span> (PIM) platforms. However, the performance of PIMs unavoidably diminishes when dealing with such complex <span class="search-hit mathjax">applications</span> seeking bulk bit-wise comparison or addition operations. In this work, we present an efficient Processing-in-MRAM Accelerated De Bruijn Graph based DNA Assembly platform named PANDA based on an <span class="search-hit mathjax">optimized</span> and hardware-friendly genome assembly algorithm. PANDA is able to assemble large-scale DNA sequence data-set from all-pair overlaps. We first design PANDA platform that exploits MRAM as a computational <span class="search-hit mathjax">memory</span> and converts it to a potent processing unit for genome assembly. PANDA can execute not only efficient bulk bit-wise X(N)OR-based comparison/addition operations heavily required for the genome assembly task but a full-set of 2-/3-input logic operations inside MRAM chip. We then develop a highly parallel and step-by-step hardware-friendly DNA assembly algorithm for PANDA that only requires the developed in-<span class="search-hit mathjax">memory</span> logic operations. The platform is then configured with a novel data partitioning and mapping technique that provides local storage and processing to fully utilize the algorithm-level&#39;s parallelism. The cross-layer simulation results demonstrate that PANDA <span class="search-hit mathjax">reduces</span> the run time and power, respectively, by a factor of 18 and 11 compared with CPU. Besides, speed-ups of up-to 2-4x can be obtained over recent processing-in-MRAM platforms to perform the same task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.06177v1-abstract-full').style.display = 'none'; document.getElementById('2008.06177v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 15 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.04447">arXiv:2008.04447</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.04447">pdf</a>, <a href="https://arxiv.org/format/2008.04447">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Spectral Theory">math.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Randomized Projection for Rank-Revealing Matrix Factorizations and Low-Rank Approximations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Duersch%2C+J+A">Jed A. Duersch</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+M">Ming Gu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.04447v1-abstract-short" style="display: inline;">
        &hellip;but it can be much slower than the unpivoted QR algorithm. For large matrices, the difference in performance is due to increased communication between the processor and slow <span class="search-hit mathjax">memory</span>, which QRCP needs in order to choose pivots during decomposition. Our main algorithm, Randomized QR with Column Pivoting (RQRCP), uses randomized projection to make pivot decisio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.04447v1-abstract-full').style.display = 'inline'; document.getElementById('2008.04447v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.04447v1-abstract-full" style="display: none;">
        Rank-revealing matrix decompositions provide an essential tool in spectral analysis of matrices, including the Singular Value Decomposition (SVD) and related low-rank approximation techniques. QR with Column Pivoting (QRCP) is usually suitable for these purposes, but it can be much slower than the unpivoted QR algorithm. For large matrices, the difference in performance is due to increased communication between the processor and slow <span class="search-hit mathjax">memory</span>, which QRCP needs in order to choose pivots during decomposition. Our main algorithm, Randomized QR with Column Pivoting (RQRCP), uses randomized projection to make pivot decisions from a much smaller sample matrix, which we can construct to reside in a faster level of <span class="search-hit mathjax">memory</span> than the original matrix. This technique may be understood as trading vastly <span class="search-hit mathjax">reduced</span> communication for a controlled increase in uncertainty during the decision process. For rank-revealing purposes, the selection mechanism in RQRCP produces results that are the same quality as the standard algorithm, but with performance near that of unpivoted QR (often an order of magnitude faster for large matrices). We also propose two formulas that facilitate further performance <span class="search-hit mathjax">improvements</span>. The first efficiently updates sample matrices to avoid computing new randomized projections. The second avoids large trailing updates during the decomposition in truncated low-rank approximations. Our truncated version of RQRCP also provides a key initial step in our truncated SVD approximation, TUXV. These advances open up a new performance domain for large matrix factorizations that will support efficient problem-solving techniques for challenging <span class="search-hit mathjax">applications</span> in science, engineering, and data analysis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.04447v1-abstract-full').style.display = 'none'; document.getElementById('2008.04447v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Revised from Randomized QR with Column Pivoting for publication in SIGEST</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68W20; 15A23; 15A18; 65F25
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.09625">arXiv:2007.09625</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.09625">pdf</a>, <a href="https://arxiv.org/format/2007.09625">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3410463.3414624">10.1145/3410463.3414624 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        cuSZ: An Efficient GPU-Based Error-Bounded Lossy Compression Framework for Scientific Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+J">Jiannan Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Di%2C+S">Sheng Di</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+K">Kai Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rivera%2C+C">Cody Rivera</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fulp%2C+M+H">Megan Hickman Fulp</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Underwood%2C+R">Robert Underwood</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+S">Sian Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+X">Xin Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Calhoun%2C+J">Jon Calhoun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tao%2C+D">Dingwen Tao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cappello%2C+F">Franck Cappello</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.09625v6-abstract-short" style="display: inline;">
        Error-bounded lossy compression is a state-of-the-art data reduction technique for HPC <span class="search-hit mathjax">applications</span> because it not only significantly&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.09625v6-abstract-full').style.display = 'inline'; document.getElementById('2007.09625v6-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.09625v6-abstract-full" style="display: none;">
        Error-bounded lossy compression is a state-of-the-art data reduction technique for HPC <span class="search-hit mathjax">applications</span> because it not only significantly <span class="search-hit mathjax">reduces</span> storage overhead but also can retain high fidelity for postanalysis. Because supercomputers and HPC <span class="search-hit mathjax">applications</span> are becoming heterogeneous using accelerator-based architectures, in particular GPUs, several development teams have recently released GPU versions of their lossy compressors. However, existing state-of-the-art GPU-based lossy compressors suffer from either low compression and decompression throughput or low compression quality. In this paper, we present an <span class="search-hit mathjax">optimized</span> GPU version, cuSZ, for one of the best error-bounded lossy compressors-SZ. To the best of our knowledge, cuSZ is the first error-bounded lossy compressor on GPUs for scientific data. Our contributions are fourfold. (1) We propose a dual-quantization scheme to entirely remove the data dependency in the prediction step of SZ such that this step can be performed very efficiently on GPUs. (2) We develop an efficient customized Huffman <span class="search-hit mathjax">coding</span> for the SZ compressor on GPUs. (3) We implement cuSZ using CUDA and <span class="search-hit mathjax">optimize</span> its performance by <span class="search-hit mathjax">improving</span> the utilization of GPU <span class="search-hit mathjax">memory</span> bandwidth. (4) We evaluate our cuSZ on five real-world HPC <span class="search-hit mathjax">application</span> datasets from the Scientific Data Reduction Benchmarks and compare it with other state-of-the-art methods on both CPUs and GPUs. Experiments show that our cuSZ <span class="search-hit mathjax">improves</span> SZ&#39;s compression throughput by up to 370.1x and 13.1x, respectively, over the production version running on single and multiple CPU cores, respectively, while getting the same quality of reconstructed data. It also <span class="search-hit mathjax">improves</span> the compression ratio by up to 3.48x on the tested data compared with another state-of-the-art GPU supported lossy compressor.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.09625v6-abstract-full').style.display = 'none'; document.getElementById('2007.09625v6-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 July, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 8 figures, 9 tables, published in PACT &#39;20</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.07539">arXiv:2007.07539</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.07539">pdf</a>, <a href="https://arxiv.org/format/2007.07539">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Accelerating Geometric Multigrid Preconditioning with Half-Precision Arithmetic on GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Oo%2C+K+L">Kyaw L. Oo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vogel%2C+A">Andreas Vogel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.07539v1-abstract-short" style="display: inline;">
        With the hardware support for half-precision arithmetic on NVIDIA V100 GPUs, high-performance computing <span class="search-hit mathjax">applications</span> can benefit from lower precision at appropriate spots to speed up the overall execution time. In this paper, we investigate a mixed-precision geometric multigrid method to solve large sparse systems of equations stemming from discretization of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.07539v1-abstract-full').style.display = 'inline'; document.getElementById('2007.07539v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.07539v1-abstract-full" style="display: none;">
        With the hardware support for half-precision arithmetic on NVIDIA V100 GPUs, high-performance computing <span class="search-hit mathjax">applications</span> can benefit from lower precision at appropriate spots to speed up the overall execution time. In this paper, we investigate a mixed-precision geometric multigrid method to solve large sparse systems of equations stemming from discretization of elliptic PDEs. While the final solution is always computed with high-precision accuracy, an iterative refinement approach with multigrid preconditioning in lower precision and residuum scaling is employed. We compare the FP64 baseline for Poisson&#39;s equation to purely FP16 multigrid preconditioning and to the employment of FP16-FP32-FP64 combinations within a mesh hierarchy. While the iteration count is almost not affected by using lower accuracy, the solver runtime is considerably decreased due to the <span class="search-hit mathjax">reduced</span> <span class="search-hit mathjax">memory</span> transfer and a speedup of up to 2.5x is gained for the overall solver. We investigate the performance of selected kernels with the hierarchical Roofline model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.07539v1-abstract-full').style.display = 'none'; document.getElementById('2007.07539v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 July, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.04552">arXiv:2007.04552</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.04552">pdf</a>, <a href="https://arxiv.org/format/2007.04552">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        IOCA: High-Speed I/O-Aware LLC Management for Network-Centric Multi-Tenant Platform
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+Y">Yifan Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alian%2C+M">Mohammad Alian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yipeng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+I">Ilia Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Ren Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tai%2C+C">Charlie Tai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+N+S">Nam Sung Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.04552v2-abstract-short" style="display: inline;">
        &hellip;This is because of an Intel architectural innovation -- Data Direct I/O (DDIO) -- that directly injects the inbound I/O traffic to (part of) the LLC instead of the main <span class="search-hit mathjax">memory</span>. We summarize two problems caused by DDIO and show that (1) the default DDIO configuration may not always achieve&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.04552v2-abstract-full').style.display = 'inline'; document.getElementById('2007.04552v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.04552v2-abstract-full" style="display: none;">
        In modern server CPUs, last-level cache (LLC) is a critical hardware resource that exerts significant influence on the performance of the workloads, and how to manage LLC is a key to the performance isolation and QoS in the cloud with multi-tenancy. In this paper, we argue that besides CPU cores, high-speed network I/O is also important for LLC management. This is because of an Intel architectural innovation -- Data Direct I/O (DDIO) -- that directly injects the inbound I/O traffic to (part of) the LLC instead of the main <span class="search-hit mathjax">memory</span>. We summarize two problems caused by DDIO and show that (1) the default DDIO configuration may not always achieve <span class="search-hit mathjax">optimal</span> performance, (2) DDIO can decrease the performance of non-I/O workloads which share LLC with it by as high as 32%.
  We then present IOCA, the first LLC management mechanism for network-centric platforms that treats the I/O as the first-class citizen. IOCA monitors and analyzes the performance of the cores, LLC, and DDIO using CPU&#39;s hardware performance counters, and adaptively adjusts the number of LLC ways for DDIO or the tenants that demand more LLC capacity. In addition, IOCA dynamically chooses the tenants that share its LLC resource with DDIO, to minimize the performance interference by both the tenants and the I/O. Our experiments with multiple microbenchmarks and real-world <span class="search-hit mathjax">applications</span> in two major end-host network models demonstrate that IOCA can effectively <span class="search-hit mathjax">reduce</span> the performance degradation caused by DDIO, with minimal overhead.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.04552v2-abstract-full').style.display = 'none'; document.getElementById('2007.04552v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 July, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by the 48th IEEE/ACM International Symposium on Computer Architecture (ISCA&#39;21). The title is &#34;Don&#39;t Forget the I/O When Allocating Your LLC&#34;</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.03317">arXiv:2007.03317</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.03317">pdf</a>, <a href="https://arxiv.org/format/2007.03317">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Learning of Generative Models via Finite-Difference Score Matching
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pang%2C+T">Tianyu Pang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+K">Kun Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chongxuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Y">Yang Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ermon%2C+S">Stefano Ermon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jun Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.03317v2-abstract-short" style="display: inline;">
        Several machine learning <span class="search-hit mathjax">applications</span> involve the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.03317v2-abstract-full').style.display = 'inline'; document.getElementById('2007.03317v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.03317v2-abstract-full" style="display: none;">
        Several machine learning <span class="search-hit mathjax">applications</span> involve the <span class="search-hit mathjax">optimization</span> of higher-order derivatives (e.g., gradients of gradients) during training, which can be expensive in respect to <span class="search-hit mathjax">memory</span> and computation even with <span class="search-hit mathjax">automatic</span> differentiation. As a typical example in generative modeling, score matching (SM) involves the <span class="search-hit mathjax">optimization</span> of the trace of a Hessian. To <span class="search-hit mathjax">improve</span> computing efficiency, we rewrite the SM objective and its variants in terms of directional derivatives, and present a generic strategy to efficiently approximate any-order directional derivative with finite difference (FD). Our approximation only involves function evaluations, which can be executed in parallel, and no gradient computations. Thus, it <span class="search-hit mathjax">reduces</span> the total computational cost while also <span class="search-hit mathjax">improving</span> numerical stability. We provide two instantiations by reformulating variants of SM objectives into the FD forms. Empirically, we demonstrate that our methods produce results comparable to the gradient-based counterparts while being much more computationally efficient.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.03317v2-abstract-full').style.display = 'none'; document.getElementById('2007.03317v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 November, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 July, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.03179">arXiv:2007.03179</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.03179">pdf</a>, <a href="https://arxiv.org/format/2007.03179">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GE-SpMM: General-purpose Sparse Matrix-Matrix Multiplication on GPUs for Graph Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+G">Guyue Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+G">Guohao Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+H">Huazhong Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.03179v1-abstract-short" style="display: inline;">
        Graph Neural Networks (GNNs) have achieved significant <span class="search-hit mathjax">improvements</span> in various domains. Sparse Matrix-Matrix multiplication (SpMM) is a fundamental operator in GNNs, which performs a multiplication between a sparse matrix and a dense matrix. Accelerating SpMM on parallel hardware like GPUs can face the following challenges: From the GNN&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.03179v1-abstract-full').style.display = 'inline'; document.getElementById('2007.03179v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.03179v1-abstract-full" style="display: none;">
        Graph Neural Networks (GNNs) have achieved significant <span class="search-hit mathjax">improvements</span> in various domains. Sparse Matrix-Matrix multiplication (SpMM) is a fundamental operator in GNNs, which performs a multiplication between a sparse matrix and a dense matrix. Accelerating SpMM on parallel hardware like GPUs can face the following challenges: From the GNN <span class="search-hit mathjax">application</span> perspective, the compatibility needs to be considered. General GNN algorithms require SpMM-like operations (e.g., pooling) between matrices, which are not supported in current high-performance GPU libraries (e.g., Nvidia cuSPARSE). Moreover, the sophisticated preprocessing in previous implementations will lead to heavy data format conversion overheads in GNN frameworks. From the GPU hardware perspective, <span class="search-hit mathjax">optimizations</span> in SpMV (Sparse Matrix-Vector) designs on GPUs do not apply well to SpMM. SpMM exposes the column-wise parallelism in the dense output matrix, but straightforward generalization from SpMV leads to inefficient, uncoalesced access to sparse matrix in global <span class="search-hit mathjax">memory</span>. The sparse row data can be reused among GPU threads, which is neither possible in SpMM designs inherited from SpMV.
  To tackle these challenges, we propose GE-SpMM. GE-SpMM performs SpMM-like operation on sparse matrices represented in the most common Compressed Sparse Row (CSR) format, so it can be embedded in GNN frameworks with no preprocessing overheads and support general GNN algorithms. We introduce the Coalesced Row Caching method to process columns in parallel and ensure coalesced access to sparse matrix data. We also present the Coarse-grained Warp Merging to <span class="search-hit mathjax">reduce</span> redundant data loading among GPU warps. Experiments on a real-world graph dataset show that GE-SpMM achieves up to 1.41X speedup over Nvidia cuSPARSE and up to 1.81X over GraphBLAST. We also embed GE-SpMM in GNN frameworks and get up to 3.67X speedup over popular GNN models like GCN and GraphSAGE.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.03179v1-abstract-full').style.display = 'none'; document.getElementById('2007.03179v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 July, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in International Conference for High Performance Computing, Networking, Storage, and Analysis (SC&#39;20)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.01348">arXiv:2007.01348</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.01348">pdf</a>, <a href="https://arxiv.org/format/2007.01348">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Neural Network Deployment for Microcontroller
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Unlu%2C+H">Hasan Unlu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.01348v1-abstract-short" style="display: inline;">
        Edge computing for neural networks is getting important especially for low power <span class="search-hit mathjax">applications</span> and offline devices. TensorFlow Lite and PyTorch Mobile were released for this purpose. But they mainly support mobile devices instead of microcontroller level yet. Microcontroller support is an emerging area now. There are many approaches to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.01348v1-abstract-full').style.display = 'inline'; document.getElementById('2007.01348v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.01348v1-abstract-full" style="display: none;">
        Edge computing for neural networks is getting important especially for low power <span class="search-hit mathjax">applications</span> and offline devices. TensorFlow Lite and PyTorch Mobile were released for this purpose. But they mainly support mobile devices instead of microcontroller level yet. Microcontroller support is an emerging area now. There are many approaches to <span class="search-hit mathjax">reduce</span> network size and compute load like pruning, binarization and layer manipulation i.e. operator reordering. This paper is going to explore and generalize convolution neural network deployment for microcontrollers with two novel <span class="search-hit mathjax">optimization</span> proposals offering <span class="search-hit mathjax">memory</span> saving and compute efficiency in 2D convolutions as well as fully connected layers. The first one is in-place max-pooling, if the stride is greater than or equal to pooling kernel size. The second <span class="search-hit mathjax">optimization</span> is to use ping-pong buffers between layers to <span class="search-hit mathjax">reduce</span> <span class="search-hit mathjax">memory</span> consumption significantly. The <span class="search-hit mathjax">memory</span> savings and performance will be compared with CMSIS-NN framework developed for ARM Cortex-M CPUs. The final purpose is to develop a tool consuming PyTorch model with trained network weights, and it turns into an <span class="search-hit mathjax">optimized</span> inference engine(forward pass) in C/C++ for low <span class="search-hit mathjax">memory</span>(kilobyte level) and limited computing capable microcontrollers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.01348v1-abstract-full').style.display = 'none'; document.getElementById('2007.01348v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 July, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.00072">arXiv:2007.00072</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.00072">pdf</a>, <a href="https://arxiv.org/format/2007.00072">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Data Movement Is All You Need: A Case Study on <span class="search-hit mathjax">Optimizing</span> Transformers
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ivanov%2C+A">Andrei Ivanov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dryden%2C+N">Nikoli Dryden</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ben-Nun%2C+T">Tal Ben-Nun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Shigang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoefler%2C+T">Torsten Hoefler</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.00072v2-abstract-short" style="display: inline;">
        &hellip;of the most important machine learning workloads today. Training one is a very compute-intensive task, often taking days or weeks, and significant attention has been given to <span class="search-hit mathjax">optimizing</span> transformers. Despite this, existing implementations do not efficiently utilize GPUs. We find that data movement is the key bottleneck when training. Due to Amdahl&#39;s Law&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.00072v2-abstract-full').style.display = 'inline'; document.getElementById('2007.00072v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.00072v2-abstract-full" style="display: none;">
        Transformers have become widely used for language modeling and sequence learning tasks, and are one of the most important machine learning workloads today. Training one is a very compute-intensive task, often taking days or weeks, and significant attention has been given to <span class="search-hit mathjax">optimizing</span> transformers. Despite this, existing implementations do not efficiently utilize GPUs. We find that data movement is the key bottleneck when training. Due to Amdahl&#39;s Law and massive <span class="search-hit mathjax">improvements</span> in compute performance, training has now become <span class="search-hit mathjax">memory</span>-bound. Further, existing frameworks use suboptimal data layouts. Using these insights, we present a recipe for globally <span class="search-hit mathjax">optimizing</span> data movement in transformers. We <span class="search-hit mathjax">reduce</span> data movement by up to 22.91% and overall achieve a 1.30x performance <span class="search-hit mathjax">improvement</span> over state-of-the-art frameworks when training BERT. Our approach is <span class="search-hit mathjax">applicable</span> more broadly to <span class="search-hit mathjax">optimizing</span> deep neural networks, and offers insight into how to tackle emerging performance bottlenecks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.00072v2-abstract-full').style.display = 'none'; document.getElementById('2007.00072v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 July, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 June, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 6 figures; minor clarifications and style updates</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.16411">arXiv:2006.16411</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.16411">pdf</a>, <a href="https://arxiv.org/format/2006.16411">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hands-off Model Integration in Spatial Index Structures
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hadian%2C+A">Ali Hadian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+A">Ankit Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Heinis%2C+T">Thomas Heinis</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.16411v2-abstract-short" style="display: inline;">
        Spatial indexes are crucial for the analysis of the increasing amounts of spatial data, for example generated through IoT <span class="search-hit mathjax">applications</span>. The plethora of indexes that has been developed in recent decades has primarily been optimised for disk. With increasing amounts of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.16411v2-abstract-full').style.display = 'inline'; document.getElementById('2006.16411v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.16411v2-abstract-full" style="display: none;">
        Spatial indexes are crucial for the analysis of the increasing amounts of spatial data, for example generated through IoT <span class="search-hit mathjax">applications</span>. The plethora of indexes that has been developed in recent decades has primarily been optimised for disk. With increasing amounts of <span class="search-hit mathjax">memory</span> even on commodity machines, however, moving them to main <span class="search-hit mathjax">memory</span> is an option. Doing so opens up the opportunity to use additional <span class="search-hit mathjax">optimizations</span> that are only amenable to main <span class="search-hit mathjax">memory</span>. In this paper we thus explore the opportunity to use light-weight machine learning models to accelerate queries on spatial indexes. We do so by exploring the potential of using interpolation and similar techniques on the R-tree, arguably the most broadly used spatial index. As we show in our experimental analysis, the query execution time can be <span class="search-hit mathjax">reduced</span> by up to 60% while simultaneously shrinking the index&#39;s <span class="search-hit mathjax">memory</span> footprint by over 90%
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.16411v2-abstract-full').style.display = 'none'; document.getElementById('2006.16411v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 August, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 June, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the 2nd International Workshop on Applied AI for Database Systems and Applications (2020)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.14270">arXiv:2006.14270</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.14270">pdf</a>, <a href="https://arxiv.org/format/2006.14270">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Adaptation and Self-Organizing Systems">nlin.AO</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TCSI.2020.3035575">10.1109/TCSI.2020.3035575 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ultra-Low-Power FDSOI Neural Circuits for Extreme-Edge Neuromorphic Intelligence
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rubino%2C+A">Arianna Rubino</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Livanelioglu%2C+C">Can Livanelioglu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiao%2C+N">Ning Qiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Payvand%2C+M">Melika Payvand</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Indiveri%2C+G">Giacomo Indiveri</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.14270v2-abstract-short" style="display: inline;">
        Recent years have seen an increasing interest in the development of artificial intelligence circuits and systems for edge computing <span class="search-hit mathjax">applications</span>. In-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.14270v2-abstract-full').style.display = 'inline'; document.getElementById('2006.14270v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.14270v2-abstract-full" style="display: none;">
        Recent years have seen an increasing interest in the development of artificial intelligence circuits and systems for edge computing <span class="search-hit mathjax">applications</span>. In-<span class="search-hit mathjax">memory</span> computing mixed-signal neuromorphic architectures provide promising ultra-low-power solutions for edge-computing sensory-processing <span class="search-hit mathjax">applications</span>, thanks to their ability to emulate spiking neural networks in real-time. The fine-grain parallelism offered by this approach allows such neural circuits to process the sensory data efficiently by adapting their dynamics to the ones of the sensed signals, without having to resort to the time-multiplexed computing paradigm of von Neumann architectures. To <span class="search-hit mathjax">reduce</span> power consumption even further, we present a set of mixed-signal analog/digital circuits that exploit the features of advanced Fully-Depleted Silicon on Insulator (FDSOI) integration processes. Specifically, we explore the options of advanced FDSOI technologies to address analog design issues and <span class="search-hit mathjax">optimize</span> the design of the synapse integrator and of the adaptive neuron circuits accordingly. We present circuit simulation results and demonstrate the circuit&#39;s ability to produce biologically plausible neural dynamics with compact designs, <span class="search-hit mathjax">optimized</span> for the realization of large-scale spiking neural networks in neuromorphic processors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.14270v2-abstract-full').style.display = 'none'; document.getElementById('2006.14270v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 July, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 June, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 9 figures, TCAS submission</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          B.7; C.3
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.13603">arXiv:2006.13603</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.13603">pdf</a>, <a href="https://arxiv.org/format/2006.13603">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Instrumentation and Detectors">physics.ins-det</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1051/epjconf/202024509002">10.1051/epjconf/202024509002 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Integrating LHCb workflows on HPC resources: status and strategies
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Stagni%2C+F">Federico Stagni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Valassi%2C+A">Andrea Valassi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Romanovskiy%2C+V">Vladimir Romanovskiy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.13603v1-abstract-short" style="display: inline;">
        &hellip;Computing (HPC) supercomputers are expected to play an increasingly important role in HEP computing in the coming years. While HPC resources are not necessarily the <span class="search-hit mathjax">optimal</span> fit for HEP workflows, computing time at HPC centers on an opportunistic basis has already been available to the LHC experiments for some time, and it is also possible that part of the pl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.13603v1-abstract-full').style.display = 'inline'; document.getElementById('2006.13603v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.13603v1-abstract-full" style="display: none;">
        High Performance Computing (HPC) supercomputers are expected to play an increasingly important role in HEP computing in the coming years. While HPC resources are not necessarily the <span class="search-hit mathjax">optimal</span> fit for HEP workflows, computing time at HPC centers on an opportunistic basis has already been available to the LHC experiments for some time, and it is also possible that part of the pledged computing resources will be offered as CPU time allocations at HPC centers in the future. The integration of the experiment workflows to make the most efficient use of HPC resources is therefore essential. This paper describes the work that has been necessary to integrate LHCb workflows at a specific HPC site, the Marconi-A2 system at CINECA in Italy, where LHCb benefited from a joint PRACE (Partnership for Advanced Computing in Europe) allocation with the other Large Hadron Collider (LHC) experiments. This has required addressing two types of challenges: on the <span class="search-hit mathjax">software</span> <span class="search-hit mathjax">application</span> workloads, for optimising their performance on a many-core hardware architecture that differs significantly from those traditionally used in WLCG (Worldwide LHC Computing Grid), by <span class="search-hit mathjax">reducing</span> <span class="search-hit mathjax">memory</span> footprint using a multi-process approach; and in the distributed computing area, for submitting these workloads using more than one logical processor per job, which had never been done yet in LHCb.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.13603v1-abstract-full').style.display = 'none'; document.getElementById('2006.13603v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, submitted to CHEP2019 proceedings in EPJ Web of Conferences</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          C.1.4; C.2.4; D.1.3; D.4.7; D.2.8
        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        EPJ Web of Conferences 245, 09002 (2020)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.12133">arXiv:2006.12133</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.12133">pdf</a>, <a href="https://arxiv.org/format/2006.12133">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Optimizing</span> Placement of Heap <span class="search-hit mathjax">Memory</span> Objects in Energy-Constrained Hybrid <span class="search-hit mathjax">Memory</span> Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+T">Taeuk Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jamil%2C+S">Safdar Jamil</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+J">Joongeon Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+Y">Youngjae Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.12133v2-abstract-short" style="display: inline;">
        Main <span class="search-hit mathjax">memory</span> (DRAM) significantly impacts the power and energy utilization of the overall server system. Non-Volatile&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.12133v2-abstract-full').style.display = 'inline'; document.getElementById('2006.12133v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.12133v2-abstract-full" style="display: none;">
        Main <span class="search-hit mathjax">memory</span> (DRAM) significantly impacts the power and energy utilization of the overall server system. Non-Volatile <span class="search-hit mathjax">Memory</span> (NVM) devices, such as Phase Change <span class="search-hit mathjax">Memory</span> and Spin-Transfer Torque RAM, are suitable candidates for main <span class="search-hit mathjax">memory</span> to <span class="search-hit mathjax">reduce</span> energy consumption. But unlike DRAM, NVMs access latencies are higher than DRAM and NVM writes are more energy sensitive than DRAM write operations. Thus, Hybrid Main <span class="search-hit mathjax">Memory</span> Systems (HMMS) employing DRAM and NVM have been proposed to <span class="search-hit mathjax">reduce</span> the overall energy depletion of main <span class="search-hit mathjax">memory</span> while <span class="search-hit mathjax">optimizing</span> the performance of NVM. This paper proposes eMap, an <span class="search-hit mathjax">optimal</span> heap <span class="search-hit mathjax">memory</span> object placement planner in HMMS. eMap considers the object-level access patterns and energy consumption at the <span class="search-hit mathjax">application</span> level and provides an ideal placement strategy for each object to augment performance and energy utilization. eMap is equipped with two modules, eMPlan and eMDyn. Specifically, eMPlan is a static placement planner which provides one time placement policies for <span class="search-hit mathjax">memory</span> object to meet the energy budget while eMDyn is a runtime placement planner to consider the change in energy limiting constraint during the runtime and shuffles the <span class="search-hit mathjax">memory</span> objects by taking into account the access patterns as well as the migration cost in terms of energy and performance. The evaluation shows that our proposed solution satisfies both the energy limiting constraint and the performance. We compare our methodology with the state-of-the-art <span class="search-hit mathjax">memory</span> object classification and allocation (MOCA) framework. Our extensive evaluation shows that our proposed solution, eMPlan meets the energy constraint with 4.17 times less costly and <span class="search-hit mathjax">reducing</span> the energy consumption up to 14% with the same performance. eMDyn also satisfies the performance and energy requirement while considering the migration cost in terms of time and energy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.12133v2-abstract-full').style.display = 'none'; document.getElementById('2006.12133v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 June, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 June, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.11401">arXiv:2006.11401</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.11401">pdf</a>, <a href="https://arxiv.org/format/2006.11401">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DEED: A General Quantization Scheme for Communication Efficiency in Bits
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+T">Tian Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+P">Peijun Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+R">Ruoyu Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.11401v1-abstract-short" style="display: inline;">
        In distributed <span class="search-hit mathjax">optimization</span>, a popular technique to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.11401v1-abstract-full').style.display = 'inline'; document.getElementById('2006.11401v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.11401v1-abstract-full" style="display: none;">
        In distributed <span class="search-hit mathjax">optimization</span>, a popular technique to <span class="search-hit mathjax">reduce</span> communication is quantization. In this paper, we provide a general analysis framework for inexact gradient descent that is <span class="search-hit mathjax">applicable</span> to quantization schemes. We also propose a quantization scheme Double Encoding and Error Diminishing (DEED). DEED can achieve small communication complexity in three settings: frequent-communication large-<span class="search-hit mathjax">memory</span>, frequent-communication small-<span class="search-hit mathjax">memory</span>, and infrequent-communication (e.g. federated learning). More specifically, in the frequent-communication large-<span class="search-hit mathjax">memory</span> setting, DEED can be easily combined with Nesterov&#39;s method, so that the total number of bits required is $\tilde{O}( \sqrtÎº \log 1/Îµ)$, where $\tilde{O}$ hides numerical constant and $\log Îº$ factors. In the frequent-communication small-<span class="search-hit mathjax">memory</span> setting, DEED combined with SGD only requires $\tilde{O}( Îº\log 1/Îµ)$ number of bits in the interpolation regime. In the infrequent communication setting, DEED combined with Federated averaging requires a smaller total number of bits than Federated Averaging. All these algorithms converge at the same rate as their non-quantized versions, while using a smaller number of bits.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.11401v1-abstract-full').style.display = 'none'; document.getElementById('2006.11401v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68Q11; 90C25; 68W15
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          G.1.6
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.11077">arXiv:2006.11077</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.11077">pdf</a>, <a href="https://arxiv.org/format/2006.11077">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Horv%C3%A1th%2C+S">Samuel HorvÃ¡th</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Richt%C3%A1rik%2C+P">Peter RichtÃ¡rik</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.11077v2-abstract-short" style="display: inline;">
        Modern large-scale machine learning <span class="search-hit mathjax">applications</span> require stochastic <span class="search-hit mathjax">optimization</span> algorithms to be implemented on distributed compute systems. A key bottleneck of such systems is the communication overhead for exchanging information across the workers, such as stochastic gradients. Among the many techniques proposed to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.11077v2-abstract-full').style.display = 'inline'; document.getElementById('2006.11077v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.11077v2-abstract-full" style="display: none;">
        Modern large-scale machine learning <span class="search-hit mathjax">applications</span> require stochastic <span class="search-hit mathjax">optimization</span> algorithms to be implemented on distributed compute systems. A key bottleneck of such systems is the communication overhead for exchanging information across the workers, such as stochastic gradients. Among the many techniques proposed to remedy this issue, one of the most successful is the framework of compressed communication with error feedback (EF). EF remains the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top-$K$. In this paper, we propose a new and theoretically and practically better alternative to EF for dealing with contractive compressors. In particular, we propose a construction which can transform any contractive compressor into an induced unbiased compressor. Following this transformation, existing methods able to work with unbiased compressors can be applied. We show that our approach leads to vast <span class="search-hit mathjax">improvements</span> over EF, including <span class="search-hit mathjax">reduced</span> <span class="search-hit mathjax">memory</span> requirements, better communication complexity guarantees and fewer assumptions. We further extend our results to federated learning with partial participation following an arbitrary distribution over the nodes, and demonstrate the benefits thereof. We perform several numerical experiments which validate our theoretical findings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.11077v2-abstract-full').style.display = 'none'; document.getElementById('2006.11077v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 June, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 7 figures, published as a conference paper at ICLR 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.08173">arXiv:2006.08173</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.08173">pdf</a>, <a href="https://arxiv.org/format/2006.08173">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Neural gradients are near-lognormal: <span class="search-hit mathjax">improved</span> quantized and sparse training
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chmiel%2C+B">Brian Chmiel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ben-Uri%2C+L">Liad Ben-Uri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shkolnik%2C+M">Moran Shkolnik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoffer%2C+E">Elad Hoffer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Banner%2C+R">Ron Banner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soudry%2C+D">Daniel Soudry</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.08173v3-abstract-short" style="display: inline;">
        While training can mostly be accelerated by <span class="search-hit mathjax">reducing</span> the time needed to propagate neural gradients back throughout the model, most previous works focus on the quantization/pruning of weights and activations. These methods are often not&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.08173v3-abstract-full').style.display = 'inline'; document.getElementById('2006.08173v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.08173v3-abstract-full" style="display: none;">
        While training can mostly be accelerated by <span class="search-hit mathjax">reducing</span> the time needed to propagate neural gradients back throughout the model, most previous works focus on the quantization/pruning of weights and activations. These methods are often not <span class="search-hit mathjax">applicable</span> to neural gradients, which have very different statistical properties. Distinguished from weights and activations, we find that the distribution of neural gradients is approximately lognormal. Considering this, we suggest two closed-form analytical methods to <span class="search-hit mathjax">reduce</span> the computational and <span class="search-hit mathjax">memory</span> burdens of neural gradients. The first method <span class="search-hit mathjax">optimizes</span> the floating-point format and scale of the gradients. The second method accurately sets sparsity thresholds for gradient pruning. Each method achieves state-of-the-art results on ImageNet. To the best of our knowledge, this paper is the first to (1) quantize the gradients to 6-bit floating-point formats, or (2) achieve up to 85% gradient sparsity -- in each case without accuracy degradation. Reference implementation accompanies the paper.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.08173v3-abstract-full').style.display = 'none'; document.getElementById('2006.08173v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 June, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.06890">arXiv:2006.06890</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.06890">pdf</a>, <a href="https://arxiv.org/format/2006.06890">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EMOGI: Efficient <span class="search-hit mathjax">Memory</span>-access for Out-of-<span class="search-hit mathjax">memory</span> Graph-traversal In GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Min%2C+S+W">Seung Won Min</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mailthody%2C+V+S">Vikram Sharma Mailthody</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qureshi%2C+Z">Zaid Qureshi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+J">Jinjun Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ebrahimi%2C+E">Eiman Ebrahimi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hwu%2C+W">Wen-mei Hwu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.06890v2-abstract-short" style="display: inline;">
        &hellip;To exploit the massive parallelism, developers are increasingly interested in using GPUs for graph traversal. However, due to their sizes, graphs often do not fit into the GPU <span class="search-hit mathjax">memory</span>. Prior works have either used input data pre-processing/partitioning or UVM to migrate chunks of data from the host&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.06890v2-abstract-full').style.display = 'inline'; document.getElementById('2006.06890v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.06890v2-abstract-full" style="display: none;">
        Modern analytics and recommendation systems are increasingly based on graph data that capture the relations between entities being analyzed. Practical graphs come in huge sizes, offer massive parallelism, and are stored in sparse-matrix formats such as CSR. To exploit the massive parallelism, developers are increasingly interested in using GPUs for graph traversal. However, due to their sizes, graphs often do not fit into the GPU <span class="search-hit mathjax">memory</span>. Prior works have either used input data pre-processing/partitioning or UVM to migrate chunks of data from the host <span class="search-hit mathjax">memory</span> to the GPU <span class="search-hit mathjax">memory</span>. However, the large, multi-dimensional, and sparse nature of graph data presents a major challenge to these schemes and results in significant amplification of data movement and <span class="search-hit mathjax">reduced</span> effective data throughput. In this work, we propose EMOGI, an alternative approach to traverse graphs that do not fit in GPU <span class="search-hit mathjax">memory</span> using direct cacheline-sized access to data stored in host <span class="search-hit mathjax">memory</span>. This paper addresses the open question of whether a sufficiently large number of overlapping cacheline-sized accesses can be sustained to 1) tolerate the long latency to host <span class="search-hit mathjax">memory</span>, 2) fully utilize the available bandwidth, and 3) achieve favorable execution performance. We analyze the data access patterns of several graph traversal <span class="search-hit mathjax">applications</span> in GPU over PCIe using an FPGA to understand the cause of poor external bandwidth utilization. By carefully coalescing and aligning external <span class="search-hit mathjax">memory</span> requests, we show that we can minimize the number of PCIe transactions and nearly fully utilize the PCIe bandwidth even with direct cache-line accesses to the host <span class="search-hit mathjax">memory</span>. EMOGI achieves 2.92$\times$ speedup on average compared to the <span class="search-hit mathjax">optimized</span> UVM implementations in various graph traversal <span class="search-hit mathjax">applications</span>. We also show that EMOGI scales better than a UVM-based solution when the system uses higher bandwidth interconnects such as PCIe 4.0.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.06890v2-abstract-full').style.display = 'none'; document.getElementById('2006.06890v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 June, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.06052">arXiv:2006.06052</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.06052">pdf</a>, <a href="https://arxiv.org/format/2006.06052">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Fluid Dynamics">physics.flu-dyn</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.jocs.2020.101285">10.1016/j.jocs.2020.101285 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Accelerating linear solvers for Stokes problems with C++ metaprogramming
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Demidov%2C+D">Denis Demidov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mu%2C+L">Lin Mu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bin Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.06052v3-abstract-short" style="display: inline;">
        &hellip;important in computational fluid mechanics. The discontinuous Galerkin finite element methods have become increasingly popular for incompressible flow problems but their <span class="search-hit mathjax">application</span> is limited due to high computational cost. We describe the C++&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.06052v3-abstract-full').style.display = 'inline'; document.getElementById('2006.06052v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.06052v3-abstract-full" style="display: none;">
        The efficient solution of large sparse saddle point systems is very important in computational fluid mechanics. The discontinuous Galerkin finite element methods have become increasingly popular for incompressible flow problems but their <span class="search-hit mathjax">application</span> is limited due to high computational cost. We describe the C++ <span class="search-hit mathjax">programming</span> techniques that may help to accelerate linear solvers for such problems. The approach is based on the policy-based design pattern and partial template specialization, and is implemented in the open source AMGCL library. The efficiency is demonstrated with the example of accelerating an iterative solver of a discontinuous Galerkin finite element method for the Stokes problem. The implementation allows selecting algorithmic components of the solver by adjusting template parameters without any changes to the codebase. It is possible to switch the system matrix to use small statically sized blocks to store the nonzero values, or use a mixed precision solution, which results in up to 4 times speedup, and <span class="search-hit mathjax">reduces</span> the <span class="search-hit mathjax">memory</span> footprint of the algorithm by about 40\%. We evaluate both monolithic and composite preconditioning strategies for the 3 benchmark problems. The performance of the proposed solution is compared with a multithreaded direct Pardiso solver and a parallel iterative PETSc solver.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.06052v3-abstract-full').style.display = 'none'; document.getElementById('2006.06052v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 December, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 June, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          35-04; 65-04; 65Y05; 65Y10; 65Y15; 97N80
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.05503">arXiv:2006.05503</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.05503">pdf</a>, <a href="https://arxiv.org/ps/2006.05503">ps</a>, <a href="https://arxiv.org/format/2006.05503">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stochastic Automata Network for Performance Evaluation of Heterogeneous SoC Communication
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Deshmukh%2C+U">Ulhas Deshmukh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sahula%2C+V">Vineet Sahula</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.05503v1-abstract-short" style="display: inline;">
        To meet ever increasing demand for performance of emerging System-on-Chip (SoC) <span class="search-hit mathjax">applications</span>, designer employ techniques for concurrent communication between components. Hence communication architecture becomes complex and major performance bottleneck. An early performance evaluation of communication architecture is the key to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.05503v1-abstract-full').style.display = 'inline'; document.getElementById('2006.05503v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.05503v1-abstract-full" style="display: none;">
        To meet ever increasing demand for performance of emerging System-on-Chip (SoC) <span class="search-hit mathjax">applications</span>, designer employ techniques for concurrent communication between components. Hence communication architecture becomes complex and major performance bottleneck. An early performance evaluation of communication architecture is the key to <span class="search-hit mathjax">reduce</span> design time, time-to-market and consequently cost of the system. Moreover, it helps to <span class="search-hit mathjax">optimize</span> system performance by selecting appropriate communication architecture. However, performance model of concurrent communication is complex to describe and hard to solve. In this paper, we propose methodology for performance evaluation of bus based communication architectures, modeling for which is based on modular Stochastic Automata Network (SAN). We employ Generalized Semi Markov Process (GSMP) model for each module of the SAN that emulates dynamic behavior of a Processing Element (PE) of an SoC architecture. The proposed modeling approach provides an early estimation of performance parameters viz. <span class="search-hit mathjax">memory</span> bandwidth, average queue length at <span class="search-hit mathjax">memory</span> and average waiting time seen by a processing element; while we provide parameters viz. number of processing elements, the mean computation time of processing elements and the first and second moments of connection time between processing elements and <span class="search-hit mathjax">memories</span>, as input to the model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.05503v1-abstract-full').style.display = 'none'; document.getElementById('2006.05503v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.02230">arXiv:2006.02230</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.02230">pdf</a>, <a href="https://arxiv.org/format/2006.02230">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PolyDL: Polyhedral <span class="search-hit mathjax">Optimizations</span> for Creation of High Performance DL primitives
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tavarageri%2C+S">Sanket Tavarageri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Heinecke%2C+A">Alexander Heinecke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Avancha%2C+S">Sasikanth Avancha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goyal%2C+G">Gagandeep Goyal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Upadrasta%2C+R">Ramakrishna Upadrasta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaul%2C+B">Bharat Kaul</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.02230v2-abstract-short" style="display: inline;">
        Deep Neural Networks (DNNs) have revolutionized many aspects of our lives. The use of DNNs is becoming ubiquitous including in <span class="search-hit mathjax">softwares</span> for image recognition, speech recognition, speech synthesis, language translation, to name a few. he training of DNN architectures however is computationally expensive. Once the model is created, its use in the intended&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.02230v2-abstract-full').style.display = 'inline'; document.getElementById('2006.02230v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.02230v2-abstract-full" style="display: none;">
        Deep Neural Networks (DNNs) have revolutionized many aspects of our lives. The use of DNNs is becoming ubiquitous including in <span class="search-hit mathjax">softwares</span> for image recognition, speech recognition, speech synthesis, language translation, to name a few. he training of DNN architectures however is computationally expensive. Once the model is created, its use in the intended <span class="search-hit mathjax">application</span> - the inference task, is computationally heavy too and the inference needs to be fast for real time use. For obtaining high performance today, the <span class="search-hit mathjax">code</span> of Deep Learning (DL) primitives <span class="search-hit mathjax">optimized</span> for specific architectures by expert programmers exposed via libraries is the norm. However, given the constant emergence of new DNN architectures, creating hand <span class="search-hit mathjax">optimized</span> <span class="search-hit mathjax">code</span> is expensive, slow and is not scalable.
  To address this performance-productivity challenge, in this paper we present compiler algorithms to <span class="search-hit mathjax">automatically</span> generate high performance implementations of DL primitives that closely match the performance of hand <span class="search-hit mathjax">optimized</span> libraries. We develop novel data reuse analysis algorithms using the polyhedral model to derive efficient execution schedules <span class="search-hit mathjax">automatically</span>. In addition, because most DL primitives use some variant of matrix multiplication at their core, we develop a flexible framework where it is possible to plug in library implementations of the same in lieu of a subset of the loops. We show that such a hybrid compiler plus a minimal library-use approach results in state-of-the-art performance. We develop compiler algorithms to also perform operator fusions that <span class="search-hit mathjax">reduce</span> data movement through the <span class="search-hit mathjax">memory</span> hierarchy of the computer system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.02230v2-abstract-full').style.display = 'none'; document.getElementById('2006.02230v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 November, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 June, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: substantial text overlap with arXiv:2002.02145</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.00422">arXiv:2006.00422</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.00422">pdf</a>, <a href="https://arxiv.org/format/2006.00422">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary Neuromorphic Vision Sensors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Singla%2C+D">Deepak Singla</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mohan%2C+V">Vivek Mohan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pulluri%2C+T">Tarun Pulluri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ussa%2C+A">Andres Ussa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramesh%2C+B">Bharath Ramesh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Basu%2C+A">Arindam Basu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.00422v1-abstract-short" style="display: inline;">
        In this paper, we present a hybrid event-frame approach for detecting and tracking objects recorded by a stationary neuromorphic vision sensor (NVS) used in the <span class="search-hit mathjax">application</span> of traffic monitoring with a hardware efficient processing pipeline that&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.00422v1-abstract-full').style.display = 'inline'; document.getElementById('2006.00422v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.00422v1-abstract-full" style="display: none;">
        In this paper, we present a hybrid event-frame approach for detecting and tracking objects recorded by a stationary neuromorphic vision sensor (NVS) used in the <span class="search-hit mathjax">application</span> of traffic monitoring with a hardware efficient processing pipeline that <span class="search-hit mathjax">optimizes</span> <span class="search-hit mathjax">memory</span> and computational needs. The usage of NVS gives the advantage of rejecting background while it has a unique disadvantage of fragmented objects due to lack of events generated by smooth areas such as glass windows. To exploit the background removal, we propose an event based binary image (EBBI) creation that signals presence or absence of events in a frame duration. This <span class="search-hit mathjax">reduces</span> <span class="search-hit mathjax">memory</span> requirement and enables usage of simple algorithms like median filtering and connected component labeling (CCL) for denoise and region proposal (RP) respectively. To overcome the fragmentation issue, a YOLO inspired neural network based detector and classifier (NNDC) to merge fragmented region proposals has been proposed. Finally, a simplified version of Kalman filter, termed overlap based tracker (OT), exploiting overlap between detections and tracks is proposed with heuristics to overcome occlusion.
  The proposed pipeline is evaluated using more than 5 hours of traffic recordings. Our proposed hybrid architecture outperformed (AUC = $0.45$) Deep learning (DL) based tracker SiamMask (AUC = $0.33$) operating on simultaneously recorded RGB frames while requiring $2200\times$ less computations. Compared to pure event based mean shift (AUC = $0.31$), our approach requires $68\times$ more computations but provides much better performance. Finally, we also evaluated our performance on two different NVS: DAVIS and CeleX and demonstrated similar gains. To the best of our knowledge, this is the first report where an NVS based solution is directly compared to other simultaneously recorded frame based method and shows tremendous promise.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.00422v1-abstract-full').style.display = 'none'; document.getElementById('2006.00422v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 12 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.10905">arXiv:2005.10905</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.10905">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.imavis.2020.103932">10.1016/j.imavis.2020.103932 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Detection and Tracking in Videos with Identification Features
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Munjal%2C+B">Bharti Munjal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aftab%2C+A+R">Abdul Rafey Aftab</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amin%2C+S">Sikandar Amin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brandlmaier%2C+M+D">Meltem D. Brandlmaier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tombari%2C+F">Federico Tombari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Galasso%2C+F">Fabio Galasso</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.10905v2-abstract-short" style="display: inline;">
        &hellip;results in higher performance for both tasks, but they require a high frame-rate as a strict requirement for performance. This is assumption is often violated in real-world <span class="search-hit mathjax">applications</span>, when models run on embedded devices, often at only a few frames per second.
  Videos at low frame-rate suffer from large object displacements. Here re-identification featur&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.10905v2-abstract-full').style.display = 'inline'; document.getElementById('2005.10905v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.10905v2-abstract-full" style="display: none;">
        Recent works have shown that combining object detection and tracking tasks, in the case of video data, results in higher performance for both tasks, but they require a high frame-rate as a strict requirement for performance. This is assumption is often violated in real-world <span class="search-hit mathjax">applications</span>, when models run on embedded devices, often at only a few frames per second.
  Videos at low frame-rate suffer from large object displacements. Here re-identification features may support to match large-displaced object detections, but current joint detection and re-identification formulations degrade the detector performance, as these two are contrasting tasks. In the real-world <span class="search-hit mathjax">application</span> having separate detector and re-id models is often not feasible, as both the <span class="search-hit mathjax">memory</span> and runtime effectively double.
  Towards robust long-term tracking <span class="search-hit mathjax">applicable</span> to <span class="search-hit mathjax">reduced</span>-computational-power devices, we propose the first joint <span class="search-hit mathjax">optimization</span> of detection, tracking and re-identification features for videos. Notably, our joint <span class="search-hit mathjax">optimization</span> maintains the detector performance, a typical multi-task challenge. At inference time, we leverage detections for tracking (tracking-by-detection) when the objects are visible, detectable and slowly moving in the image. We leverage instead re-identification features to match objects which disappeared (e.g. due to occlusion) for several frames or were not tracked due to fast motion (or low-frame-rate videos). Our proposed method reaches the state-of-the-art on MOT, it ranks 1st in the UA-DETRAC&#39;18 tracking challenge among online trackers, and 3rd overall.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.10905v2-abstract-full').style.display = 'none'; document.getElementById('2005.10905v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 May, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 May, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Image and Vision Computing Journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.10709">arXiv:2005.10709</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.10709">pdf</a>, <a href="https://arxiv.org/format/2005.10709">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TASO: Time and Space <span class="search-hit mathjax">Optimization</span> for <span class="search-hit mathjax">Memory</span>-Constrained DNN Inference
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+Y">Yuan Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Radu%2C+V">Valentin Radu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=O%27Boyle%2C+M+F+P">Michael F. P. O&#39;Boyle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.10709v1-abstract-short" style="display: inline;">
        Convolutional neural networks (CNNs) are used in many embedded <span class="search-hit mathjax">applications</span>, from industrial robotics and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.10709v1-abstract-full').style.display = 'inline'; document.getElementById('2005.10709v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.10709v1-abstract-full" style="display: none;">
        Convolutional neural networks (CNNs) are used in many embedded <span class="search-hit mathjax">applications</span>, from industrial robotics and <span class="search-hit mathjax">automation</span> systems to biometric identification on mobile devices. State-of-the-art classification is typically achieved by large networks, which are prohibitively expensive to run on mobile and embedded devices with tightly constrained <span class="search-hit mathjax">memory</span> and energy budgets. We propose an approach for ahead-of-time domain specific <span class="search-hit mathjax">optimization</span> of CNN models, based on an integer linear <span class="search-hit mathjax">programming</span> (ILP) for selecting primitive operations to implement convolutional layers. We <span class="search-hit mathjax">optimize</span> the trade-off between execution time and <span class="search-hit mathjax">memory</span> consumption by: 1) attempting to minimize execution time across the whole network by selecting data layouts and primitive operations to implement each layer; and 2) allocating an appropriate workspace that reflects the upper bound of <span class="search-hit mathjax">memory</span> footprint per layer. These two <span class="search-hit mathjax">optimization</span> strategies can be used to run any CNN on any platform with a C compiler. Our evaluation with a range of popular ImageNet neural architectures (GoogleNet, AlexNet, VGG, ResNet and SqueezeNet) on the ARM Cortex-A15 yields speedups of 8x compared to a greedy algorithm based primitive selection, <span class="search-hit mathjax">reduces</span> <span class="search-hit mathjax">memory</span> requirement by 2.2x while sacrificing only 15% of inference time compared to a solver that considers inference time only. In addition, our <span class="search-hit mathjax">optimization</span> approach exposes a range of <span class="search-hit mathjax">optimal</span> points for different configurations across the Pareto frontier of <span class="search-hit mathjax">memory</span> and latency trade-off, which can be used under arbitrary system constraints.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.10709v1-abstract-full').style.display = 'none'; document.getElementById('2005.10709v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.09748">arXiv:2005.09748</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.09748">pdf</a>, <a href="https://arxiv.org/format/2005.09748">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Virtual Block Interface: A Flexible Alternative to the Conventional Virtual <span class="search-hit mathjax">Memory</span> Framework
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hajinazar%2C+N">Nastaran Hajinazar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Patel%2C+P">Pratyush Patel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Patel%2C+M">Minesh Patel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kanellopoulos%2C+K">Konstantinos Kanellopoulos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghose%2C+S">Saugata Ghose</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ausavarungnirun%2C+R">Rachata Ausavarungnirun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oliveira%2C+G+F+d">Geraldo Francisco de Oliveira Jr.</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Appavoo%2C+J">Jonathan Appavoo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Seshadri%2C+V">Vivek Seshadri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mutlu%2C+O">Onur Mutlu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.09748v1-abstract-short" style="display: inline;">
        Computers continue to diversify with respect to system designs, emerging <span class="search-hit mathjax">memory</span> technologies, and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.09748v1-abstract-full').style.display = 'inline'; document.getElementById('2005.09748v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.09748v1-abstract-full" style="display: none;">
        Computers continue to diversify with respect to system designs, emerging <span class="search-hit mathjax">memory</span> technologies, and <span class="search-hit mathjax">application</span> <span class="search-hit mathjax">memory</span> demands. Unfortunately, continually adapting the conventional virtual <span class="search-hit mathjax">memory</span> framework to each possible system configuration is challenging, and often results in performance loss or requires non-trivial workarounds. To address these challenges, we propose a new virtual <span class="search-hit mathjax">memory</span> framework, the Virtual Block Interface (VBI). We design VBI based on the key idea that delegating <span class="search-hit mathjax">memory</span> management duties to hardware can <span class="search-hit mathjax">reduce</span> the overheads and <span class="search-hit mathjax">software</span> complexity associated with virtual <span class="search-hit mathjax">memory</span>. VBI introduces a set of variable-sized virtual blocks (VBs) to <span class="search-hit mathjax">applications</span>. Each VB is a contiguous region of the globally-visible VBI address space, and an <span class="search-hit mathjax">application</span> can allocate each semantically meaningful unit of information (e.g., a data structure) in a separate VB. VBI decouples access protection from <span class="search-hit mathjax">memory</span> allocation and address translation. While the OS controls which <span class="search-hit mathjax">programs</span> have access to which VBs, dedicated hardware in the <span class="search-hit mathjax">memory</span> controller manages the physical <span class="search-hit mathjax">memory</span> allocation and address translation of the VBs. This approach enables several architectural <span class="search-hit mathjax">optimizations</span> to (1) efficiently and flexibly cater to different and increasingly diverse system configurations, and (2) eliminate key inefficiencies of conventional virtual <span class="search-hit mathjax">memory</span>. We demonstrate the benefits of VBI with two important use cases: (1) <span class="search-hit mathjax">reducing</span> the overheads of address translation (for both native execution and virtual machine environments), as VBI <span class="search-hit mathjax">reduces</span> the number of translation requests and associated <span class="search-hit mathjax">memory</span> accesses; and (2) two heterogeneous main <span class="search-hit mathjax">memory</span> architectures, where VBI increases the effectiveness of managing fast <span class="search-hit mathjax">memory</span> regions. For both cases, VBI significanttly <span class="search-hit mathjax">improves</span> performance over conventional virtual <span class="search-hit mathjax">memory</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.09748v1-abstract-full').style.display = 'none'; document.getElementById('2005.09748v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.03002">arXiv:2005.03002</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.03002">pdf</a>, <a href="https://arxiv.org/format/2005.03002">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TVLSI.2020.3017595">10.1109/TVLSI.2020.3017595 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Computing-in-<span class="search-hit mathjax">Memory</span> for Performance and Energy Efficient Homomorphic Encryption
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Reis%2C+D">Dayane Reis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Takeshita%2C+J">Jonathan Takeshita</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jung%2C+T">Taeho Jung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niemier%2C+M">Michael Niemier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+X+S">Xiaobo Sharon Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.03002v2-abstract-short" style="display: inline;">
        &hellip;the practicality of HE schemes remains to be demonstrated. In this regard, the enormous size of ciphertexts involved in HE computations degrades computational efficiency. Near-<span class="search-hit mathjax">memory</span> Processing (NMP) and Computing-in-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.03002v2-abstract-full').style.display = 'inline'; document.getElementById('2005.03002v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.03002v2-abstract-full" style="display: none;">
        Homomorphic encryption (HE) allows direct computations on encrypted data. Despite numerous research efforts, the practicality of HE schemes remains to be demonstrated. In this regard, the enormous size of ciphertexts involved in HE computations degrades computational efficiency. Near-<span class="search-hit mathjax">memory</span> Processing (NMP) and Computing-in-<span class="search-hit mathjax">memory</span> (CiM) - paradigms where computation is done within the <span class="search-hit mathjax">memory</span> boundaries - represent architectural solutions for <span class="search-hit mathjax">reducing</span> latency and energy associated with data transfers in data-intensive <span class="search-hit mathjax">applications</span> such as HE. This paper introduces CiM-HE, a Computing-in-<span class="search-hit mathjax">memory</span> (CiM) architecture that can support operations for the B/FV scheme, a somewhat homomorphic encryption scheme for general computation. CiM-HE hardware consists of customized peripherals such as sense amplifiers, adders, bit-shifters, and sequencing circuits. The peripherals are based on CMOS technology, and could support computations with <span class="search-hit mathjax">memory</span> cells of different technologies. Circuit-level simulations are used to evaluate our CiM-HE framework assuming a 6T-SRAM <span class="search-hit mathjax">memory</span>. We compare our CiM-HE implementation against (i) two <span class="search-hit mathjax">optimized</span> CPU HE implementations, and (ii) an FPGA-based HE accelerator implementation. When compared to a CPU solution, CiM-HE obtains speedups between 4.6x and 9.1x, and energy savings between 266.4x and 532.8x for homomorphic multiplications (the most expensive HE operation). Also, a set of four end-to-end tasks, i.e., mean, variance, linear regression, and inference are up to 1.1x, 7.7x, 7.1x, and 7.5x faster (and 301.1x, 404.6x, 532.3x, and 532.8x more energy efficient). Compared to CPU-based HE in a previous work, CiM-HE obtain 14.3x speed-up and &gt;2600x energy savings. Finally, our design offers 2.2x speed-up with 88.1x energy savings compared to a state-of-the-art FPGA-based accelerator.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.03002v2-abstract-full').style.display = 'none'; document.getElementById('2005.03002v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 August, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 May, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Transactions on Very Large Scale Integration (VLSI) Systems ( Volume: 28, Issue: 11, Nov. 2020)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.02088">arXiv:2005.02088</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.02088">pdf</a>, <a href="https://arxiv.org/format/2005.02088">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards QoS-Aware and Resource-Efficient GPU Microservices Based on Spatial Multitasking GPUs In Datacenters
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Quan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+K">Kaihua Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+N">Ningxin Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Z">Zhiyi Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leng%2C+J">Jingwen Leng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+W">Wenli Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+M">Minyi Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.02088v1-abstract-short" style="display: inline;">
        While prior researches focus on CPU-based microservices, they are not <span class="search-hit mathjax">applicable</span> for GPU-based microservices due to the different contention patterns. It is challenging to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.02088v1-abstract-full').style.display = 'inline'; document.getElementById('2005.02088v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.02088v1-abstract-full" style="display: none;">
        While prior researches focus on CPU-based microservices, they are not <span class="search-hit mathjax">applicable</span> for GPU-based microservices due to the different contention patterns. It is challenging to <span class="search-hit mathjax">optimize</span> the resource utilization while guaranteeing the QoS for GPU microservices. We find that the overhead is caused by inter microservice communication, GPU resource contention and imbalanced throughput within microservice pipeline. We propose Camelot, a runtime system that manages GPU micorservices considering the above factors. In Camelot, a global <span class="search-hit mathjax">memory</span>-based communication mechanism enables onsite data sharing that significantly <span class="search-hit mathjax">reduces</span> the end-to-end latencies of user queries. We also propose two contention aware resource allocation policies that either maximize the peak supported service load or minimize the resource usage at low load while ensuring the required QoS. The two policies consider the microservice pipeline effect and the runtime GPU resource contention when allocating resources for the microservices. Compared with state-of-the-art work, Camelot increases the supported peak load by up to 64.5% with limited GPUs, and <span class="search-hit mathjax">reduces</span> 35% resource usage at low load while achieving the desired 99%-ile latency target.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.02088v1-abstract-full').style.display = 'none'; document.getElementById('2005.02088v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.00057">arXiv:2005.00057</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.00057">pdf</a>, <a href="https://arxiv.org/format/2005.00057">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CP-NAS: Child-Parent Neural Architecture Search for Binary Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhuo%2C+L">Li&#39;an Zhuo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Baochang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Hanlin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+L">Linlin Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chen Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Y">Yanjun Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Doermann%2C+D">David Doermann</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.00057v2-abstract-short" style="display: inline;">
        Neural architecture search (NAS) proves to be among the best approaches for many tasks by generating an <span class="search-hit mathjax">application</span>-adaptive neural architecture, which is still challenged by high computational cost and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.00057v2-abstract-full').style.display = 'inline'; document.getElementById('2005.00057v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.00057v2-abstract-full" style="display: none;">
        Neural architecture search (NAS) proves to be among the best approaches for many tasks by generating an <span class="search-hit mathjax">application</span>-adaptive neural architecture, which is still challenged by high computational cost and <span class="search-hit mathjax">memory</span> consumption. At the same time, 1-bit convolutional neural networks (CNNs) with binarized weights and activations show their potential for resource-limited embedded devices. One natural approach is to use 1-bit CNNs to <span class="search-hit mathjax">reduce</span> the computation and <span class="search-hit mathjax">memory</span> cost of NAS by taking advantage of the strengths of each in a unified framework. To this end, a Child-Parent (CP) model is introduced to a differentiable NAS to search the binarized architecture (Child) under the supervision of a full-precision model (Parent). In the search stage, the Child-Parent model uses an indicator generated by the child and parent model accuracy to evaluate the performance and abandon operations with less potential. In the training stage, a kernel-level CP loss is introduced to <span class="search-hit mathjax">optimize</span> the binarized network. Extensive experiments demonstrate that the proposed CP-NAS achieves a comparable accuracy with traditional NAS on both the CIFAR and ImageNet databases. It achieves the accuracy of $95.27\%$ on CIFAR-10, $64.3\%$ on ImageNet with binarized weights and activations, and a $30\%$ faster search than prior arts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.00057v2-abstract-full').style.display = 'none'; document.getElementById('2005.00057v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 May, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 April, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.14471">arXiv:2004.14471</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.14471">pdf</a>, <a href="https://arxiv.org/format/2004.14471">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mainlining Databases: Supporting Fast Transactional Workloads on Universal Columnar Data File Formats
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+T">Tianyu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Butrovich%2C+M">Matthew Butrovich</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ngom%2C+A">Amadou Ngom</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lim%2C+W+S">Wan Shen Lim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McKinney%2C+W">Wes McKinney</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pavlo%2C+A">Andrew Pavlo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.14471v1-abstract-short" style="display: inline;">
        &hellip;has given rise to open-source columnar data formats. The advantage of these formats is that they help organizations avoid repeatedly converting data to a new format for each <span class="search-hit mathjax">application</span>. These formats, however, are read-only, and organizations must use a heavy-weight transformation process to load data from on-line transactional processing (OLTP) systems. We&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.14471v1-abstract-full').style.display = 'inline'; document.getElementById('2004.14471v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.14471v1-abstract-full" style="display: none;">
        The proliferation of modern data processing tools has given rise to open-source columnar data formats. The advantage of these formats is that they help organizations avoid repeatedly converting data to a new format for each <span class="search-hit mathjax">application</span>. These formats, however, are read-only, and organizations must use a heavy-weight transformation process to load data from on-line transactional processing (OLTP) systems. We aim to <span class="search-hit mathjax">reduce</span> or even eliminate this process by developing a storage architecture for in-<span class="search-hit mathjax">memory</span> database management systems (DBMSs) that is aware of the eventual usage of its data and emits columnar storage blocks in a universal open-source format. We introduce relaxations to common analytical data formats to efficiently update records and rely on a lightweight transformation process to convert blocks to a read-<span class="search-hit mathjax">optimized</span> layout when they are cold. We also describe how to access data from third-party analytical tools with minimal serialization overhead. To evaluate our work, we implemented our storage engine based on the Apache Arrow format and integrated it into the DB-X DBMS. Our experiments show that our approach achieves comparable performance with dedicated OLTP DBMSs while enabling orders-of-magnitude faster data exports to external data science and machine learning tools than existing methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.14471v1-abstract-full').style.display = 'none'; document.getElementById('2004.14471v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.11075">arXiv:2004.11075</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.11075">pdf</a>, <a href="https://arxiv.org/format/2004.11075">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fast Convex Relaxations using Graph Discretizations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Geiping%2C+J">Jonas Geiping</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gaede%2C+F">Fjedor Gaede</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bauermeister%2C+H">Hartmut Bauermeister</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moeller%2C+M">Michael Moeller</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.11075v2-abstract-short" style="display: inline;">
        Matching and partitioning problems are fundamentals of computer vision <span class="search-hit mathjax">applications</span> with examples in multilabel segmentation, stereo estimation and optical-flow computation. These tasks can be posed as non-convex energy minimization problems and solved near-globally&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.11075v2-abstract-full').style.display = 'inline'; document.getElementById('2004.11075v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.11075v2-abstract-full" style="display: none;">
        Matching and partitioning problems are fundamentals of computer vision <span class="search-hit mathjax">applications</span> with examples in multilabel segmentation, stereo estimation and optical-flow computation. These tasks can be posed as non-convex energy minimization problems and solved near-globally <span class="search-hit mathjax">optimal</span> by recent convex lifting approaches. Yet, applying these techniques comes with a significant computational effort, <span class="search-hit mathjax">reducing</span> their feasibility in practical <span class="search-hit mathjax">applications</span>. We discuss spatial discretization of continuous partitioning problems into a graph structure, generalizing discretization onto a Cartesian grid. This setup allows us to faithfully work on super-pixel graphs constructed by SLIC or Cut-Pursuit, massively decreasing the computational effort for lifted partitioning problems compared to a Cartesian grid, while <span class="search-hit mathjax">optimal</span> energy values remain similar: The global matching is still solved near-globally <span class="search-hit mathjax">optimal</span>. We discuss this methodology in detail and show examples in multi-label segmentation by minimal partitions and stereo estimation, where we demonstrate that the proposed graph discretization can <span class="search-hit mathjax">reduce</span> runtime as well as <span class="search-hit mathjax">memory</span> consumption of convex relaxations of matching problems by up to a factor of 10.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.11075v2-abstract-full').style.display = 'none'; document.getElementById('2004.11075v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 April, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 9 figures. BMVC 2020 publication</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.05962">arXiv:2004.05962</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.05962">pdf</a>, <a href="https://arxiv.org/format/2004.05962">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.cmpb.2020.105431">10.1016/j.cmpb.2020.105431 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Accelerating B-spline Interpolation on GPUs: <span class="search-hit mathjax">Application</span> to Medical Image Registration
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zachariadis%2C+O">Orestis Zachariadis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Teatini%2C+A">Andrea Teatini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Satpute%2C+N">Nitin Satpute</a>, 
      
      <a href="/search/?searchtype=author&amp;query=G%C3%B3mez-Luna%2C+J">Juan GÃ³mez-Luna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mutlu%2C+O">Onur Mutlu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elle%2C+O+J">Ole Jakob Elle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olivares%2C+J">JoaquÃ­n Olivares</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.05962v2-abstract-short" style="display: inline;">
        &hellip;data. However, such IGS tasks are computationally demanding, especially when applied to 3D medical images, due to the complexity and amount of data involved. Therefore, <span class="search-hit mathjax">optimization</span> of IGS algorithms is greatly desirable, for example, to perform image registration tasks intra-operatively and to enable real-time&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.05962v2-abstract-full').style.display = 'inline'; document.getElementById('2004.05962v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.05962v2-abstract-full" style="display: none;">
        Background and Objective. B-spline interpolation (BSI) is a popular technique in the context of medical imaging due to its adaptability and robustness in 3D object modeling. A field that utilizes BSI is Image Guided Surgery (IGS). IGS provides navigation using medical images, which can be segmented and reconstructed into 3D models, often through BSI. Image registration tasks also use BSI to align pre-operative data to intra-operative data. However, such IGS tasks are computationally demanding, especially when applied to 3D medical images, due to the complexity and amount of data involved. Therefore, <span class="search-hit mathjax">optimization</span> of IGS algorithms is greatly desirable, for example, to perform image registration tasks intra-operatively and to enable real-time <span class="search-hit mathjax">applications</span>. A traditional CPU does not have sufficient computing power to achieve these goals. In this paper, we introduce a novel GPU implementation of BSI to accelerate the calculation of the deformation field in non-rigid image registration algorithms.
  Methods. Our BSI implementation on GPUs minimizes the data that needs to be moved between <span class="search-hit mathjax">memory</span> and processing cores during loading of the input grid, and leverages the large on-chip GPU register file for reuse of input values. Moreover, we re-formulate our method as trilinear interpolations to <span class="search-hit mathjax">reduce</span> computational complexity and increase accuracy. To provide pre-clinical validation of our method and demonstrate its benefits in medical <span class="search-hit mathjax">applications</span>, we integrate our <span class="search-hit mathjax">improved</span> BSI into a registration workflow for compensation of liver deformation (caused by pneumoperitoneum, i.e., inflation of the abdomen) and evaluate its performance.
  Results. Our approach <span class="search-hit mathjax">improves</span> the performance of BSI by an average of 6.5x and interpolation accuracy by 2x compared to three state-of-the-art GPU implementations. We observe up to 34% acceleration of non-rigid image registration.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.05962v2-abstract-full').style.display = 'none'; document.getElementById('2004.05962v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 April, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 April, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in CMPB</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Comput. Methods Programs Biomed. 193 (2020) 105431
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.13054">arXiv:2003.13054</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.13054">pdf</a>, <a href="https://arxiv.org/format/2003.13054">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Analytical Model of <span class="search-hit mathjax">Memory</span>-Bound <span class="search-hit mathjax">Applications</span> Compiled with High Level Synthesis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=D%C3%A1vila-Guzm%C3%A1n%2C+M+A">Maria A. DÃ¡vila-GuzmÃ¡n</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tejero%2C+R+G">RubÃ©n Gran Tejero</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Villarroya-Gaud%C3%B3%2C+M">MarÃ­a Villarroya-GaudÃ³</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gracia%2C+D+S">DarÃ­o SuÃ¡rez Gracia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.13054v1-abstract-short" style="display: inline;">
        The increasing demand of dedicated accelerators to <span class="search-hit mathjax">improve</span> energy efficiency and performance has highlighted FPGAs as a promising option to deliver both. However,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.13054v1-abstract-full').style.display = 'inline'; document.getElementById('2003.13054v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.13054v1-abstract-full" style="display: none;">
        The increasing demand of dedicated accelerators to <span class="search-hit mathjax">improve</span> energy efficiency and performance has highlighted FPGAs as a promising option to deliver both. However, <span class="search-hit mathjax">programming</span> FPGAs in hardware description languages requires long time and effort to achieve <span class="search-hit mathjax">optimal</span> results, which discourages many programmers from adopting this technology.
  High Level Synthesis tools <span class="search-hit mathjax">improve</span> the accessibility to FPGAs, but the <span class="search-hit mathjax">optimization</span> process is still time expensive due to the large compilation time, between minutes and days, required to generate a single bitstream. Whereas placing and routing take most of this time, the RTL pipeline and <span class="search-hit mathjax">memory</span> organization are known in seconds. This early information about the organization of the upcoming bitstream is enough to provide an accurate and fast performance model.
  This paper presents a performance analytical model for HLS designs focused on <span class="search-hit mathjax">memory</span> bound <span class="search-hit mathjax">applications</span>. With a careful analysis of the generated <span class="search-hit mathjax">memory</span> architecture and DRAM organization, the model predicts the execution time with a maximum error of 9.2% for a set of representative <span class="search-hit mathjax">applications</span>. Compared with previous works, our predictions <span class="search-hit mathjax">reduce</span> on average at least $2\times$ the estimation error.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.13054v1-abstract-full').style.display = 'none'; document.getElementById('2003.13054v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.10690">arXiv:2003.10690</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.10690">pdf</a>, <a href="https://arxiv.org/format/2003.10690">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1117/12.2551024">10.1117/12.2551024 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Organ Segmentation From Full-size CT Images Using <span class="search-hit mathjax">Memory</span>-Efficient FCN
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chenglong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oda%2C+M">Masahiro Oda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mori%2C+K">Kensaku Mori</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.10690v1-abstract-short" style="display: inline;">
        In this work, we present a <span class="search-hit mathjax">memory</span>-efficient fully convolutional network (FCN) incorporated with several&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.10690v1-abstract-full').style.display = 'inline'; document.getElementById('2003.10690v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.10690v1-abstract-full" style="display: none;">
        In this work, we present a <span class="search-hit mathjax">memory</span>-efficient fully convolutional network (FCN) incorporated with several <span class="search-hit mathjax">memory</span>-<span class="search-hit mathjax">optimized</span> techniques to <span class="search-hit mathjax">reduce</span> the run-time GPU <span class="search-hit mathjax">memory</span> demand during training phase. In medical image segmentation tasks, subvolume cropping has become a common preprocessing. Subvolumes (or small patch volumes) were cropped to <span class="search-hit mathjax">reduce</span> GPU <span class="search-hit mathjax">memory</span> demand. However, small patch volumes capture less spatial context that leads to lower accuracy. As a pilot study, the purpose of this work is to propose a <span class="search-hit mathjax">memory</span>-efficient FCN which enables us to train the model on full size CT image directly without subvolume cropping, while maintaining the segmentation accuracy. We <span class="search-hit mathjax">optimize</span> our network from both architecture and implementation. With the development of computing hardware, such as graphics processing unit (GPU) and tensor processing unit (TPU), now deep learning <span class="search-hit mathjax">applications</span> is able to train networks with large datasets within acceptable time. Among these <span class="search-hit mathjax">applications</span>, semantic segmentation using fully convolutional network (FCN) also has gained a significant <span class="search-hit mathjax">improvement</span> against traditional image processing approaches in both computer vision and medical image processing fields. However, unlike general color images used in computer vision tasks, medical images have larger scales than color images such as 3D computed tomography (CT) images, micro CT images, and histopathological images. For training these medical images, the large demand of computing resource become a severe problem. In this paper, we present a <span class="search-hit mathjax">memory</span>-efficient FCN to tackle the high GPU <span class="search-hit mathjax">memory</span> demand challenge in organ segmentation problem from clinical CT images. The experimental results demonstrated that our GPU <span class="search-hit mathjax">memory</span> demand is about 40% of baseline architecture, parameter amount is about 30% of the baseline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.10690v1-abstract-full').style.display = 'none'; document.getElementById('2003.10690v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proc. SPIE 11314, Medical Imaging 2020: Computer-Aided Diagnosis, 113140I
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.08436">arXiv:2003.08436</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.08436">pdf</a>, <a href="https://arxiv.org/format/2003.08436">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Collaborative Distillation for Ultra-Resolution Universal Style Transfer
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Huan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yijun Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuehai Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+H">Haoji Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.08436v2-abstract-short" style="display: inline;">
        &hellip;leverage rich representations from deep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on large collections of images. Despite the effectiveness, its <span class="search-hit mathjax">application</span> is heavily constrained by the large model size to handle ultra-resolution images given limited&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.08436v2-abstract-full').style.display = 'inline'; document.getElementById('2003.08436v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.08436v2-abstract-full" style="display: none;">
        Universal style transfer methods typically leverage rich representations from deep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on large collections of images. Despite the effectiveness, its <span class="search-hit mathjax">application</span> is heavily constrained by the large model size to handle ultra-resolution images given limited <span class="search-hit mathjax">memory</span>. In this work, we present a new knowledge distillation method (named Collaborative Distillation) for encoder-decoder based neural style transfer to <span class="search-hit mathjax">reduce</span> the convolutional filters. The main idea is underpinned by a finding that the encoder-decoder pairs construct an exclusive collaborative relationship, which is regarded as a new kind of knowledge for style transfer models. Moreover, to overcome the feature size mismatch when applying collaborative distillation, a linear embedding loss is introduced to drive the student network to learn a linear embedding of the teacher&#39;s features. Extensive experiments show the effectiveness of our method when applied to different universal style transfer approaches (WCT and AdaIN), even if the model size is <span class="search-hit mathjax">reduced</span> by 15.5 times. Especially, on WCT with the compressed models, we achieve ultra-resolution (over 40 megapixels) universal style transfer on a 12GB GPU for the first time. Further experiments on <span class="search-hit mathjax">optimization</span>-based stylization scheme show the generality of our algorithm on different stylization paradigms. Our <span class="search-hit mathjax">code</span> and trained models are available at https://github.com/mingsun-tse/collaborative-distillation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.08436v2-abstract-full').style.display = 'none'; document.getElementById('2003.08436v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 March, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR 2020, higher-resolution images than the camera-ready version</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.06464">arXiv:2003.06464</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.06464">pdf</a>, <a href="https://arxiv.org/format/2003.06464">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LCP: A Low-Communication Parallelization Method for Fast Neural Network Inference in Image Recognition
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hadidi%2C+R">Ramyad Hadidi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Asgari%2C+B">Bahar Asgari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+J">Jiashen Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bae%2C+Y">Younmin Bae</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shim%2C+D+E">Da Eun Shim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+H">Hyojong Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lim%2C+S">Sung-Kyu Lim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ryoo%2C+M+S">Michael S. Ryoo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+H">Hyesoon Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.06464v2-abstract-short" style="display: inline;">
        Deep neural networks (DNNs) have inspired new studies in myriad edge <span class="search-hit mathjax">applications</span> with robots, autonomous agents, and Internet-of-things (IoT) devices. However, performing inference of DNNs in the edge is still a severe challenge, mainly because of the contradiction between the intensive resource requirements of DNNs and the tight resource availability in se&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.06464v2-abstract-full').style.display = 'inline'; document.getElementById('2003.06464v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.06464v2-abstract-full" style="display: none;">
        Deep neural networks (DNNs) have inspired new studies in myriad edge <span class="search-hit mathjax">applications</span> with robots, autonomous agents, and Internet-of-things (IoT) devices. However, performing inference of DNNs in the edge is still a severe challenge, mainly because of the contradiction between the intensive resource requirements of DNNs and the tight resource availability in several edge domains. Further, as communication is costly, taking advantage of other available edge devices by using data- or model-parallelism methods is not an effective solution. To benefit from available compute resources with low communication overhead, we propose the first DNN parallelization method for <span class="search-hit mathjax">reducing</span> the communication overhead in a distributed system. We propose a low-communication parallelization (LCP) method in which models consist of several almost-independent and narrow branches. LCP offers close-to-minimum communication overhead with better distribution and parallelization opportunities while significantly <span class="search-hit mathjax">reducing</span> <span class="search-hit mathjax">memory</span> footprint and computation compared to data- and model-parallelism methods. We deploy LCP models on three distributed systems: AWS instances, Raspberry Pis, and PYNQ boards. We also evaluate the performance of LCP models on a customized hardware (tailored for low latency) implemented on a small edge FPGA and as a 16mW 0.107mm2 ASIC @7nm chip. LCP models achieve a maximum and average speedups of 56x and 7x, compared to the originals, which could be <span class="search-hit mathjax">improved</span> by up to an average speedup of 33x by incorporating common <span class="search-hit mathjax">optimizations</span> such as pruning and quantization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.06464v2-abstract-full').style.display = 'none'; document.getElementById('2003.06464v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 November, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 March, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.01294">arXiv:2003.01294</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.01294">pdf</a>, <a href="https://arxiv.org/ps/2003.01294">ps</a>, <a href="https://arxiv.org/format/2003.01294">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Accelerating Generalized Benders Decomposition for Wireless Resource Allocation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+M">Mengyuan Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+N">Ning Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+G">Guanding Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+H">Huaiyu Dai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.01294v2-abstract-short" style="display: inline;">
        Generalized Benders decomposition (GBD) is a globally <span class="search-hit mathjax">optimal</span> algorithm for mixed integer nonlinear&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.01294v2-abstract-full').style.display = 'inline'; document.getElementById('2003.01294v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.01294v2-abstract-full" style="display: none;">
        Generalized Benders decomposition (GBD) is a globally <span class="search-hit mathjax">optimal</span> algorithm for mixed integer nonlinear <span class="search-hit mathjax">programming</span> (MINLP) problems, which are NP-hard and can be widely found in the area of wireless resource allocation. The main idea of GBD is decomposing an MINLP problem into a primal problem and a master problem, which are iteratively solved until their solutions converge. However, a direct implementation of GBD is time- and <span class="search-hit mathjax">memory</span>-consuming. The main bottleneck is the high complexity of the master problem, which increases over the iterations. Therefore, we propose to leverage machine learning (ML) techniques to accelerate GBD aiming at decreasing the complexity of the master problem. Specifically, we utilize two different ML techniques, classification and regression, to deal with this acceleration task. In this way, a cut classifier and a cut regressor are learned, respectively, to distinguish between useful and useless cuts. Only useful cuts are added to the master problem and thus the complexity of the master problem is <span class="search-hit mathjax">reduced</span>. By using a resource allocation problem in device-to-device communication networks as an example, we validate that the proposed method can <span class="search-hit mathjax">reduce</span> the computational complexity of GBD without loss of <span class="search-hit mathjax">optimality</span> and has strong generalization ability. The proposed method is <span class="search-hit mathjax">applicable</span> for solving various MINLP problems in wireless networks since the designs are invariant for different problems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.01294v2-abstract-full').style.display = 'none'; document.getElementById('2003.01294v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 March, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper was accpeted by the IEEE Transactions on Wireless Communications</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.00119">arXiv:2003.00119</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.00119">pdf</a>, <a href="https://arxiv.org/ps/2003.00119">ps</a>, <a href="https://arxiv.org/format/2003.00119">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Communication-<span class="search-hit mathjax">Optimal</span> Tilings for Projective Nested Loops with Arbitrary Bounds
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dinh%2C+G">Grace Dinh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Demmel%2C+J">James Demmel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.00119v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Reducing</span> communication - either between levels of a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.00119v1-abstract-full').style.display = 'inline'; document.getElementById('2003.00119v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.00119v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Reducing</span> communication - either between levels of a <span class="search-hit mathjax">memory</span> hierarchy or between processors over a network - is a key component of performance <span class="search-hit mathjax">optimization</span> (in both time and energy) for many problems, including dense linear algebra, particle interactions, and machine learning. For these problems, which can be represented as nested-loop computations, previous tiling based approaches have been used to find both lower bounds on the communication required to execute them and <span class="search-hit mathjax">optimal</span> rearrangements, or blockings, to attain such lower bounds. However, such general approaches have typically assumed the problem sizes are large, an assumption that is often not met in practice. For instance, the classical $(\text{# arithmetic operations})/(\text{cache size})^{1/2}$ lower bound for matrix multiplication is not tight for matrix-vector multiplications, which must read in at least $O(\text{# arithmetic operations})$ words of <span class="search-hit mathjax">memory</span>; similar issues occur for almost all convolutions in machine learning <span class="search-hit mathjax">applications</span>, which use extremely small filter sizes (and therefore, loop bounds).
  In this paper, we provide an efficient way to both find and obtain, via an appropriate, efficiently constructible blocking, communication lower bounds and matching tilings which attain these lower bounds for nested loop <span class="search-hit mathjax">programs</span> with arbitrary loop bounds that operate on multidimensional arrays in the projective case, where the array indices are subsets of the loop indices. Our approach works on all such problems, regardless of dimensionality, size, <span class="search-hit mathjax">memory</span> access patterns, or number of arrays, and directly applies to (among other examples) matrix multiplication and similar dense linear algebra operations, tensor contractions, n-body pairwise interactions, pointwise convolutions, and fully connected layers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.00119v1-abstract-full').style.display = 'none'; document.getElementById('2003.00119v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 February, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2002.08119">arXiv:2002.08119</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2002.08119">pdf</a>, <a href="https://arxiv.org/format/2002.08119">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Offloading and Resource Allocation with General Task Graph in Mobile Edge Computing: A Deep Reinforcement Learning Approach
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+J">Jia Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bi%2C+S">Suzhi Bi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y+A">Ying-Jun Angela Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2002.08119v1-abstract-short" style="display: inline;">
        In this paper, we consider a mobile-edge computing system, where an access point assists a mobile device (MD) to execute an <span class="search-hit mathjax">application</span> consisting of multiple tasks following a general task call graph. The objective is to jointly determine the offloading decision of each task and the resource allocation under time-varying wireless fading channels and stochas&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.08119v1-abstract-full').style.display = 'inline'; document.getElementById('2002.08119v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2002.08119v1-abstract-full" style="display: none;">
        In this paper, we consider a mobile-edge computing system, where an access point assists a mobile device (MD) to execute an <span class="search-hit mathjax">application</span> consisting of multiple tasks following a general task call graph. The objective is to jointly determine the offloading decision of each task and the resource allocation under time-varying wireless fading channels and stochastic edge computing capability, so that the energy-time cost (ETC) of the MD is minimized. Solving the problem is particularly hard due to the combinatorial offloading decisions and the strong coupling among task executions under the general dependency model. Conventional numerical <span class="search-hit mathjax">optimization</span> methods are inefficient to solve such a problem, especially when the problem size is large. To address the issue, we propose a deep reinforcement learning (DRL) framework based on the actor-critic learning structure. In particular, the actor network utilizes a DNN to learn the <span class="search-hit mathjax">optimal</span> mapping from the input states to the binary offloading decision of each task. Meanwhile, by analyzing the structure of the <span class="search-hit mathjax">optimal</span> solution, we derive a low-complexity algorithm for the critic network to quickly evaluate the ETC performance of the offloading decisions output by the actor network. With the low-complexity critic network, we can quickly select the best offloading action and subsequently store the state-action pair in an experience replay <span class="search-hit mathjax">memory</span> as the training dataset to continuously <span class="search-hit mathjax">improve</span> the action generation DNN. To further <span class="search-hit mathjax">reduce</span> the complexity, we show that the <span class="search-hit mathjax">optimal</span> offloading decision exhibits an one-climb structure, which can be utilized to significantly <span class="search-hit mathjax">reduce</span> the search space of action generation. Numerical results show that for various types of task graphs, the proposed algorithm achieves up to $99.1\%$ of the <span class="search-hit mathjax">optimal</span> performance while significantly <span class="search-hit mathjax">reducing</span> the computational complexity compared to the existing <span class="search-hit mathjax">optimization</span> methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.08119v1-abstract-full').style.display = 'none'; document.getElementById('2002.08119v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 February, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper is under minor revision with IEEE Transactions on Wireless Communications</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2002.07752">arXiv:2002.07752</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2002.07752">pdf</a>, <a href="https://arxiv.org/format/2002.07752">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Marvel: A Data-centric Compiler for DNN Operators on Spatial Accelerators
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chatarasi%2C+P">Prasanth Chatarasi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kwon%2C+H">Hyoukjun Kwon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raina%2C+N">Natesh Raina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Malik%2C+S">Saurabh Malik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haridas%2C+V">Vaisakh Haridas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Parashar%2C+A">Angshuman Parashar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pellauer%2C+M">Michael Pellauer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Krishna%2C+T">Tushar Krishna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sarkar%2C+V">Vivek Sarkar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2002.07752v2-abstract-short" style="display: inline;">
        The efficiency of a spatial DNN accelerator depends heavily on the compiler and its cost model ability to generate <span class="search-hit mathjax">optimized</span> mappings for various operators of DNN models on to the accelerator&#39;s compute and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.07752v2-abstract-full').style.display = 'inline'; document.getElementById('2002.07752v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2002.07752v2-abstract-full" style="display: none;">
        The efficiency of a spatial DNN accelerator depends heavily on the compiler and its cost model ability to generate <span class="search-hit mathjax">optimized</span> mappings for various operators of DNN models on to the accelerator&#39;s compute and <span class="search-hit mathjax">memory</span> resources. But, existing cost models lack a formal boundary over the operators for precise and tractable analysis, which poses adaptability challenges for new DNN operators. To address this challenge, we leverage the recently introduced Maestro Data-Centric (MDC) notation. We develop a formal understanding of DNN operators whose mappings can be described in the MDC notation, because any mapping adhering to the notation is always analyzable by the MDC&#39;s cost model. Furthermore, we introduce a transformation for translating mappings into the MDC notation for exploring the mapping space.
  Searching for the <span class="search-hit mathjax">optimal</span> mappings is challenging because of the large space of mappings, and this challenge gets exacerbated with new operators and diverse accelerator configurations.To address this challenge, we propose a decoupled off-chip/on-chip approach that decomposes the mapping space into off-chip and on-chip subspaces, and first <span class="search-hit mathjax">optimizes</span> the off-chip subspace followed by the on-chip subspace. The motivation for this decomposition is to <span class="search-hit mathjax">reduce</span> the size of the search space dramatically and also to prioritize the <span class="search-hit mathjax">optimization</span> of off-chip data movement, which is 2-3 orders of magnitude more compared to the on-chip data movement. We implemented our approach in a tool called {\em Marvel}, and another major benefit of our approach is that it is <span class="search-hit mathjax">applicable</span> to any DNN operator conformable with the MDC notation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.07752v2-abstract-full').style.display = 'none'; document.getElementById('2002.07752v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 February, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2001.06935">arXiv:2001.06935</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2001.06935">pdf</a>, <a href="https://arxiv.org/format/2001.06935">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/IPDPSW50202.2020.00046">10.1109/IPDPSW50202.2020.00046 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        75,000,000,000 Streaming Inserts/Second Using Hierarchical Hypersparse GraphBLAS Matrices
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kepner%2C+J">Jeremy Kepner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Davis%2C+T">Tim Davis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Byun%2C+C">Chansup Byun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arcand%2C+W">William Arcand</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bestor%2C+D">David Bestor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bergeron%2C+W">William Bergeron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gadepally%2C+V">Vijay Gadepally</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hubbell%2C+M">Matthew Hubbell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Houle%2C+M">Michael Houle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jones%2C+M">Michael Jones</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Klein%2C+A">Anna Klein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Michaleas%2C+P">Peter Michaleas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Milechin%2C+L">Lauren Milechin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mullen%2C+J">Julie Mullen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prout%2C+A">Andrew Prout</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosa%2C+A">Antonio Rosa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Samsi%2C+S">Siddharth Samsi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yee%2C+C">Charles Yee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Reuther%2C+A">Albert Reuther</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2001.06935v2-abstract-short" style="display: inline;">
        &hellip;C-library implements high performance hypersparse matrices with bindings to a variety of languages (Python, Julia, and Matlab/Octave). GraphBLAS provides a lightweight in-<span class="search-hit mathjax">memory</span> database implementation of hypersparse matrices that are ideal for analyzing many types of network data, while providing rigorous mathematical guarantees, such as linearity. Streamin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.06935v2-abstract-full').style.display = 'inline'; document.getElementById('2001.06935v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2001.06935v2-abstract-full" style="display: none;">
        The SuiteSparse GraphBLAS C-library implements high performance hypersparse matrices with bindings to a variety of languages (Python, Julia, and Matlab/Octave). GraphBLAS provides a lightweight in-<span class="search-hit mathjax">memory</span> database implementation of hypersparse matrices that are ideal for analyzing many types of network data, while providing rigorous mathematical guarantees, such as linearity. Streaming updates of hypersparse matrices put enormous pressure on the <span class="search-hit mathjax">memory</span> hierarchy. This work benchmarks an implementation of hierarchical hypersparse matrices that <span class="search-hit mathjax">reduces</span> <span class="search-hit mathjax">memory</span> pressure and dramatically increases the update rate into a hypersparse matrices. The parameters of hierarchical hypersparse matrices rely on controlling the number of entries in each level in the hierarchy before an update is cascaded. The parameters are easily tunable to achieve <span class="search-hit mathjax">optimal</span> performance for a variety of <span class="search-hit mathjax">applications</span>. Hierarchical hypersparse matrices achieve over 1,000,000 updates per second in a single instance. Scaling to 31,000 instances of hierarchical hypersparse matrices arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 75,000,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.06935v2-abstract-full').style.display = 'none'; document.getElementById('2001.06935v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 January, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4 pages, 4 figures, 28 references, accepted to IPDPS GrAPL 2020. arXiv admin note: substantial text overlap with arXiv:1907.04217</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2001.06598">arXiv:2001.06598</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2001.06598">pdf</a>, <a href="https://arxiv.org/format/2001.06598">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Scalable Decoder Micro-architecture for Fault-Tolerant Quantum Computing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+P">Poulami Das</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pattison%2C+C+A">Christopher A. Pattison</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Manne%2C+S">Srilatha Manne</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carmean%2C+D">Douglas Carmean</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Svore%2C+K">Krysta Svore</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qureshi%2C+M">Moinuddin Qureshi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delfosse%2C+N">Nicolas Delfosse</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2001.06598v1-abstract-short" style="display: inline;">
        &hellip;identify errors faster than they accumulate in the quantum computer and that must be implemented with minimum hardware resources in order to scale to the regime of practical <span class="search-hit mathjax">applications</span>. In this work, we consider surface&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.06598v1-abstract-full').style.display = 'inline'; document.getElementById('2001.06598v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2001.06598v1-abstract-full" style="display: none;">
        Quantum computation promises significant computational advantages over classical computation for some problems. However, quantum hardware suffers from much higher error rates than in classical hardware. As a result, extensive quantum error correction is required to execute a useful quantum algorithm. The decoder is a key component of the error correction scheme whose role is to identify errors faster than they accumulate in the quantum computer and that must be implemented with minimum hardware resources in order to scale to the regime of practical <span class="search-hit mathjax">applications</span>. In this work, we consider surface <span class="search-hit mathjax">code</span> error correction, which is the most popular family of error correcting <span class="search-hit mathjax">codes</span> for quantum computing, and we design a decoder micro-architecture for the Union-Find decoding algorithm. We propose a three-stage fully pipelined hardware implementation of the decoder that significantly speeds up the decoder. Then, we <span class="search-hit mathjax">optimize</span> the amount of decoding hardware required to perform error correction simultaneously over all the logical qubits of the quantum computer. By sharing resources between logical qubits, we obtain a 67% reduction of the number of hardware units and the <span class="search-hit mathjax">memory</span> capacity is <span class="search-hit mathjax">reduced</span> by 70%. Moreover, we <span class="search-hit mathjax">reduce</span> the bandwidth required for the decoding process by a factor at least 30x using low-overhead compression algorithms. Finally, we provide numerical evidence that our <span class="search-hit mathjax">optimized</span> micro-architecture can be executed fast enough to correct errors in a quantum computer.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.06598v1-abstract-full').style.display = 'none'; document.getElementById('2001.06598v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 January, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">19 pages, 159 references</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2001.01473">arXiv:2001.01473</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2001.01473">pdf</a>, <a href="https://arxiv.org/format/2001.01473">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3368826.3377904">10.1145/3368826.3377904 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AN5D: <span class="search-hit mathjax">Automated</span> Stencil Framework for High-Degree Temporal Blocking on GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Matsumura%2C+K">Kazuaki Matsumura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zohouri%2C+H+R">Hamid Reza Zohouri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wahib%2C+M">Mohamed Wahib</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Endo%2C+T">Toshio Endo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matsuoka%2C+S">Satoshi Matsuoka</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2001.01473v4-abstract-short" style="display: inline;">
        Stencil computation is one of the most widely-used compute patterns in high performance computing <span class="search-hit mathjax">applications</span>. Spatial and temporal blocking have been proposed to overcome the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.01473v4-abstract-full').style.display = 'inline'; document.getElementById('2001.01473v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2001.01473v4-abstract-full" style="display: none;">
        Stencil computation is one of the most widely-used compute patterns in high performance computing <span class="search-hit mathjax">applications</span>. Spatial and temporal blocking have been proposed to overcome the <span class="search-hit mathjax">memory</span>-bound nature of this type of computation by moving <span class="search-hit mathjax">memory</span> pressure from external <span class="search-hit mathjax">memory</span> to on-chip <span class="search-hit mathjax">memory</span> on GPUs. However, correctly implementing those <span class="search-hit mathjax">optimizations</span> while considering the complexity of the architecture and <span class="search-hit mathjax">memory</span> hierarchy of GPUs to achieve high performance is difficult. We propose AN5D, an <span class="search-hit mathjax">automated</span> stencil framework which is capable of <span class="search-hit mathjax">automatically</span> transforming and <span class="search-hit mathjax">optimizing</span> stencil patterns in a given C source <span class="search-hit mathjax">code</span>, and generating corresponding CUDA <span class="search-hit mathjax">code</span>. Parameter tuning in our framework is guided by our performance model. Our novel <span class="search-hit mathjax">optimization</span> strategy <span class="search-hit mathjax">reduces</span> shared <span class="search-hit mathjax">memory</span> and register pressure in comparison to existing implementations, allowing performance scaling up to a temporal blocking degree of 10. We achieve the highest performance reported so far for all evaluated stencil benchmarks on the state-of-the-art Tesla V100 GPU.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.01473v4-abstract-full').style.display = 'none'; document.getElementById('2001.01473v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 February, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 January, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2001.01395">arXiv:2001.01395</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2001.01395">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Accumulated Polar Feature-based Deep Learning for Efficient and Lightweight <span class="search-hit mathjax">Automatic</span> Modulation Classification with Channel Compensation Mechanism
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Teng%2C+C">Chieh-Fang Teng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chou%2C+C">Ching-Yao Chou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chun-Hsiang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+A">An-Yeu Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2001.01395v2-abstract-short" style="display: inline;">
        In next-generation communications, massive machine-type communications (mMTC) induce severe burden on base stations. To address such an issue, <span class="search-hit mathjax">automatic</span> modulation classification (AMC) can help to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.01395v2-abstract-full').style.display = 'inline'; document.getElementById('2001.01395v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2001.01395v2-abstract-full" style="display: none;">
        In next-generation communications, massive machine-type communications (mMTC) induce severe burden on base stations. To address such an issue, <span class="search-hit mathjax">automatic</span> modulation classification (AMC) can help to <span class="search-hit mathjax">reduce</span> signaling overhead by blindly recognizing the modulation types without handshaking. Thus, it plays an important role in future intelligent modems. The emerging deep learning (DL) technique stores intelligence in the network, resulting in superior performance over traditional approaches. However, conventional DL-based approaches suffer from heavy training overhead, <span class="search-hit mathjax">memory</span> overhead, and computational complexity, which severely hinder practical <span class="search-hit mathjax">applications</span> for resource-limited scenarios, such as Vehicle-to-Everything (V2X) <span class="search-hit mathjax">applications</span>. Furthermore, the overhead of online retraining under time-varying fading channels has not been studied in the prior arts. In this work, an accumulated polar feature-based DL with a channel compensation mechanism is proposed to cope with the aforementioned issues. Firstly, the simulation results show that learning features from the polar domain with historical data information can approach near-<span class="search-hit mathjax">optimal</span> performance while <span class="search-hit mathjax">reducing</span> training overhead by 99.8 times. Secondly, the proposed neural network-based channel estimator (NN-CE) can learn the channel response and compensate for the distorted channel with 13% <span class="search-hit mathjax">improvement</span>. Moreover, in applying this lightweight NN-CE in a time-varying fading channel, two efficient mechanisms of online retraining are proposed, which can <span class="search-hit mathjax">reduce</span> transmission overhead and retraining overhead by 90% and 76%, respectively. Finally, the performance of the proposed approach is evaluated and compared with prior arts on a public dataset to demonstrate its great efficiency and lightness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.01395v2-abstract-full').style.display = 'none'; document.getElementById('2001.01395v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 February, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 January, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 13 figures, 8 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2001.00281">arXiv:2001.00281</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2001.00281">pdf</a>, <a href="https://arxiv.org/format/2001.00281">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ZeroQ: A Novel Zero Shot Quantization Framework
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Y">Yaohui Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Z">Zhewei Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Z">Zhen Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gholami%2C+A">Amir Gholami</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahoney%2C+M+W">Michael W. Mahoney</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keutzer%2C+K">Kurt Keutzer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2001.00281v1-abstract-short" style="display: inline;">
        Quantization is a promising approach for <span class="search-hit mathjax">reducing</span> the inference time and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.00281v1-abstract-full').style.display = 'inline'; document.getElementById('2001.00281v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2001.00281v1-abstract-full" style="display: none;">
        Quantization is a promising approach for <span class="search-hit mathjax">reducing</span> the inference time and <span class="search-hit mathjax">memory</span> footprint of neural networks. However, most existing quantization methods require access to the original training dataset for retraining during quantization. This is often not possible for <span class="search-hit mathjax">applications</span> with sensitive or proprietary data, e.g., due to privacy and security concerns. Existing zero-shot quantization methods use different heuristics to address this, but they result in poor performance, especially when quantizing to ultra-low precision. Here, we propose ZeroQ , a novel zero-shot quantization framework to address this. ZeroQ enables mixed-precision quantization without any access to the training or validation data. This is achieved by <span class="search-hit mathjax">optimizing</span> for a Distilled Dataset, which is engineered to match the statistics of batch normalization across different layers of the network. ZeroQ supports both uniform and mixed-precision quantization. For the latter, we introduce a novel Pareto frontier based method to <span class="search-hit mathjax">automatically</span> determine the mixed-precision bit setting for all layers, with no manual search involved. We extensively test our proposed method on a diverse set of models, including ResNet18/50/152, MobileNetV2, ShuffleNet, SqueezeNext, and InceptionV3 on ImageNet, as well as RetinaNet-ResNet50 on the Microsoft COCO dataset. In particular, we show that ZeroQ can achieve 1.71\% higher accuracy on MobileNetV2, as compared to the recently proposed DFQ method. Importantly, ZeroQ has a very low computational overhead, and it can finish the entire quantization process in less than 30s (0.5\% of one epoch training time of ResNet50 on ImageNet). We have open-sourced the ZeroQ framework\footnote{https://github.com/amirgholami/ZeroQ}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.00281v1-abstract-full').style.display = 'none'; document.getElementById('2001.00281v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 January, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1912.06976">arXiv:1912.06976</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1912.06976">pdf</a>, <a href="https://arxiv.org/format/1912.06976">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Tutorial and Open Source <span class="search-hit mathjax">Software</span> for the Efficient Evaluation of Gravity and Magnetic Kernels
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hogue%2C+J+D">Jarom D Hogue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Renaut%2C+R+A">Rosemary A Renaut</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vatankhah%2C+S">Saeed Vatankhah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1912.06976v1-abstract-short" style="display: inline;">
        &hellip;problem. In each case, the structure facilitates fast forward computation using two-dimensional fast Fourier transforms. The construction of the kernel matrices and the <span class="search-hit mathjax">application</span> of the transform for fast forward multiplication, for each problem, is carefully described. But, for purposes of comparison with the transform approach, the generation of the uniq&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.06976v1-abstract-full').style.display = 'inline'; document.getElementById('1912.06976v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1912.06976v1-abstract-full" style="display: none;">
        Fast computation of three-dimensional gravity and magnetic forward models is considered. Measurement data is assumed to be obtained on a uniform grid which is staggered with respect to the discretization of the parameter volume. Then, the resulting kernel sensitivity matrices exhibit block-Toeplitz Toeplitz-block (BTTB) structure. These matrices are symmetric for the gravity problem but non-symmetric for the magnetic problem. In each case, the structure facilitates fast forward computation using two-dimensional fast Fourier transforms. The construction of the kernel matrices and the <span class="search-hit mathjax">application</span> of the transform for fast forward multiplication, for each problem, is carefully described. But, for purposes of comparison with the transform approach, the generation of the unique entries that define a given kernel matrix is also explained. It is also demonstrated how the matrices, and hence transforms, are adjusted when padding around the volume domain is introduced. The transform algorithms for fast forward matrix multiplication with the sensitivity matrix and its transpose, without the direct construction of the relevant matrices, are presented. Numerical experiments demonstrate the significant reduction in computation time that is achieved using the transform implementation. Moreover, it becomes feasible, both in terms of <span class="search-hit mathjax">reduced</span> <span class="search-hit mathjax">memory</span> requirements and computational time, to implement the transform algorithms for large three-dimensional volumes. All presented algorithms, including with variable padding, are <span class="search-hit mathjax">coded</span> for <span class="search-hit mathjax">optimal</span> <span class="search-hit mathjax">memory</span>, storage and computation as an open source MATLAB <span class="search-hit mathjax">code</span> which can be adapted for any convolution kernel which generates a BTTB matrix. This work, therefore, provides a general tool for the efficient simulation of gravity and magnetic field data, as well as any formulation which admits a sensitivity matrix with the required structure.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.06976v1-abstract-full').style.display = 'none'; document.getElementById('1912.06976v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 December, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1912.03507">arXiv:1912.03507</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1912.03507">pdf</a>, <a href="https://arxiv.org/format/1912.03507">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generalized Data Placement Strategies for Racetrack <span class="search-hit mathjax">Memories</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Khan%2C+A+A">Asif Ali Khan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goens%2C+A">Andres Goens</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hameed%2C+F">Fazal Hameed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castrillon%2C+J">Jeronimo Castrillon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1912.03507v1-abstract-short" style="display: inline;">
        Ultra-dense non-volatile racetrack <span class="search-hit mathjax">memories</span> (RTMs) have been investigated at various levels in the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.03507v1-abstract-full').style.display = 'inline'; document.getElementById('1912.03507v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1912.03507v1-abstract-full" style="display: none;">
        Ultra-dense non-volatile racetrack <span class="search-hit mathjax">memories</span> (RTMs) have been investigated at various levels in the <span class="search-hit mathjax">memory</span> hierarchy for <span class="search-hit mathjax">improved</span> performance and <span class="search-hit mathjax">reduced</span> energy consumption. However, the innate shift operations in RTMs hinder their <span class="search-hit mathjax">applicability</span> to replace low-latency on-chip <span class="search-hit mathjax">memories</span>. Recent research has demonstrated that intelligent placement of <span class="search-hit mathjax">memory</span> objects in RTMs can significantly <span class="search-hit mathjax">reduce</span> the amount of shifts with no hardware overhead, albeit for specific system setups. However, existing placement strategies may lead to sub-<span class="search-hit mathjax">optimal</span> performance when applied to different architectures. In this paper we look at generalized data placement mechanisms that <span class="search-hit mathjax">improve</span> upon existing ones by taking into account the underlying <span class="search-hit mathjax">memory</span> architecture and the timing and liveliness information of <span class="search-hit mathjax">memory</span> objects. We propose a novel heuristic and a formulation using genetic algorithms that <span class="search-hit mathjax">optimize</span> key performance parameters. We show that, on average, our generalized approach <span class="search-hit mathjax">improves</span> the number of shifts, performance and energy consumption by 4.3x, 46% and 55% respectively compared to the state-of-the-art.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.03507v1-abstract-full').style.display = 'none'; document.getElementById('1912.03507v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 December, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.13252">arXiv:1911.13252</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.13252">pdf</a>, <a href="https://arxiv.org/format/1911.13252">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An <span class="search-hit mathjax">Optimized</span> and Energy-Efficient Parallel Implementation of Non-Iteratively Trained Recurrent Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zini%2C+J+E">Julia El Zini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rizk%2C+Y">Yara Rizk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Awad%2C+M">Mariette Awad</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.13252v1-abstract-short" style="display: inline;">
        Recurrent neural networks (RNN) have been successfully applied to various sequential decision-making tasks, natural language processing <span class="search-hit mathjax">applications</span>, and time-series predictions. Such networks are usually trained through back-propagation through time (BPTT) which is prohibitively expensive, especially when the length of the time dependencies and the number o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.13252v1-abstract-full').style.display = 'inline'; document.getElementById('1911.13252v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.13252v1-abstract-full" style="display: none;">
        Recurrent neural networks (RNN) have been successfully applied to various sequential decision-making tasks, natural language processing <span class="search-hit mathjax">applications</span>, and time-series predictions. Such networks are usually trained through back-propagation through time (BPTT) which is prohibitively expensive, especially when the length of the time dependencies and the number of hidden neurons increase. To <span class="search-hit mathjax">reduce</span> the training time, extreme learning machines (ELMs) have been recently applied to RNN training, reaching a 99\% speedup on some <span class="search-hit mathjax">applications</span>. Due to its non-iterative nature, ELM training, when parallelized, has the potential to reach higher speedups than BPTT.
  In this work, we present \opt, an <span class="search-hit mathjax">optimized</span> parallel RNN training algorithm based on ELM that takes advantage of the GPU shared <span class="search-hit mathjax">memory</span> and of parallel QR factorization algorithms to efficiently reach <span class="search-hit mathjax">optimal</span> solutions. The theoretical analysis of the proposed algorithm is presented on six RNN architectures, including LSTM and GRU, and its performance is empirically tested on ten time-series prediction <span class="search-hit mathjax">applications</span>. \opt~is shown to reach up to 845 times speedup over its sequential counterpart and to require up to 20x less time to train than parallel BPTT.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.13252v1-abstract-full').style.display = 'none'; document.getElementById('1911.13252v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.12598">arXiv:1911.12598</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.12598">pdf</a>, <a href="https://arxiv.org/format/1911.12598">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Online Pricing with Reserve Price Constraint for Personal Data Markets
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+C">Chaoyue Niu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Z">Zhenzhe Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+F">Fan Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+S">Shaojie Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+G">Guihai Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.12598v1-abstract-short" style="display: inline;">
        &hellip;sequential queries. We thus propose a contextual dynamic pricing mechanism with the reserve price constraint, which features the properties of ellipsoid for efficient online <span class="search-hit mathjax">optimization</span>, and can support linear and non-linear market value models with uncertainty. In particular, under low uncertainty, our pricing mechanism provides a worst-case regret logarit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.12598v1-abstract-full').style.display = 'inline'; document.getElementById('1911.12598v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.12598v1-abstract-full" style="display: none;">
        The society&#39;s insatiable appetites for personal data are driving the emergency of data markets, allowing data consumers to launch customized queries over the datasets collected by a data broker from data owners. In this paper, we study how the data broker can maximize her cumulative revenue by posting reasonable prices for sequential queries. We thus propose a contextual dynamic pricing mechanism with the reserve price constraint, which features the properties of ellipsoid for efficient online <span class="search-hit mathjax">optimization</span>, and can support linear and non-linear market value models with uncertainty. In particular, under low uncertainty, our pricing mechanism provides a worst-case regret logarithmic in the number of queries. We further extend to other similar <span class="search-hit mathjax">application</span> scenarios, including hospitality service, online advertising, and loan <span class="search-hit mathjax">application</span>, and extensively evaluate three pricing instances of noisy linear query, accommodation rental, and impression over MovieLens 20M dataset, Airbnb listings in U.S. major cities, and Avazu mobile ad click dataset, respectively. The analysis and evaluation results reveal that our proposed pricing mechanism incurs low practical regret, online latency, and <span class="search-hit mathjax">memory</span> overhead, and also demonstrate that the existence of reserve price can mitigate the cold-start problem in a posted price mechanism, and thus can <span class="search-hit mathjax">reduce</span> the cumulative regret.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.12598v1-abstract-full').style.display = 'none'; document.getElementById('1911.12598v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">A short version is accepted by IEEE International Conference on Data Engineering (ICDE) 2020, and the source <span class="search-hit mathjax">code</span> is available from https://github.com/NiuChaoyue/Personal-Data-Pricing/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.11576">arXiv:1911.11576</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.11576">pdf</a>, <a href="https://arxiv.org/ps/1911.11576">ps</a>, <a href="https://arxiv.org/format/1911.11576">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FusionStitching: Boosting Execution Efficiency of <span class="search-hit mathjax">Memory</span> Intensive Computations for DL Workloads
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+G">Guoping Long</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jun Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+W">Wei Lin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.11576v1-abstract-short" style="display: inline;">
        Performance <span class="search-hit mathjax">optimization</span> is the art of continuous seeking a harmonious mapping between the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.11576v1-abstract-full').style.display = 'inline'; document.getElementById('1911.11576v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.11576v1-abstract-full" style="display: none;">
        Performance <span class="search-hit mathjax">optimization</span> is the art of continuous seeking a harmonious mapping between the <span class="search-hit mathjax">application</span> domain and hardware. Recent years have witnessed a surge of deep learning (DL) <span class="search-hit mathjax">applications</span> in industry. Conventional wisdom for <span class="search-hit mathjax">optimizing</span> such workloads mainly focus on compute intensive ops (GEMM, Convolution, etc). Yet we show in this work, that the performance of <span class="search-hit mathjax">memory</span> intensive computations is vital to E2E performance in practical DL models. We propose \emph{FusionStitching}, a <span class="search-hit mathjax">optimization</span> framework capable of fusing <span class="search-hit mathjax">memory</span> intensive \emph{elementwise}, \emph{reduction} and fine grained \emph{GEMM/Batched-GEMM} ops, with or without data dependences, into large computation units, then mapping and transforming them into efficient GPU kernels. We formulate the fusion plan <span class="search-hit mathjax">optimization</span> as an integer linear <span class="search-hit mathjax">programming</span> (ILP) problem, and propose a set of empirical heuristics to <span class="search-hit mathjax">reduce</span> the combinatorial search space. In order to map <span class="search-hit mathjax">optimized</span> fusion plans to hardware, we propose a technique to effectively compose various groups of computations into a single GPU kernel, by fully leveraging on chip resources like scratchpads or registers. Experimental results on six benchmarks and four industry scale practical models are encouraging. Overall, \emph{FusionStitching} can reach up to 5.7x speedup compared to Tensorflow baseline, and achieves 1.25x to 1.85x performance speedups compared to current state of the art, with 1.4x on average (geometric mean).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.11576v1-abstract-full').style.display = 'none'; document.getElementById('1911.11576v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11+ pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.10558">arXiv:1911.10558</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.10558">pdf</a>, <a href="https://arxiv.org/format/1911.10558">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fast Polynomial Kernel Classification for Massive Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+J">Jinshan Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+M">Minrun Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+S">Shao-Bo Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+D">Ding-Xuan Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.10558v2-abstract-short" style="display: inline;">
        &hellip;tools are a suitable selected feature mapping based on polynomial kernels and an alternating direction method of multipliers (ADMM) algorithm for a related non-smooth convex <span class="search-hit mathjax">optimization</span> problem. Fast learning rates as well as feasibility verifications including the convergence of ADMM and the selection of center points are established to justify theoretical&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.10558v2-abstract-full').style.display = 'inline'; document.getElementById('1911.10558v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.10558v2-abstract-full" style="display: none;">
        In the era of big data, it is highly desired to develop efficient machine learning algorithms to tackle massive data challenges such as storage bottleneck, algorithmic scalability, and interpretability. In this paper, we develop a novel efficient classification algorithm, called fast polynomial kernel classification (FPC), to conquer the scalability and storage challenges. Our main tools are a suitable selected feature mapping based on polynomial kernels and an alternating direction method of multipliers (ADMM) algorithm for a related non-smooth convex <span class="search-hit mathjax">optimization</span> problem. Fast learning rates as well as feasibility verifications including the convergence of ADMM and the selection of center points are established to justify theoretical behaviors of FPC. Our theoretical assertions are verified by a series of simulations and real data <span class="search-hit mathjax">applications</span>. The numerical results demonstrate that FPC significantly <span class="search-hit mathjax">reduces</span> the computational burden and storage <span class="search-hit mathjax">memory</span> of the existing learning schemes such as support vector machines and boosting, without sacrificing their generalization abilities much.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.10558v2-abstract-full').style.display = 'none'; document.getElementById('1911.10558v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 December, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 November, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: text overlap with arXiv:1402.4735 by other authors</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.07187">arXiv:1911.07187</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.07187">pdf</a>, <a href="https://arxiv.org/format/1911.07187">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FPGA Energy Efficiency by Leveraging Thermal Margin
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Khaleghi%2C+B">Behnam Khaleghi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Salamat%2C+S">Sahand Salamat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Imani%2C+M">Mohsen Imani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosing%2C+T">Tajana Rosing</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.07187v1-abstract-short" style="display: inline;">
        &hellip;design is set based on worst-case conditions to ensure reliable operation under all circumstances. This usually leaves a considerable timing margin that can be exploited to <span class="search-hit mathjax">reduce</span> power consumption by scaling voltage without lowering clock frequency. There are hurdles for such opportunistic voltage scaling in FPGAs because (a) critical paths change with desi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.07187v1-abstract-full').style.display = 'inline'; document.getElementById('1911.07187v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.07187v1-abstract-full" style="display: none;">
        Cutting edge FPGAs are not energy efficient as conventionally presumed to be, and therefore, aggressive power-saving techniques have become imperative. The clock rate of an FPGA-mapped design is set based on worst-case conditions to ensure reliable operation under all circumstances. This usually leaves a considerable timing margin that can be exploited to <span class="search-hit mathjax">reduce</span> power consumption by scaling voltage without lowering clock frequency. There are hurdles for such opportunistic voltage scaling in FPGAs because (a) critical paths change with designs, making timing evaluation difficult as voltage changes, (b) each FPGA resource has particular power-delay trade-off with voltage, (c) data corruption of configuration cells and <span class="search-hit mathjax">memory</span> blocks further hampers voltage scaling. In this paper, we propose a systematical approach to leverage the available thermal headroom of FPGA-mapped designs for power and energy <span class="search-hit mathjax">improvement</span>. By comprehensively analyzing the timing and power consumption of FPGA building blocks under varying temperatures and voltages, we propose a thermal-aware voltage scaling flow that effectively utilizes the thermal margin to <span class="search-hit mathjax">reduce</span> power consumption without degrading performance. We show the proposed flow can be employed for energy <span class="search-hit mathjax">optimization</span> as well, whereby power consumption and delay are compromised to accomplish the tasks with minimum energy. Lastly, we propose a simulation framework to be able to examine the efficiency of the proposed method for other <span class="search-hit mathjax">applications</span> that are inherently tolerant to a certain amount of error, granting further power saving opportunity. Experimental results over a set of industrial benchmarks indicate up to 36% power reduction with the same performance, and 66% total energy saving when energy is the <span class="search-hit mathjax">optimization</span> target.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.07187v1-abstract-full').style.display = 'none'; document.getElementById('1911.07187v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in IEEE International Conference on Computer Design (ICCD) 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.06969">arXiv:1911.06969</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.06969">pdf</a>, <a href="https://arxiv.org/format/1911.06969">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Pangolin: An Efficient and Flexible Graph Pattern Mining System on CPU and GPU
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xuhao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dathathri%2C+R">Roshan Dathathri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gill%2C+G">Gurbinder Gill</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pingali%2C+K">Keshav Pingali</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.06969v2-abstract-short" style="display: inline;">
        There is growing interest in graph pattern mining (GPM) problems such as motif counting. GPM systems have been developed to provide unified interfaces for <span class="search-hit mathjax">programming</span> algorithms for these problems and for running them on parallel systems. However, existing systems may take hours to mine even simple patterns in moderate-sized graphs, which significantly limit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.06969v2-abstract-full').style.display = 'inline'; document.getElementById('1911.06969v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.06969v2-abstract-full" style="display: none;">
        There is growing interest in graph pattern mining (GPM) problems such as motif counting. GPM systems have been developed to provide unified interfaces for <span class="search-hit mathjax">programming</span> algorithms for these problems and for running them on parallel systems. However, existing systems may take hours to mine even simple patterns in moderate-sized graphs, which significantly limits their real-world usability.
  We present Pangolin, a high-performance and flexible in-<span class="search-hit mathjax">memory</span> GPM framework targeting shared-<span class="search-hit mathjax">memory</span> CPUs and GPUs. Pangolin is the first GPM system that provides high-level abstractions for GPU processing. It provides a simple <span class="search-hit mathjax">programming</span> interface based on the extend-<span class="search-hit mathjax">reduce</span>-filter model, which enables users to specify <span class="search-hit mathjax">application</span>-specific knowledge for search space pruning and isomorphism test elimination. We describe novel <span class="search-hit mathjax">optimizations</span> that exploit locality, <span class="search-hit mathjax">reduce</span> <span class="search-hit mathjax">memory</span> consumption, and mitigate the overheads of dynamic <span class="search-hit mathjax">memory</span> allocation and synchronization.
  Evaluation on a 28-core CPU demonstrates that Pangolin outperforms existing GPM frameworks Arabesque, RStream, and Fractal by 49x, 88x, and 80x on average, respectively. Acceleration on a V100 GPU further <span class="search-hit mathjax">improves</span> performance of Pangolin by 15x on average. Compared to state-of-the-art hand-<span class="search-hit mathjax">optimized</span> GPM <span class="search-hit mathjax">applications</span>, Pangolin provides competitive performance with less <span class="search-hit mathjax">programming</span> effort.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.06969v2-abstract-full').style.display = 'none'; document.getElementById('1911.06969v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 January, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 November, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.06817">arXiv:1911.06817</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.06817">pdf</a>, <a href="https://arxiv.org/format/1911.06817">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Role-Oriented <span class="search-hit mathjax">Code</span> Generation in an Engine for Solving Hyperbolic PDE Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gallard%2C+J">Jean-Matthieu Gallard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Krenz%2C+L">Lukas Krenz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rannabauer%2C+L">Leonhard Rannabauer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Reinarz%2C+A">Anne Reinarz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bader%2C+M">Michael Bader</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.06817v2-abstract-short" style="display: inline;">
        The development of a high performance PDE solver requires the combined expertise of interdisciplinary teams with respect to <span class="search-hit mathjax">application</span> domain, numerical scheme and low-level&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.06817v2-abstract-full').style.display = 'inline'; document.getElementById('1911.06817v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.06817v2-abstract-full" style="display: none;">
        The development of a high performance PDE solver requires the combined expertise of interdisciplinary teams with respect to <span class="search-hit mathjax">application</span> domain, numerical scheme and low-level <span class="search-hit mathjax">optimization</span>. In this paper, we present how the ExaHyPE engine facilitates the collaboration of such teams by isolating three roles: <span class="search-hit mathjax">application</span>, algorithms, and <span class="search-hit mathjax">optimization</span> expert. We thus support team members in letting them focus on their own area of expertise while integrating their contributions into an HPC production <span class="search-hit mathjax">code</span>. Inspired by web <span class="search-hit mathjax">application</span> development practices, ExaHyPE relies on two custom <span class="search-hit mathjax">code</span> generation modules, the Toolkit and the Kernel Generator, which follow a Model-View-Controller architectural pattern on top of the Jinja2 template engine library. Using Jinja2&#39;s templates to abstract the critical components of the engine and generated glue <span class="search-hit mathjax">code</span>, we isolate the <span class="search-hit mathjax">application</span> development from the engine. The template language also allows us to define and use custom template macros that isolate low-level <span class="search-hit mathjax">optimizations</span> from the numerical scheme described in the templates. We present three use cases, each focusing on one of our user roles, showcasing how the design of the <span class="search-hit mathjax">code</span> generation modules allows to easily expand the solver schemes to support novel demands from <span class="search-hit mathjax">applications</span>, to add <span class="search-hit mathjax">optimized</span> algorithmic schemes (with <span class="search-hit mathjax">reduced</span> <span class="search-hit mathjax">memory</span> footprint, e.g.), or provide <span class="search-hit mathjax">improved</span> low-level SIMD vectorization support.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.06817v2-abstract-full').style.display = 'none'; document.getElementById('1911.06817v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 November, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">SC19 SE-HER</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.06347">arXiv:1911.06347</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.06347">pdf</a>, <a href="https://arxiv.org/format/1911.06347">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        In Search of the Fastest Concurrent Union-Find Algorithm
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Alistarh%2C+D">Dan Alistarh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fedorov%2C+A">Alexander Fedorov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koval%2C+N">Nikita Koval</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.06347v1-abstract-short" style="display: inline;">
        &hellip;the practical performance of concurrent Union-Find.
  This work addresses this gap. We evaluate and analyze the performance of several concurrent Union-Find algorithms and <span class="search-hit mathjax">optimization</span> strategies across a wide range of platforms (Intel, AMD, and ARM) and workloads (social, random, and road networks, as well as integrations into more complex algorithms). We f&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.06347v1-abstract-full').style.display = 'inline'; document.getElementById('1911.06347v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.06347v1-abstract-full" style="display: none;">
        Union-Find (or Disjoint-Set Union) is one of the fundamental problems in computer science; it has been well-studied from both theoretical and practical perspectives in the sequential case. Recently, there has been mounting interest in analyzing this problem in the concurrent scenario, and several asymptotically-efficient algorithms have been proposed. Yet, to date, there is very little known about the practical performance of concurrent Union-Find.
  This work addresses this gap. We evaluate and analyze the performance of several concurrent Union-Find algorithms and <span class="search-hit mathjax">optimization</span> strategies across a wide range of platforms (Intel, AMD, and ARM) and workloads (social, random, and road networks, as well as integrations into more complex algorithms). We first observe that, due to the limited computational cost, the number of induced cache misses is the critical determining factor for the performance of existing algorithms. We introduce new techniques to <span class="search-hit mathjax">reduce</span> this cost by storing node priorities implicitly and by using plain reads and writes in a way that does not affect the correctness of the algorithms. Finally, we show that Union-Find implementations are an interesting <span class="search-hit mathjax">application</span> for Transactional <span class="search-hit mathjax">Memory</span> (TM): one of the fastest algorithm variants we discovered is a sequential one that uses coarse-grained locking with the lock elision <span class="search-hit mathjax">optimization</span> to <span class="search-hit mathjax">reduce</span> synchronization cost and increase scalability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.06347v1-abstract-full').style.display = 'none'; document.getElementById('1911.06347v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.02086">arXiv:1911.02086</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.02086">pdf</a>, <a href="https://arxiv.org/format/1911.02086">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Small-Footprint Keyword Spotting on Raw Audio Data with Sinc-Convolutions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mittermaier%2C+S">Simon Mittermaier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%BCrzinger%2C+L">Ludwig KÃ¼rzinger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Waschneck%2C+B">Bernd Waschneck</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rigoll%2C+G">Gerhard Rigoll</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.02086v2-abstract-short" style="display: inline;">
        Keyword Spotting (KWS) enables speech-based user interaction on smart devices. Always-on and battery-powered <span class="search-hit mathjax">application</span> scenarios for smart devices put constraints on hardware resources and power consumption, while also demanding high accuracy as well as real-time capability. Previous architectures first extracted acoustic features and then applied a neural&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.02086v2-abstract-full').style.display = 'inline'; document.getElementById('1911.02086v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.02086v2-abstract-full" style="display: none;">
        Keyword Spotting (KWS) enables speech-based user interaction on smart devices. Always-on and battery-powered <span class="search-hit mathjax">application</span> scenarios for smart devices put constraints on hardware resources and power consumption, while also demanding high accuracy as well as real-time capability. Previous architectures first extracted acoustic features and then applied a neural network to classify keyword probabilities, <span class="search-hit mathjax">optimizing</span> towards <span class="search-hit mathjax">memory</span> footprint and execution time. Compared to previous publications, we took additional steps to <span class="search-hit mathjax">reduce</span> power and <span class="search-hit mathjax">memory</span> consumption without <span class="search-hit mathjax">reducing</span> classification accuracy. Power-consuming audio preprocessing and data transfer steps are eliminated by directly classifying from raw audio. For this, our end-to-end architecture extracts spectral features using parametrized Sinc-convolutions. Its <span class="search-hit mathjax">memory</span> footprint is further <span class="search-hit mathjax">reduced</span> by grouping depthwise separable convolutions. Our network achieves the competitive accuracy of 96.4% on Google&#39;s Speech Commands test set with only 62k parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.02086v2-abstract-full').style.display = 'none'; document.getElementById('1911.02086v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 May, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 November, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ICASSP 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.00527">arXiv:1911.00527</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.00527">pdf</a>, <a href="https://arxiv.org/format/1911.00527">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Memory</span> Requirement Reduction of Deep Neural Networks Using Low-bit Quantization of Parameters
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nicodemo%2C+N">NiccolÃ³ Nicodemo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Naithani%2C+G">Gaurav Naithani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Drossos%2C+K">Konstantinos Drossos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Virtanen%2C+T">Tuomas Virtanen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saletti%2C+R">Roberto Saletti</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.00527v1-abstract-short" style="display: inline;">
        Effective employment of deep neural networks (DNNs) in mobile devices and embedded systems is hampered by requirements for <span class="search-hit mathjax">memory</span> and computational power. This paper presents a non-uniform quantization approach which allows for dynamic quantization of DNN parameters for different layers and within the same layer. A virtual bit shift (VBS) scheme is also prop&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.00527v1-abstract-full').style.display = 'inline'; document.getElementById('1911.00527v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.00527v1-abstract-full" style="display: none;">
        Effective employment of deep neural networks (DNNs) in mobile devices and embedded systems is hampered by requirements for <span class="search-hit mathjax">memory</span> and computational power. This paper presents a non-uniform quantization approach which allows for dynamic quantization of DNN parameters for different layers and within the same layer. A virtual bit shift (VBS) scheme is also proposed to <span class="search-hit mathjax">improve</span> the accuracy of the proposed scheme. Our method <span class="search-hit mathjax">reduces</span> the <span class="search-hit mathjax">memory</span> requirements, preserving the performance of the network. The performance of our method is validated in a speech enhancement <span class="search-hit mathjax">application</span>, where a fully connected DNN is used to predict the clean speech spectrum from the input noisy speech spectrum. A DNN is <span class="search-hit mathjax">optimized</span> and its <span class="search-hit mathjax">memory</span> footprint and performance are evaluated using the short-time objective intelligibility, STOI, metric. The <span class="search-hit mathjax">application</span> of the low-bit quantization allows a 50% reduction of the DNN <span class="search-hit mathjax">memory</span> footprint while the STOI performance drops only by 2.7%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.00527v1-abstract-full').style.display = 'none'; document.getElementById('1911.00527v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1910.02319">arXiv:1910.02319</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1910.02319">pdf</a>, <a href="https://arxiv.org/ps/1910.02319">ps</a>, <a href="https://arxiv.org/format/1910.02319">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Covariance-free Partial Least Squares: An Incremental Dimensionality Reduction Method
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jordao%2C+A">Artur Jordao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lie%2C+M">Maiko Lie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Melo%2C+V+H+C">Victor Hugo Cunha de Melo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schwartz%2C+W+R">William Robson Schwartz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1910.02319v2-abstract-short" style="display: inline;">
        Dimensionality reduction plays an important role in computer vision problems since it <span class="search-hit mathjax">reduces</span> computational cost and is often capable of yielding more discriminative data representation. In this context, Partial Least Squares (PLS) has presented notable results in tasks such as image classification and neural network&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.02319v2-abstract-full').style.display = 'inline'; document.getElementById('1910.02319v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1910.02319v2-abstract-full" style="display: none;">
        Dimensionality reduction plays an important role in computer vision problems since it <span class="search-hit mathjax">reduces</span> computational cost and is often capable of yielding more discriminative data representation. In this context, Partial Least Squares (PLS) has presented notable results in tasks such as image classification and neural network <span class="search-hit mathjax">optimization</span>. However, PLS is infeasible on large datasets, such as ImageNet, because it requires all the data to be in <span class="search-hit mathjax">memory</span> in advance, which is often impractical due to hardware limitations. Additionally, this requirement prevents us from employing PLS on streaming <span class="search-hit mathjax">applications</span> where the data are being continuously generated. Motivated by this, we propose a novel incremental PLS, named Covariance-free Incremental Partial Least Squares (CIPLS), which learns a low-dimensional representation of the data using a single sample at a time. In contrast to other state-of-the-art approaches, instead of adopting a partially-discriminative or SGD-based model, we extend Nonlinear Iterative Partial Least Squares (NIPALS) -- the standard algorithm used to compute PLS -- for incremental processing. Among the advantages of this approach are the preservation of discriminative information across all components, the possibility of employing its score matrices for feature selection, and its computational efficiency. We validate CIPLS on face verification and image classification tasks, where it outperforms several other incremental dimensionality reduction techniques. In the context of feature selection, CIPLS achieves comparable results when compared to state-of-the-art techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.02319v2-abstract-full').style.display = 'none'; document.getElementById('1910.02319v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 November, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 October, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication at Winter Conference on Applications of Computer Vision (WACV) 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.13282">arXiv:1909.13282</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1909.13282">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Service Embedding in IoT Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Al-Shammari%2C+H+Q">Haider Qays Al-Shammari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lawey%2C+A">Ahmed Lawey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=El-Gorashi%2C+T">Taisir El-Gorashi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elmirghani%2C+J+M+H">Jaafar M. H. Elmirghani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.13282v1-abstract-short" style="display: inline;">
        &hellip;in the execution of a variety of complex tasks in the near future. IoT objects capable of handling multiple sensing and actuating functions are the cornerstone of smart <span class="search-hit mathjax">applications</span> such as smart buildings, smart factories, home&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.13282v1-abstract-full').style.display = 'inline'; document.getElementById('1909.13282v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.13282v1-abstract-full" style="display: none;">
        The Internet of Things is anticipated to participate in the execution of a variety of complex tasks in the near future. IoT objects capable of handling multiple sensing and actuating functions are the cornerstone of smart <span class="search-hit mathjax">applications</span> such as smart buildings, smart factories, home <span class="search-hit mathjax">automation</span>, and healthcare <span class="search-hit mathjax">automation</span>. These smart <span class="search-hit mathjax">applications</span> express their demands in terms of high-level requests. These requests are characterised by the different requirements of sensing actuating functions, processing and <span class="search-hit mathjax">memory</span> needs, activation zones, latency, etc. In service-oriented architecture-based IoT, <span class="search-hit mathjax">application</span> requests are translated into a business process BP workflow. In this study, we model such a BP as a virtual network containing a set of virtual nodes and links connected in a specific topology. These virtual nodes represent the requested processing and location where sensing or actuation are needed. The virtual links capture the requested communication requirements between nodes. In this paper, we introduce a framework, optimised using mixed integer linear <span class="search-hit mathjax">programming</span> MILP, that embeds the BPs from the virtual layer into a lower-level implementation at the IoT physical layer. The proposed framework results in a physical plan that <span class="search-hit mathjax">optimally</span> allocates the processing needed and provisions the sensing and actuation at the required locations requirements to an appropriate set of IoT nodes. The optimisation goal is to minimise the IoT layer total power consumption and optimise the traffic distribution in a manner that minimises the traffic latency of each IoT node. Our results show that the proposed framework enhances the network performance by <span class="search-hit mathjax">reducing</span> the power consumption and latency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.13282v1-abstract-full').style.display = 'none'; document.getElementById('1909.13282v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.11644">arXiv:1909.11644</a>
        <span>&nbsp;&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An <span class="search-hit mathjax">Improvement</span> Over Threads Communications on Multi-Core Processors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fotohi%2C+R">Reza Fotohi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Effatparvar%2C+M">Mehdi Effatparvar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sarkohaki%2C+F">Fateme Sarkohaki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Behzad%2C+S">Shahram Behzad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=balov%2C+J+H">Jaber Hoseini balov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.11644v2-abstract-short" style="display: inline;">
        &hellip;circuit chip that uses two or more computational engines (cores) places in a single processor. This new approach is used to split the computational work of a threaded <span class="search-hit mathjax">application</span> and spread it over multiple execution cores, so that the computer system can benefits from a better performance and better responsiveness of the system. A thread is a unit of execut&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.11644v2-abstract-full').style.display = 'inline'; document.getElementById('1909.11644v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.11644v2-abstract-full" style="display: none;">
        Multicore is an integrated circuit chip that uses two or more computational engines (cores) places in a single processor. This new approach is used to split the computational work of a threaded <span class="search-hit mathjax">application</span> and spread it over multiple execution cores, so that the computer system can benefits from a better performance and better responsiveness of the system. A thread is a unit of execution inside a process that is created and maintained to execute a set of actions/ instructions. Threads can be implemented differently from an operating system to another, but the operating system is in most cases responsible to schedule the execution of different threads. Multi-threading <span class="search-hit mathjax">improving</span> efficiency of processor performance with a cost-effective <span class="search-hit mathjax">memory</span> system. In this paper, we explore one approach to <span class="search-hit mathjax">improve</span> communications for multithreaded. Pre-send is a <span class="search-hit mathjax">software</span> Controlled data forwarding technique that sends data to destination&#39;s cache before it is needed, eliminating cache misses in the destination&#39;s cache as well as <span class="search-hit mathjax">reducing</span> the coherence traffic on the bus. we show how we could <span class="search-hit mathjax">improve</span> the overall system performance by addition of these architecture <span class="search-hit mathjax">optimizations</span> to multi-core processors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.11644v2-abstract-full').style.display = 'none'; document.getElementById('1909.11644v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 September, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This submission has been withdrawn by arXiv administrators due to inappropriate text reuse from external sources</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2012, Volume 6, Issue 12, pp 379-384
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.09765">arXiv:1909.09765</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1909.09765">pdf</a>, <a href="https://arxiv.org/format/1909.09765">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gene-Patterns: Should Architecture be Customized for Each <span class="search-hit mathjax">Application</span>?
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yuhang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Luming Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+M">Mingyu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+Y">Yungang Bao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.09765v2-abstract-short" style="display: inline;">
        Providing architectural support is crucial for newly arising <span class="search-hit mathjax">applications</span> to achieve high performance and high system efficiency. Currently there is a trend in designing accelerators for special&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.09765v2-abstract-full').style.display = 'inline'; document.getElementById('1909.09765v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.09765v2-abstract-full" style="display: none;">
        Providing architectural support is crucial for newly arising <span class="search-hit mathjax">applications</span> to achieve high performance and high system efficiency. Currently there is a trend in designing accelerators for special <span class="search-hit mathjax">applications</span>, while arguably a debate is sparked whether we should customize architecture for each <span class="search-hit mathjax">application</span>. In this study, we introduce what we refer to as Gene-Patterns, which are the base patterns of diverse <span class="search-hit mathjax">applications</span>. We present a Recursive <span class="search-hit mathjax">Reduce</span> methodology to identify the hotspots, and a HOtspot Trace Suite (HOTS) is provided for the research community. We first extract the hotspot patterns, and then, remove the redundancy to obtain the base patterns. We find that although the number of <span class="search-hit mathjax">applications</span> is huge and ever-increasing, the amount of base patterns is relatively small, due to the similarity among the patterns of diverse <span class="search-hit mathjax">applications</span>. The similarity stems not only from the algorithms but also from the data structures. We build the Periodic Table of <span class="search-hit mathjax">Memory</span> Access Patterns (PT-MAP), where the indifference curves are analogous to the energy levels in physics, and <span class="search-hit mathjax">memory</span> performance <span class="search-hit mathjax">optimization</span> is essentially an energy level transition. We find that inefficiency results from the mismatch between some of the base patterns and the micro-architecture of modern processors. We have identified the key micro-architecture demands of the base patterns. The Gene-Pattern concept, methodology, and toolkit will facilitate the design of both hardware and <span class="search-hit mathjax">software</span> for the matching between architectures and <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.09765v2-abstract-full').style.display = 'none'; document.getElementById('1909.09765v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 November, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 September, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.06168">arXiv:1909.06168</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1909.06168">pdf</a>, <a href="https://arxiv.org/format/1909.06168">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Particle Swarm Based Algorithm for Functional Distributed Constraint <span class="search-hit mathjax">Optimization</span> Problems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Choudhury%2C+M">Moumita Choudhury</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahmud%2C+S">Saaduddin Mahmud</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khan%2C+M+M">Md. Mosaddek Khan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.06168v1-abstract-short" style="display: inline;">
        Distributed Constraint <span class="search-hit mathjax">Optimization</span> Problems (DCOPs) are a widely studied constraint handling framework. The objective of a DCOP algorithm is to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.06168v1-abstract-full').style.display = 'inline'; document.getElementById('1909.06168v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.06168v1-abstract-full" style="display: none;">
        Distributed Constraint <span class="search-hit mathjax">Optimization</span> Problems (DCOPs) are a widely studied constraint handling framework. The objective of a DCOP algorithm is to <span class="search-hit mathjax">optimize</span> a global objective function that can be described as the aggregation of a number of distributed constraint cost functions. In a DCOP, each of these functions is defined by a set of discrete variables. However, in many <span class="search-hit mathjax">applications</span>, such as target tracking or sleep scheduling in sensor networks, continuous valued variables are more suited than the discrete ones. Considering this, Functional DCOPs (F-DCOPs) have been proposed that is able to explicitly model a problem containing continuous variables. Nevertheless, the state-of-the-art F-DCOPs approaches experience onerous <span class="search-hit mathjax">memory</span> or computation overhead. To address this issue, we propose a new F-DCOP algorithm, namely Particle Swarm Based F-DCOP (PFD), which is inspired by a meta-heuristic, Particle Swarm <span class="search-hit mathjax">Optimization</span> (PSO). Although it has been successfully applied to many continuous <span class="search-hit mathjax">optimization</span> problems, the potential of PSO has not been utilized in F-DCOPs. To be exact, PFD devises a distributed method of solution construction while significantly <span class="search-hit mathjax">reducing</span> the computation and <span class="search-hit mathjax">memory</span> requirements. Moreover, we theoretically prove that PFD is an anytime algorithm. Finally, our empirical results indicate that PFD outperforms the state-of-the-art approaches in terms of solution quality and computation overhead.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.06168v1-abstract-full').style.display = 'none'; document.getElementById('1909.06168v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.00155">arXiv:1909.00155</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1909.00155">pdf</a>, <a href="https://arxiv.org/format/1909.00155">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EnGN: A High-Throughput and Energy-Efficient Accelerator for Large Graph Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+S">Shengwen Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Ying Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Member"> Member</a>, 
      
      <a href="/search/?searchtype=author&amp;query=IEEE"> IEEE</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Cheng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+L">Lei He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Huawei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Member%2C+S">Senior Member</a>, 
      
      <a href="/search/?searchtype=author&amp;query=IEEE"> IEEE</a>, 
      
      <a href="/search/?searchtype=author&amp;query=and"> and</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiaowei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Member%2C+S">Senior Member</a>, 
      
      <a href="/search/?searchtype=author&amp;query=IEEE"> IEEE</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.00155v3-abstract-short" style="display: inline;">
        Graph neural networks (GNNs) emerge as a powerful approach to process non-euclidean data structures and have been proved powerful in various <span class="search-hit mathjax">application</span> domains such as social networks and e-commerce. While such graph data maintained in real-world systems can be extremely large and sparse, thus employing GNNs to deal with them requires substantial computatio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.00155v3-abstract-full').style.display = 'inline'; document.getElementById('1909.00155v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.00155v3-abstract-full" style="display: none;">
        Graph neural networks (GNNs) emerge as a powerful approach to process non-euclidean data structures and have been proved powerful in various <span class="search-hit mathjax">application</span> domains such as social networks and e-commerce. While such graph data maintained in real-world systems can be extremely large and sparse, thus employing GNNs to deal with them requires substantial computational and <span class="search-hit mathjax">memory</span> overhead, which induces considerable energy and resource cost on CPUs and GPUs. In this work, we present a specialized accelerator architecture, EnGN, to enable high-throughput and energy-efficient processing of large-scale GNNs. The proposed EnGN is designed to accelerate the three key stages of GNN propagation, which is abstracted as common computing patterns shared by typical GNNs. To support the key stages simultaneously, we propose the ring-edge-<span class="search-hit mathjax">reduce</span>(RER) dataflow that tames the poor locality of sparsely-and-randomly connected vertices, and the RER PE-array to practice RER dataflow. In addition, we utilize a graph tiling strategy to fit large graphs into EnGN and make good use of the hierarchical on-chip buffers through adaptive computation reordering and tile scheduling. Overall, EnGN achieves performance speedup by 1802.9X, 19.75X, and 2.97X and energy efficiency by 1326.35X, 304.43X, and 6.2X on average compared to CPU, GPU, and a state-of-the-art GCN accelerator HyGCN, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.00155v3-abstract-full').style.display = 'none'; document.getElementById('1909.00155v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 August, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1908.11660">arXiv:1908.11660</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1908.11660">pdf</a>, <a href="https://arxiv.org/format/1908.11660">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/IACC48062.2019.8971591">10.1109/IACC48062.2019.8971591 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Comparative study of performance of parallel Alpha Beta Pruning for different architectures
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Singhal%2C+S+P">Shubhendra Pal Singhal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sridevi%2C+M">M. Sridevi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1908.11660v2-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Optimization</span> of searching the best possible action depending on various states like state of environment, system goal etc. has been a major area of study in computer systems. In any search algorithm, searching best possible solution from the pool of every possibility known can lead to the construction of the whole state search space popularly called as minim&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.11660v2-abstract-full').style.display = 'inline'; document.getElementById('1908.11660v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1908.11660v2-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Optimization</span> of searching the best possible action depending on various states like state of environment, system goal etc. has been a major area of study in computer systems. In any search algorithm, searching best possible solution from the pool of every possibility known can lead to the construction of the whole state search space popularly called as minimax algorithm. This may lead to a impractical time complexities which may not be suitable for real time searching operations. One of the practical solution for the reduction in computational time is Alpha Beta pruning. Instead of searching for the whole state space, we prune the unnecessary branches, which helps <span class="search-hit mathjax">reduce</span> the time by significant amount. This paper focuses on the various possible implementations of the Alpha Beta pruning algorithms and gives an insight of what algorithm can be used for parallelism. Various studies have been conducted on how to make Alpha Beta pruning faster. Parallelizing Alpha Beta pruning for the GPUs specific architectures like mesh(CUDA) etc. or shared <span class="search-hit mathjax">memory</span> model(OpenMP) helps in the reduction of the computational time. This paper studies the comparison between sequential and different parallel forms of Alpha Beta pruning and their respective efficiency for the chess game as an <span class="search-hit mathjax">application</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.11660v2-abstract-full').style.display = 'none'; document.getElementById('1908.11660v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 October, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 August, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 6 figures, Accepted in 2019 IEEE 9th International Advance Computing Conference(IEEE Xplore)</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2019 IEEE 9th International Conference on Advanced Computing (IACC), Tiruchirappalli, India, 2019, pp. 115-119
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1908.09378">arXiv:1908.09378</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1908.09378">pdf</a>, <a href="https://arxiv.org/format/1908.09378">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A parallel priority queue with fast updates for GPU architectures
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Iacono%2C+J">John Iacono</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Karsin%2C+B">Ben Karsin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sitchinava%2C+N">Nodari Sitchinava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1908.09378v1-abstract-short" style="display: inline;">
        The high computational throughput of modern graphics processing units (GPUs) make them the de-facto architecture for high-performance computing <span class="search-hit mathjax">applications</span>. However, to achieve peak performance, GPUs require highly parallel workloads, as well as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.09378v1-abstract-full').style.display = 'inline'; document.getElementById('1908.09378v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1908.09378v1-abstract-full" style="display: none;">
        The high computational throughput of modern graphics processing units (GPUs) make them the de-facto architecture for high-performance computing <span class="search-hit mathjax">applications</span>. However, to achieve peak performance, GPUs require highly parallel workloads, as well as <span class="search-hit mathjax">memory</span> access patterns that exhibit good locality of reference. As a result, many state-of-the-art algorithms and data structures designed for GPUs sacrifice work-<span class="search-hit mathjax">optimality</span> to achieve the necessary parallelism. Furthermore, some abstract data types are avoided completely due to there being no corresponding data structure that performs well on the GPU. One such abstract data type is the priority queue. Many well-known algorithms rely on priority queue operations as a building block. While various priority queue structures have been developed that are parallel, cache-aware, or cache-oblivious, none has been shown to be efficient on GPUs. In this paper, we present the parBucketHeap, a parallel, cache-efficient data structure designed for modern GPU architectures that supports standard priority queue operations, as well as bulk update. We analyze the structure in several well-known computational models and show that it provides both <span class="search-hit mathjax">optimal</span> parallelism and is cache-efficient. We implement the parBucketHeap and, using it, we solve the single-source shortest path (SSSP) problem. Experimental results indicate that, for sufficiently large, dense graphs with high diameter, we out-perform current state-of-the-art SSSP algorithms on the GPU by up to a factor of 5. Unlike existing GPU SSSP algorithms, our approach is work-<span class="search-hit mathjax">optimal</span> and places significantly less load on the GPU, <span class="search-hit mathjax">reducing</span> power consumption.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.09378v1-abstract-full').style.display = 'none'; document.getElementById('1908.09378v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1908.08553">arXiv:1908.08553</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1908.08553">pdf</a>, <a href="https://arxiv.org/format/1908.08553">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Strongly Correlated Electrons">cond-mat.str-el</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.cpc.2020.107750">10.1016/j.cpc.2020.107750 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Simulation of Quantum Many-Body Systems on Amazon Cloud
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Reyes%2C+J+A">Justin A. Reyes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mucciolo%2C+E+R">Eduardo R. Mucciolo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marinescu%2C+D">Dan Marinescu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1908.08553v2-abstract-short" style="display: inline;">
        &hellip;when considering systems in more than one dimension. In this paper, we explore the exact computation of TN contractions on two-dimensional geometries and present a heuristic <span class="search-hit mathjax">improvement</span> of TN contraction that&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.08553v2-abstract-full').style.display = 'inline'; document.getElementById('1908.08553v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1908.08553v2-abstract-full" style="display: none;">
        Quantum many-body systems (QMBs) are some of the most challenging physical systems to simulate numerically. Methods involving approximations for tensor network (TN) contractions have proven to be viable alternatives to algorithms such as quantum Monte Carlo or simulated annealing. However, these methods are cumbersome, difficult to implement, and often have significant limitations in their accuracy and efficiency when considering systems in more than one dimension. In this paper, we explore the exact computation of TN contractions on two-dimensional geometries and present a heuristic <span class="search-hit mathjax">improvement</span> of TN contraction that <span class="search-hit mathjax">reduces</span> the computing time, the amount of <span class="search-hit mathjax">memory</span>, and the communication time. We run our algorithm for the Ising model using <span class="search-hit mathjax">memory</span> <span class="search-hit mathjax">optimized</span> x1.32x large instances on Amazon Web Services (AWS) Elastic Compute Cloud (EC2). Our results show that cloud computing is a viable alternative to supercomputers for this class of scientific <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.08553v2-abstract-full').style.display = 'none'; document.getElementById('1908.08553v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 August, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">25 pages, 11 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Computer Physics Communications 261 (2021) 107750
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1908.07985">arXiv:1908.07985</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1908.07985">pdf</a>, <a href="https://arxiv.org/format/1908.07985">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3300061.3345455">10.1145/3300061.3345455 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MobiSR: Efficient On-Device Super-Resolution through Heterogeneous Mobile Processors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+R">Royson Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Venieris%2C+S+I">Stylianos I. Venieris</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dudziak%2C+%C5%81">Åukasz Dudziak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhattacharya%2C+S">Sourav Bhattacharya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lane%2C+N+D">Nicholas D. Lane</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1908.07985v1-abstract-short" style="display: inline;">
        &hellip;demonstrated unprecedented performance in the image restoration task of super-resolution (SR). SR entails the upscaling of a single low-resolution image in order to meet <span class="search-hit mathjax">application</span>-specific image quality demands and plays a key role in mobile devices. To comply with privacy regulations and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.07985v1-abstract-full').style.display = 'inline'; document.getElementById('1908.07985v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1908.07985v1-abstract-full" style="display: none;">
        In recent years, convolutional networks have demonstrated unprecedented performance in the image restoration task of super-resolution (SR). SR entails the upscaling of a single low-resolution image in order to meet <span class="search-hit mathjax">application</span>-specific image quality demands and plays a key role in mobile devices. To comply with privacy regulations and <span class="search-hit mathjax">reduce</span> the overhead of cloud computing, executing SR models locally on-device constitutes a key alternative approach. Nevertheless, the excessive compute and <span class="search-hit mathjax">memory</span> requirements of SR workloads pose a challenge in mapping SR networks on resource-constrained mobile platforms. This work presents MobiSR, a novel framework for performing efficient super-resolution on-device. Given a target mobile platform, the proposed framework considers popular model compression techniques and traverses the design space to reach the highest performing trade-off between image quality and processing speed. At run time, a novel scheduler dispatches incoming image patches to the appropriate model-engine pair based on the patch&#39;s estimated upscaling difficulty in order to meet the required image quality with minimum processing latency. Quantitative evaluation shows that the proposed framework yields on-device SR designs that achieve an average speedup of 2.13x over highly-<span class="search-hit mathjax">optimized</span> parallel difficulty-unaware mappings and 4.79x over highly-<span class="search-hit mathjax">optimized</span> single compute engine implementations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.07985v1-abstract-full').style.display = 'none'; document.getElementById('1908.07985v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at the 25th Annual International Conference on Mobile Computing and Networking (MobiCom), 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1908.04249">arXiv:1908.04249</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1908.04249">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cache <span class="search-hit mathjax">Optimization</span> for <span class="search-hit mathjax">Memory</span> Intensive Workloads on Multi-socket Multi-core servers
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Durbhakula%2C+M">Murthy Durbhakula</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1908.04249v1-abstract-short" style="display: inline;">
        Major chip manufacturers have all introduced multicore microprocessors. Multi-socket systems built from these processors are used for running various server <span class="search-hit mathjax">applications</span>. Depending on the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.04249v1-abstract-full').style.display = 'inline'; document.getElementById('1908.04249v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1908.04249v1-abstract-full" style="display: none;">
        Major chip manufacturers have all introduced multicore microprocessors. Multi-socket systems built from these processors are used for running various server <span class="search-hit mathjax">applications</span>. Depending on the <span class="search-hit mathjax">application</span> that is run on the system, remote <span class="search-hit mathjax">memory</span> accesses can impact overall performance. This paper presents a cache <span class="search-hit mathjax">optimization</span> that can cut down remote DRAM accesses. By keeping track of remote cache lines loaded from remote DRAM and by biasing the cache replacement policy towards such remote DRAM cache lines the number of cache misses are <span class="search-hit mathjax">reduced</span>. This in turn results in <span class="search-hit mathjax">improvement</span> of overall performance. I present the design details in this paper. I do a qualitative comparison of various solutions to the problem of performance impact of remote DRAM accesses. This work can be extended by doing a quantitative evaluation and by further refining cache <span class="search-hit mathjax">optimization</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.04249v1-abstract-full').style.display = 'none'; document.getElementById('1908.04249v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 August, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1908.00936">arXiv:1908.00936</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1908.00936">pdf</a>, <a href="https://arxiv.org/format/1908.00936">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Scale Learned Iterative Reconstruction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hauptmann%2C+A">Andreas Hauptmann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adler%2C+J">Jonas Adler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arridge%2C+S">Simon Arridge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%C3%96ktem%2C+O">Ozan Ãktem</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1908.00936v3-abstract-short" style="display: inline;">
        Model-based learned iterative reconstruction methods have recently been shown to outperform classical reconstruction algorithms. <span class="search-hit mathjax">Applicability</span> of these methods to large scale inverse problems is however limited by the available&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.00936v3-abstract-full').style.display = 'inline'; document.getElementById('1908.00936v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1908.00936v3-abstract-full" style="display: none;">
        Model-based learned iterative reconstruction methods have recently been shown to outperform classical reconstruction algorithms. <span class="search-hit mathjax">Applicability</span> of these methods to large scale inverse problems is however limited by the available <span class="search-hit mathjax">memory</span> for training and extensive training times, the latter due to computationally expensive forward models. As a possible solution to these restrictions we propose a multi-scale learned iterative reconstruction scheme that computes iterates on discretisations of increasing resolution. This procedure does not only <span class="search-hit mathjax">reduce</span> <span class="search-hit mathjax">memory</span> requirements, it also considerably speeds up reconstruction and training times, but most importantly is scalable to large scale inverse problems with non-trivial forward operators, such as those that arise in many 3D tomographic <span class="search-hit mathjax">applications</span>. In particular, we propose a hybrid network that combines the multi-scale iterative approach with a particularly expressive network architecture which in combination exhibits excellent scalability in 3D.
  <span class="search-hit mathjax">Applicability</span> of the algorithm is demonstrated for 3D cone beam computed tomography from real measurement data of an organic phantom. Additionally, we examine scalability and reconstruction quality in comparison to established learned reconstruction methods in two dimensions for low dose computed tomography on human phantoms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.00936v3-abstract-full').style.display = 'none'; document.getElementById('1908.00936v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 August, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.12349">arXiv:1907.12349</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.12349">pdf</a>, <a href="https://arxiv.org/ps/1907.12349">ps</a>, <a href="https://arxiv.org/format/1907.12349">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PyLops -- A Linear-Operator Python Library for large scale <span class="search-hit mathjax">optimization</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ravasi%2C+M">Matteo Ravasi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vasconcelos%2C+I">Ivan Vasconcelos</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.12349v1-abstract-short" style="display: inline;">
        &hellip;and optimisation are at the core of many algorithms used in signal and image processing, remote sensing, and inverse problems. For small to medium-scale problems, existing <span class="search-hit mathjax">software</span> packages (e.g., MATLAB, Python numpy and scipy) allow for explicitly building dense (or sparse) matrices and performing algebraic operations (e.g., computation of matrix-vector pr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.12349v1-abstract-full').style.display = 'inline'; document.getElementById('1907.12349v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.12349v1-abstract-full" style="display: none;">
        Linear operators and optimisation are at the core of many algorithms used in signal and image processing, remote sensing, and inverse problems. For small to medium-scale problems, existing <span class="search-hit mathjax">software</span> packages (e.g., MATLAB, Python numpy and scipy) allow for explicitly building dense (or sparse) matrices and performing algebraic operations (e.g., computation of matrix-vector products and manipulation of matrices) with syntax that closely represents their corresponding analytical forms. However, many real <span class="search-hit mathjax">application</span>, large-scale operators do not lend themselves to explicit matrix representations, usually forcing practitioners to forego of the convenient linear-algebra syntax available for their explicit-matrix counterparts. PyLops is an open-source Python library providing a flexible and scalable framework for the creation and combination of so-called linear operators, class-based entities that represent matrices and inherit their associated syntax convenience, but do not rely on the creation of explicit matrices. We show that PyLops operators can dramatically <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">memory</span> load and CPU computations compared to explicit-matrix calculations, while still allowing users to seamlessly use their existing knowledge of compact matrix-based syntax that scales to any problem size because no explicit matrices are required.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.12349v1-abstract-full').style.display = 'none'; document.getElementById('1907.12349v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 July, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.10554">arXiv:1907.10554</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.10554">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1002/mp.14198">10.1002/mp.14198 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Development of a Real-time Indoor Location System using Bluetooth Low Energy Technology and Deep Learning to Facilitate Clinical <span class="search-hit mathjax">Applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+G">Guanglin Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+Y">Yulong Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+C">Chenyang Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+X">Xun Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zinn%2C+M">Meyer Zinn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Trivedi%2C+Z">Zipalkumar Trivedi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yingling%2C+A">Alicia Yingling</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Westover%2C+K">Kenneth Westover</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+S">Steve Jiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.10554v2-abstract-short" style="display: inline;">
        An indoor, real-time location system (RTLS) can benefit both hospitals and patients by <span class="search-hit mathjax">improving</span> clinical efficiency through data-driven&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.10554v2-abstract-full').style.display = 'inline'; document.getElementById('1907.10554v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.10554v2-abstract-full" style="display: none;">
        An indoor, real-time location system (RTLS) can benefit both hospitals and patients by <span class="search-hit mathjax">improving</span> clinical efficiency through data-driven <span class="search-hit mathjax">optimization</span> of procedures. Bluetooth-based RTLS systems are cost-effective but lack accuracy and robustness because Bluetooth signal strength is subject to fluctuation. We developed a machine learning-based solution using a Long Short-Term <span class="search-hit mathjax">Memory</span> (LSTM) network followed by a Multilayer Perceptron classifier and a posterior constraint algorithm to <span class="search-hit mathjax">improve</span> RTLS performance. Training and validation datasets showed that most machine learning models perform well in classifying individual location zones, although LSTM was most reliable. However, when faced with data indicating cross-zone trajectories, all models showed erratic zone switching. Thus, we implemented a history-based posterior constraint algorithm to <span class="search-hit mathjax">reduce</span> the variability in exchange for a slight decrease in responsiveness. This network increases robustness at the expense of latency. When latency is less of a concern, we computed the latency-corrected accuracy which is 100% for our testing data, significantly <span class="search-hit mathjax">improved</span> from LSTM without constraint which is 96.2%. The balance between robustness and responsiveness can be considered and adjusted on a case-by-case basis, according to the specific needs of downstream clinical <span class="search-hit mathjax">applications</span>. This system was deployed and validated in an academic medical center. Industry best practices enabled system scaling without substantial compromises to performance or cost.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.10554v2-abstract-full').style.display = 'none'; document.getElementById('1907.10554v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 March, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 July, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 6 figures, submitted to Physics in Medicine &amp; Biology</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.09150">arXiv:1907.09150</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.09150">pdf</a>, <a href="https://arxiv.org/format/1907.09150">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stochastic Variance <span class="search-hit mathjax">Reduced</span> Primal Dual Algorithms for Empirical Composition <span class="search-hit mathjax">Optimization</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Devraj%2C+A+M">Adithya M. Devraj</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jianshu Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.09150v2-abstract-short" style="display: inline;">
        We consider a generic empirical composition <span class="search-hit mathjax">optimization</span> problem, where there are empirical averages present both outside and inside nonlinear loss functions. Such a problem is of interest in various machine learning&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.09150v2-abstract-full').style.display = 'inline'; document.getElementById('1907.09150v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.09150v2-abstract-full" style="display: none;">
        We consider a generic empirical composition <span class="search-hit mathjax">optimization</span> problem, where there are empirical averages present both outside and inside nonlinear loss functions. Such a problem is of interest in various machine learning <span class="search-hit mathjax">applications</span>, and cannot be directly solved by standard methods such as stochastic gradient descent. We take a novel approach to solving this problem by reformulating the original minimization objective into an equivalent min-max objective, which brings out all the empirical averages that are originally inside the nonlinear loss functions. We exploit the rich structures of the reformulated problem and develop a stochastic primal-dual algorithm, SVRPDA-I, to solve the problem efficiently. We carry out extensive theoretical analysis of the proposed algorithm, obtaining the convergence rate, the computation complexity and the storage complexity. In particular, the algorithm is shown to converge at a linear rate when the problem is strongly convex. Moreover, we also develop an approximate version of the algorithm, named SVRPDA-II, which further <span class="search-hit mathjax">reduces</span> the <span class="search-hit mathjax">memory</span> requirement. Finally, we evaluate our proposed algorithms on several real-world benchmarks, and experimental results show that the proposed algorithms significantly outperform existing techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.09150v2-abstract-full').style.display = 'none'; document.getElementById('1907.09150v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 October, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 July, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">42 pages; Published at Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.04217">arXiv:1907.04217</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.04217">pdf</a>, <a href="https://arxiv.org/format/1907.04217">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/HPEC.2019.8916508">10.1109/HPEC.2019.8916508 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kepner%2C+J">Jeremy Kepner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gadepally%2C+V">Vijay Gadepally</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Milechin%2C+L">Lauren Milechin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Samsi%2C+S">Siddharth Samsi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arcand%2C+W">William Arcand</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bestor%2C+D">David Bestor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bergeron%2C+W">William Bergeron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Byun%2C+C">Chansup Byun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hubbell%2C+M">Matthew Hubbell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Houle%2C+M">Michael Houle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jones%2C+M">Michael Jones</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Klein%2C+A">Anne Klein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Michaleas%2C+P">Peter Michaleas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mullen%2C+J">Julie Mullen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prout%2C+A">Andrew Prout</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosa%2C+A">Antonio Rosa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yee%2C+C">Charles Yee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Reuther%2C+A">Albert Reuther</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.04217v1-abstract-short" style="display: inline;">
        &hellip;Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-<span class="search-hit mathjax">memory</span> database implementation of hypersparse arrays that are ideal for analyzing many types of network data. D4M relies on associative arrays which combine properties of spreadsheets, datab&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.04217v1-abstract-full').style.display = 'inline'; document.getElementById('1907.04217v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.04217v1-abstract-full" style="display: none;">
        The Dynamic Distributed Dimensional Data Model (D4M) library implements associative arrays in a variety of languages (Python, Julia, and Matlab/Octave) and provides a lightweight in-<span class="search-hit mathjax">memory</span> database implementation of hypersparse arrays that are ideal for analyzing many types of network data. D4M relies on associative arrays which combine properties of spreadsheets, databases, matrices, graphs, and networks, while providing rigorous mathematical guarantees, such as linearity. Streaming updates of D4M associative arrays put enormous pressure on the <span class="search-hit mathjax">memory</span> hierarchy. This work describes the design and performance <span class="search-hit mathjax">optimization</span> of an implementation of hierarchical associative arrays that <span class="search-hit mathjax">reduces</span> <span class="search-hit mathjax">memory</span> pressure and dramatically increases the update rate into an associative array. The parameters of hierarchical associative arrays rely on controlling the number of entries in each level in the hierarchy before an update is cascaded. The parameters are easily tunable to achieve <span class="search-hit mathjax">optimal</span> performance for a variety of <span class="search-hit mathjax">applications</span>. Hierarchical arrays achieve over 40,000 updates per second in a single instance. Scaling to 34,000 instances of hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of 1,900,000,000 updates per second. This capability allows the MIT SuperCloud to analyze extremely large streaming network data sets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.04217v1-abstract-full').style.display = 'none'; document.getElementById('1907.04217v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 July, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages; 6 figures; accepted to IEEE High Performance Extreme Computing (HPEC) Conference 2019. arXiv admin note: text overlap with arXiv:1807.05308, arXiv:1902.00846</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.03247">arXiv:1907.03247</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.03247">pdf</a>, <a href="https://arxiv.org/format/1907.03247">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Resource-Efficient Computing in Wearable Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pedram%2C+M">Mahdi Pedram</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rofouei%2C+M">Mahsan Rofouei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fraternali%2C+F">Francesco Fraternali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ashari%2C+Z+E">Zhila Esna Ashari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghasemzadeh%2C+H">Hassan Ghasemzadeh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.03247v1-abstract-short" style="display: inline;">
        We propose two <span class="search-hit mathjax">optimization</span> techniques to minimize&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.03247v1-abstract-full').style.display = 'inline'; document.getElementById('1907.03247v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.03247v1-abstract-full" style="display: none;">
        We propose two <span class="search-hit mathjax">optimization</span> techniques to minimize <span class="search-hit mathjax">memory</span> usage and computation while meeting system timing constraints for real-time classification in wearable systems. Our method derives a hierarchical classifier structure for Support Vector Machine (SVM) in order to <span class="search-hit mathjax">reduce</span> the amount of computations, based on the probability distribution of output classes occurrences. Also, we propose a <span class="search-hit mathjax">memory</span> <span class="search-hit mathjax">optimization</span> technique based on SVM parameters, which results in storing fewer support vectors and as a result requiring less <span class="search-hit mathjax">memory</span>. To demonstrate the efficiency of our proposed techniques, we performed an activity recognition experiment and were able to save up to 35% and 56% in <span class="search-hit mathjax">memory</span> storage when classifying 14 and 6 different activities, respectively. In addition, we demonstrated that there is a trade-off between accuracy of classification and <span class="search-hit mathjax">memory</span> savings, which can be controlled based on <span class="search-hit mathjax">application</span> requirements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.03247v1-abstract-full').style.display = 'none'; document.getElementById('1907.03247v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Fourth IEEE Workshop on Smart Service Systems (SmartSys 2019), 12 June 2019, Washington D.C., USA
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.02129">arXiv:1907.02129</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.02129">pdf</a>, <a href="https://arxiv.org/format/1907.02129">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Indirect Convolution Algorithm
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dukhan%2C+M">Marat Dukhan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.02129v1-abstract-short" style="display: inline;">
        &hellip;convolution operators with GEMM-based algorithms. In these algorithms, convolution is implemented on top of matrix-matrix multiplication (GEMM) functions, provided by highly <span class="search-hit mathjax">optimized</span> BLAS libraries. Convolutions with 1x1 kernels can be directly represented as a GEMM call, but convolutions with larger kernels require a special&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.02129v1-abstract-full').style.display = 'inline'; document.getElementById('1907.02129v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.02129v1-abstract-full" style="display: none;">
        Deep learning frameworks commonly implement convolution operators with GEMM-based algorithms. In these algorithms, convolution is implemented on top of matrix-matrix multiplication (GEMM) functions, provided by highly <span class="search-hit mathjax">optimized</span> BLAS libraries. Convolutions with 1x1 kernels can be directly represented as a GEMM call, but convolutions with larger kernels require a special <span class="search-hit mathjax">memory</span> layout transformation - im2col or im2row - to fit into GEMM interface.
  The Indirect Convolution algorithm provides the efficiency of the GEMM primitive without the overhead of im2col transformation. In contrast to GEMM-based algorithms, the Indirect Convolution does not reshuffle the data to fit into the GEMM primitive but introduces an indirection buffer - a buffer of pointers to the start of each row of image pixels. This broadens the <span class="search-hit mathjax">application</span> of our modified GEMM function to convolutions with arbitrary kernel size, padding, stride, and dilation.
  The Indirect Convolution algorithm <span class="search-hit mathjax">reduces</span> <span class="search-hit mathjax">memory</span> overhead proportionally to the number of input channels and outperforms the GEMM-based algorithm by up to 62% on convolution parameters which involve im2col transformations in GEMM-based algorithms. This, however, comes at cost of minor performance reduction on 1x1 stride-1 convolutions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.02129v1-abstract-full').style.display = 'none'; document.getElementById('1907.02129v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented on Efficient Deep Learning for Computer Vision workshop at CVPR 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.01112">arXiv:1907.01112</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.01112">pdf</a>, <a href="https://arxiv.org/format/1907.01112">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/GLOBECOM38437.2019.9013465">10.1109/GLOBECOM38437.2019.9013465 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the <span class="search-hit mathjax">Optimal</span> Refresh Power Allocation for Energy-Efficient <span class="search-hit mathjax">Memories</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+Y">Yongjune Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Choi%2C+W+H">Won Ho Choi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guyot%2C+C">Cyril Guyot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cassuto%2C+Y">Yuval Cassuto</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.01112v2-abstract-short" style="display: inline;">
        Refresh is an important operation to prevent loss of data in dynamic random-access <span class="search-hit mathjax">memory</span> (DRAM). However, frequent refresh operations incur considerable power consumption and degrade system performance. Refresh power cost is especially significant in high-capacity&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.01112v2-abstract-full').style.display = 'inline'; document.getElementById('1907.01112v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.01112v2-abstract-full" style="display: none;">
        Refresh is an important operation to prevent loss of data in dynamic random-access <span class="search-hit mathjax">memory</span> (DRAM). However, frequent refresh operations incur considerable power consumption and degrade system performance. Refresh power cost is especially significant in high-capacity <span class="search-hit mathjax">memory</span> devices and battery-powered edge/mobile <span class="search-hit mathjax">applications</span>. In this paper, we propose a principled approach to <span class="search-hit mathjax">optimizing</span> the refresh power allocation. Given a model for the bit error rate dependence on power, we formulate a convex <span class="search-hit mathjax">optimization</span> problem to minimize the word mean squared error for a refresh power constraint; hence we can guarantee the <span class="search-hit mathjax">optimality</span> of the obtained refresh power allocations. In addition, we provide an integer <span class="search-hit mathjax">programming</span> problem to <span class="search-hit mathjax">optimize</span> the discrete refresh interval assignments. For an 8-bit accessed word, numerical results show that the <span class="search-hit mathjax">optimized</span> nonuniform refresh intervals <span class="search-hit mathjax">reduce</span> the refresh power by 29% at a peak signal-to-noise ratio of 50dB compared to the uniform assignment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.01112v2-abstract-full').style.display = 'none'; document.getElementById('1907.01112v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 July, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1906.05922">arXiv:1906.05922</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1906.05922">pdf</a>, <a href="https://arxiv.org/format/1906.05922">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Thread Batching for High-performance Energy-efficient GPU <span class="search-hit mathjax">Memory</span> Design
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mao%2C+M">Mengjie Mao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xiaoxiao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zihao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+W">Wujie Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yiran Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hai"> Hai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li"> Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1906.05922v1-abstract-short" style="display: inline;">
        Massive multi-threading in GPU imposes tremendous pressure on <span class="search-hit mathjax">memory</span> subsystems. Due to rapid growth in thread-level parallelism of GPU and slowly&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.05922v1-abstract-full').style.display = 'inline'; document.getElementById('1906.05922v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1906.05922v1-abstract-full" style="display: none;">
        Massive multi-threading in GPU imposes tremendous pressure on <span class="search-hit mathjax">memory</span> subsystems. Due to rapid growth in thread-level parallelism of GPU and slowly <span class="search-hit mathjax">improved</span> peak <span class="search-hit mathjax">memory</span> bandwidth, the <span class="search-hit mathjax">memory</span> becomes a bottleneck of GPU&#39;s performance and energy efficiency. In this work, we propose an integrated architectural scheme to <span class="search-hit mathjax">optimize</span> the <span class="search-hit mathjax">memory</span> accesses and therefore boost the performance and energy efficiency of GPU. Firstly, we propose a thread batch enabled <span class="search-hit mathjax">memory</span> partitioning (TEMP) to <span class="search-hit mathjax">improve</span> GPU <span class="search-hit mathjax">memory</span> access parallelism. In particular, TEMP groups multiple thread blocks that share the same set of pages into a thread batch and applies a page coloring mechanism to bound each stream multiprocessor (SM) to the dedicated <span class="search-hit mathjax">memory</span> banks. After that, TEMP dispatches the thread batch to an SM to ensure high-parallel <span class="search-hit mathjax">memory</span>-access streaming from the different thread blocks. Secondly, a thread batch-aware scheduling (TBAS) scheme is introduced to <span class="search-hit mathjax">improve</span> the GPU <span class="search-hit mathjax">memory</span> access locality and to <span class="search-hit mathjax">reduce</span> the contention on <span class="search-hit mathjax">memory</span> controllers and interconnection networks. Experimental results show that the integration of TEMP and TBAS can achieve up to 10.3% performance <span class="search-hit mathjax">improvement</span> and 11.3% DRAM energy reduction across diverse GPU <span class="search-hit mathjax">applications</span>. We also evaluate the performance interference of the mixed CPU+GPU workloads when they are run on a heterogeneous system that employs our proposed schemes. Our results show that a simple solution can effectively ensure the efficient execution of both GPU and CPU <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.05922v1-abstract-full').style.display = 'none'; document.getElementById('1906.05922v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1906.03304">arXiv:1906.03304</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1906.03304">pdf</a>, <a href="https://arxiv.org/format/1906.03304">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MoMIT: Porting a JavaScript Interpreter on a Quarter Coin
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Morales%2C+R">Rodrigo Morales</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saborido%2C+R">Ruben Saborido</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%C3%A9h%C3%A9neuc%2C+Y">Yann-GaÃ«l GuÃ©hÃ©neuc</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1906.03304v1-abstract-short" style="display: inline;">
        &hellip;for communicating Web pages and Web apps is JavaScript (JS) and extending the same standard platform to connect IoT devices seems more than appropriate. However, porting JS <span class="search-hit mathjax">applications</span> to the large variety of IoT devices, specifically on System-on-a-Chip (SoCs) devices (\eg~Arduino Uno, Particle \photon), is challenging because these devices are constrained&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.03304v1-abstract-full').style.display = 'inline'; document.getElementById('1906.03304v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1906.03304v1-abstract-full" style="display: none;">
        The Internet of Things (IoT) is a network of physical, heterogeneous, connected devices providing services through private networks and the Internet. It connects a range of new devices to the Internet so they can communicate with Web servers and other devices around the world. Today&#39;s standard platform for communicating Web pages and Web apps is JavaScript (JS) and extending the same standard platform to connect IoT devices seems more than appropriate. However, porting JS <span class="search-hit mathjax">applications</span> to the large variety of IoT devices, specifically on System-on-a-Chip (SoCs) devices (\eg~Arduino Uno, Particle \photon), is challenging because these devices are constrained in terms of <span class="search-hit mathjax">memory</span> and storage capacity. Running JS <span class="search-hit mathjax">applications</span> adds an overhead of resources to deploy a <span class="search-hit mathjax">code</span> interpreter on the devices. Also, running JS <span class="search-hit mathjax">applications</span> may not be possible ``as is&#39;&#39; on some device missing some hardware/<span class="search-hit mathjax">software</span> capabilities. To address this problem, we propose \momit~a multiobjective <span class="search-hit mathjax">optimization</span> approach to miniaturize JS <span class="search-hit mathjax">applications</span> to run on IoT constrained devices. To validate \momit, we miniaturize a JS interpreter to execute a testbed comprised of 23 <span class="search-hit mathjax">applications</span> and measure their performances before and after applying the miniaturization process. We implement \momit~using three different search algorithms and found that it can <span class="search-hit mathjax">reduce</span> <span class="search-hit mathjax">code</span> size, <span class="search-hit mathjax">memory</span> usage, and CPU time by median values of 31\%, 56\%, and 36\% respectively. Finally, MoMIT ported the miniaturized JS interpreters up to to 2 SoCs additional devices, in comparison of using default JS interpreter features.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.03304v1-abstract-full').style.display = 'none'; document.getElementById('1906.03304v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 May, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1906.02362">arXiv:1906.02362</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1906.02362">pdf</a>, <a href="https://arxiv.org/format/1906.02362">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Lookout for Zombies: Mitigating Flush+Reload Attack on Shared Caches by Monitoring Invalidated Lines
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Saileshwar%2C+G">Gururaj Saileshwar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qureshi%2C+M+K">Moinuddin K. Qureshi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1906.02362v1-abstract-short" style="display: inline;">
        OS-based page sharing is a commonly used <span class="search-hit mathjax">optimization</span> in modern systems to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.02362v1-abstract-full').style.display = 'inline'; document.getElementById('1906.02362v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1906.02362v1-abstract-full" style="display: none;">
        OS-based page sharing is a commonly used <span class="search-hit mathjax">optimization</span> in modern systems to <span class="search-hit mathjax">reduce</span> <span class="search-hit mathjax">memory</span> footprint. Unfortunately, such sharing can cause Flush+Reload cache attacks, whereby a spy periodically flushes a cache line of shared data (using the clflush instruction) and reloads it to infer the access patterns of a victim <span class="search-hit mathjax">application</span>. Current proposals to mitigate Flush+Reload attacks are impractical as they either disable page sharing, or require <span class="search-hit mathjax">application</span> rewrite, or require OS support, or incur ISA changes. Ideally, we want to tolerate attacks without requiring any OS or ISA support and while incurring negligible performance and storage overheads.
  This paper makes the key observation that when a cache line is invalidated due to a Flush-Caused Invalidation (FCI), the tag and data of the invalidated line are still resident in the cache and can be used for detecting Flush-based attacks. We call lines invalidated due to FCI as Zombie lines. Our design explicitly marks such lines as Zombies, preserves the Zombie lines in the cache, and uses the hits and misses to Zombie lines to tolerate the attacks. We propose Zombie-Based Mitigation (ZBM), a simple hardware-based design that successfully guards against attacks by simply treating hits on Zombie-lines as misses to avoid any timing leaks to the attacker. We analyze the robustness of ZBM using three spy <span class="search-hit mathjax">programs</span>: attacking AES T-Tables, attacking RSA Square-and-Multiply, and Function Watcher (FW), and show that ZBM successfully defends against these attacks. Our solution requires negligible storage (4-bits per cache line), retains OS-based page sharing, requires no OS/ISA changes, and does not incur slowdown for benign <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.02362v1-abstract-full').style.display = 'none'; document.getElementById('1906.02362v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 June, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1905.10587">arXiv:1905.10587</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1905.10587">pdf</a>, <a href="https://arxiv.org/ps/1905.10587">ps</a>, <a href="https://arxiv.org/format/1905.10587">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fast and Accurate Gaussian Kernel Ridge Regression Using Matrix Decompositions for Preconditioning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shabat%2C+G">Gil Shabat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Choshen%2C+E">Era Choshen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Or%2C+D+B">Dvir Ben Or</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carmel%2C+N">Nadav Carmel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1905.10587v3-abstract-short" style="display: inline;">
        This paper presents a method for building a preconditioner for a kernel ridge regression problem, where the preconditioner is not only effective in its ability to <span class="search-hit mathjax">reduce</span> the condition number substantially, but also efficient in its&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1905.10587v3-abstract-full').style.display = 'inline'; document.getElementById('1905.10587v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1905.10587v3-abstract-full" style="display: none;">
        This paper presents a method for building a preconditioner for a kernel ridge regression problem, where the preconditioner is not only effective in its ability to <span class="search-hit mathjax">reduce</span> the condition number substantially, but also efficient in its <span class="search-hit mathjax">application</span> in terms of computational cost and <span class="search-hit mathjax">memory</span> consumption. The suggested approach is based on randomized matrix decomposition methods, combined with the fast multipole method to achieve an algorithm that can process large datasets in complexity linear to the number of data points. In addition, a detailed theoretical analysis is provided, including an upper bound to the condition number. Finally, for Gaussian kernels, the analysis shows that the required rank for a desired condition number can be determined directly from the dataset itself without performing any analysis on the kernel matrix.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1905.10587v3-abstract-full').style.display = 'none'; document.getElementById('1905.10587v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 May, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted to SIAM Journal on Matrix Analysis and Applications</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1905.06975">arXiv:1905.06975</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1905.06975">pdf</a>, <a href="https://arxiv.org/ps/1905.06975">ps</a>, <a href="https://arxiv.org/format/1905.06975">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ACCESS.2020.3015045">10.1109/ACCESS.2020.3015045 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Auto-tuning of dynamic scheduling applied to 3D reverse time migration on multicore systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Assis%2C+%C3%8D+A+S">Ãtalo A. S. Assis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fernandes%2C+J+B">JoÃ£o B. Fernandes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barros%2C+T">Tiago Barros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xavier-de-Souza%2C+S">Samuel Xavier-de-Souza</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1905.06975v2-abstract-short" style="display: inline;">
        &hellip;oil and gas industry to process seismic data. It is a computationally intensive task that suits well in parallel computers. Methods such as RTM can be parallelized in shared <span class="search-hit mathjax">memory</span> systems through scheduling iterations of parallel loops to threads. However, several aspects, such as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1905.06975v2-abstract-full').style.display = 'inline'; document.getElementById('1905.06975v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1905.06975v2-abstract-full" style="display: none;">
        Reverse time migration (RTM) is an algorithm widely used in the oil and gas industry to process seismic data. It is a computationally intensive task that suits well in parallel computers. Methods such as RTM can be parallelized in shared <span class="search-hit mathjax">memory</span> systems through scheduling iterations of parallel loops to threads. However, several aspects, such as <span class="search-hit mathjax">memory</span> size and hierarchy, number of cores, and input size, make <span class="search-hit mathjax">optimal</span> scheduling very challenging. In this paper, we introduce a run-time strategy to <span class="search-hit mathjax">automatically</span> tune the dynamic scheduling of parallel loops iterations in iterative <span class="search-hit mathjax">applications</span>, such as the RTM, in multicore systems. The proposed method aims to <span class="search-hit mathjax">reduce</span> the execution time of such <span class="search-hit mathjax">applications</span>. To find the <span class="search-hit mathjax">optimal</span> granularity, we propose a coupled simulated annealing (CSA) based auto-tuning strategy that adjusts the chunk size of work that OpenMP parallel loops assign dynamically to worker threads during the initialization of a 3D RTM <span class="search-hit mathjax">application</span>. Experiments performed with different computational systems and input sizes show that the proposed method is consistently better than the default OpenMP schedulers, static, auto, and guided, causing the <span class="search-hit mathjax">application</span> to be up to 33% faster. We show that the possible reason for this performance is the reduction of cache misses, mainly level L3, and low overhead, inferior to 2%. Having shown to be robust and scalable for the 3D RTM, the proposed method could also <span class="search-hit mathjax">improve</span> the performance of similar wave-based algorithms, such as full-waveform inversion (FWI) and other iterative <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1905.06975v2-abstract-full').style.display = 'none'; document.getElementById('1905.06975v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 May, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1905.05725">arXiv:1905.05725</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1905.05725">pdf</a>, <a href="https://arxiv.org/format/1905.05725">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Store-to-Leak Forwarding: Leaking Data on Meltdown-resistant CPUs (Updated and Extended Version)
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Schwarz%2C+M">Michael Schwarz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Canella%2C+C">Claudio Canella</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Giner%2C+L">Lukas Giner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gruss%2C+D">Daniel Gruss</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1905.05725v2-abstract-short" style="display: inline;">
        &hellip;exploit microarchitectural changes the CPU makes during transient out-of-order execution. Using side-channel techniques, these attacks enable leaking arbitrary data from <span class="search-hit mathjax">memory</span>. As state-of-the-art&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1905.05725v2-abstract-full').style.display = 'inline'; document.getElementById('1905.05725v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1905.05725v2-abstract-full" style="display: none;">
        Meltdown and Spectre exploit microarchitectural changes the CPU makes during transient out-of-order execution. Using side-channel techniques, these attacks enable leaking arbitrary data from <span class="search-hit mathjax">memory</span>. As state-of-the-art <span class="search-hit mathjax">software</span> mitigations for Meltdown may incur significant performance overheads, they are only seen as a temporary solution. Thus, <span class="search-hit mathjax">software</span> mitigations are disabled on more recent processors, which are not susceptible to Meltdown anymore.
  In this paper, we show that Meltdown-like attacks are still possible on recent CPUs which are not vulnerable to the original Meltdown attack. We show that the store buffer - a microarchitectural <span class="search-hit mathjax">optimization</span> to <span class="search-hit mathjax">reduce</span> the latency for data stores - in combination with the TLB enables powerful attacks. We present several ASLRrelated attacks, including a KASLR break from unprivileged <span class="search-hit mathjax">applications</span>, and breaking ASLR from JavaScript. We can also mount side-channel attacks, breaking the atomicity of TSX, and monitoring control flow of the kernel. Furthermore, when combined with a simple Spectre gadget, we can leak arbitrary data from <span class="search-hit mathjax">memory</span>. Our paper shows that Meltdown-like attacks are still possible, and <span class="search-hit mathjax">software</span> fixes are still necessary to ensure proper isolation between the kernel and user space.
  This updated extended version of the original paper includes new results and explanations on the root cause of the vulnerability and shows how it is different to MDS attacks like Fallout.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1905.05725v2-abstract-full').style.display = 'none'; document.getElementById('1905.05725v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 May, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1905.03113">arXiv:1905.03113</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1905.03113">pdf</a>, <a href="https://arxiv.org/ps/1905.03113">ps</a>, <a href="https://arxiv.org/format/1905.03113">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Locality-Sensitive Sketching for Resilient Network Flow Monitoring
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+Y">Yongquan Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+D">Dongsheng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+S">Siqi Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yiming Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+K">Kai Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1905.03113v1-abstract-short" style="display: inline;">
        &hellip;to heavy hitters. To cope with increasing network rates and massive traffic volumes, sketch based approximate measurement has been extensively studied to trade the accuracy for <span class="search-hit mathjax">memory</span> and computation cost, which unfortunately, is sensitive to hash collisions. In addition, deploying the sketch involves fine-grained performance control and instrumentation.
  T&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1905.03113v1-abstract-full').style.display = 'inline'; document.getElementById('1905.03113v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1905.03113v1-abstract-full" style="display: none;">
        Network monitoring is vital in modern clouds and data center networks for traffic engineering, network diagnosis, network intrusion detection, which need diverse traffic statistics ranging from flow size distributions to heavy hitters. To cope with increasing network rates and massive traffic volumes, sketch based approximate measurement has been extensively studied to trade the accuracy for <span class="search-hit mathjax">memory</span> and computation cost, which unfortunately, is sensitive to hash collisions. In addition, deploying the sketch involves fine-grained performance control and instrumentation.
  This paper presents a locality-sensitive sketch (LSS) to be resilient to hash collisions. LSS proactively minimizes the estimation error due to hash collisions with an autoencoder based <span class="search-hit mathjax">optimization</span> model, and <span class="search-hit mathjax">reduces</span> the estimation variance by keeping similar network flows to the same bucket array. To illustrate the feasibility of the sketch, we develop a disaggregated monitoring <span class="search-hit mathjax">application</span> that supports non-intrusive sketching deployment and native network-wide analysis. Testbed shows that the framework adapts to line rates and provides accurate query results. Real-world trace-driven simulations show that LSS remains stable performance under wide ranges of parameters and dramatically outperforms state-of-the-art sketching structures, with over $10^3$ to $10^5$ times reduction in relative errors for per-flow queries as the ratio of the number of buckets to the number of network flows <span class="search-hit mathjax">reduces</span> from 10\% to 0.1\%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1905.03113v1-abstract-full').style.display = 'none'; document.getElementById('1905.03113v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 May, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.12661">arXiv:1904.12661</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.12661">pdf</a>, <a href="https://arxiv.org/format/1904.12661">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring <span class="search-hit mathjax">Memory</span> Persistency Models for GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Z">Zhen Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alshboul%2C+M">Mohammad Alshboul</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Solihin%2C+Y">Yan Solihin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+H">Huiyang Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.12661v1-abstract-short" style="display: inline;">
        Given its high integration density, high speed, byte addressability, and low standby power, non-volatile or persistent <span class="search-hit mathjax">memory</span> is expected to supplement/replace DRAM as main&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.12661v1-abstract-full').style.display = 'inline'; document.getElementById('1904.12661v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.12661v1-abstract-full" style="display: none;">
        Given its high integration density, high speed, byte addressability, and low standby power, non-volatile or persistent <span class="search-hit mathjax">memory</span> is expected to supplement/replace DRAM as main <span class="search-hit mathjax">memory</span>. Through persistency <span class="search-hit mathjax">programming</span> models (which define durability ordering of stores) and durable transaction constructs, the programmer can provide recoverable data structure (RDS) which allows <span class="search-hit mathjax">programs</span> to recover to a consistent state after a failure. While persistency models have been well studied for CPUs, they have been neglected for graphics processing units (GPUs). Considering the importance of GPUs as a dominant accelerator for high performance computing, we investigate persistency models for GPUs.
  GPU <span class="search-hit mathjax">applications</span> exhibit substantial differences with CPUs <span class="search-hit mathjax">applications</span>, hence in this paper we adapt, re-architect, and <span class="search-hit mathjax">optimize</span> CPU persistency models for GPUs. We design a pragma-based compiler scheme to express persistency models for GPUs. We identify that the thread hierarchy in GPUs offers intuitive scopes to form epochs and durable transactions. We find that undo logging produces significant performance overheads. We propose to use idempotency analysis to <span class="search-hit mathjax">reduce</span> both logging frequency and the size of logs. Through both real-system and simulation evaluations, we show low overheads of our proposed architecture support.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.12661v1-abstract-full').style.display = 'none'; document.getElementById('1904.12661v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 April, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages, 16 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.11812">arXiv:1904.11812</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.11812">pdf</a>, <a href="https://arxiv.org/format/1904.11812">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Benchmarking Study to Evaluate Apache Spark on Large-Scale Supercomputers
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Thiruvathukal%2C+G+K">George K. Thiruvathukal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Christensen%2C+C">Cameron Christensen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+X">Xiaoyong Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tessier%2C+F">FranÃ§ois Tessier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vishwanath%2C+V">Venkatram Vishwanath</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.11812v2-abstract-short" style="display: inline;">
        &hellip;performance computing (HPC) are increasingly dependent on sophisticated dataflows and out-of-core methods for efficient system utilization. In addition, as HPC systems grow, <span class="search-hit mathjax">memory</span> access and data sharing are becoming performance bottlenecks. Cloud computing employs a data processing paradigm typically built on a loosely connected group of low-cost computing&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.11812v2-abstract-full').style.display = 'inline'; document.getElementById('1904.11812v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.11812v2-abstract-full" style="display: none;">
        As dataset sizes increase, data analysis tasks in high performance computing (HPC) are increasingly dependent on sophisticated dataflows and out-of-core methods for efficient system utilization. In addition, as HPC systems grow, <span class="search-hit mathjax">memory</span> access and data sharing are becoming performance bottlenecks. Cloud computing employs a data processing paradigm typically built on a loosely connected group of low-cost computing nodes without relying upon shared storage and/or <span class="search-hit mathjax">memory</span>. Apache Spark is a popular engine for large-scale data analysis in the cloud, which we have successfully deployed via job submission scripts on production clusters.
  In this paper, we describe common parallel analysis dataflows for both Message Passing Interface (MPI) and cloud based <span class="search-hit mathjax">applications</span>. We developed an effective benchmark to measure the performance characteristics of these tasks using both types of systems, specifically comparing MPI/C-based analyses with Spark. The benchmark is a data processing pipeline representative of a typical analytics framework implemented using map-<span class="search-hit mathjax">reduce</span>. In the case of Spark, we also consider whether language plays a role by writing tests using both Python and Scala, a language built on the Java Virtual Machine (JVM). We include performance results from two large systems at Argonne National Laboratory including Theta, a Cray XC40 supercomputer on which our experiments run with 65,536 cores (1024 nodes with 64 cores each). The results of our experiments are discussed in the context of their <span class="search-hit mathjax">applicability</span> to future HPC architectures. Beyond understanding performance, our work demonstrates that technologies such as Spark, while typically aimed at multi-tenant cloud-based environments, show promise for data analysis needs in a traditional clustering/supercomputing environment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.11812v2-abstract-full').style.display = 'none'; document.getElementById('1904.11812v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 September, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 April, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to IEEE Cloud 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.08365">arXiv:1904.08365</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.08365">pdf</a>, <a href="https://arxiv.org/format/1904.08365">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Probability">math.PR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Information and <span class="search-hit mathjax">Memory</span> in Dynamic Resource Allocation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+K">Kuang Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhong%2C+Y">Yuan Zhong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.08365v2-abstract-short" style="display: inline;">
        We propose a general framework, dubbed Stochastic Processing under Imperfect Information (SPII), to study the impact of information constraints and <span class="search-hit mathjax">memories</span> on dynamic resource allocation. The framework involves a Stochastic Processing Network (SPN) scheduling problem in which the scheduler may access the system state only through a noisy channel, and resour&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.08365v2-abstract-full').style.display = 'inline'; document.getElementById('1904.08365v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.08365v2-abstract-full" style="display: none;">
        We propose a general framework, dubbed Stochastic Processing under Imperfect Information (SPII), to study the impact of information constraints and <span class="search-hit mathjax">memories</span> on dynamic resource allocation. The framework involves a Stochastic Processing Network (SPN) scheduling problem in which the scheduler may access the system state only through a noisy channel, and resource allocation decisions must be carried out through the interaction between an encoding policy (who observes the state) and allocation policy (who chooses the allocation). <span class="search-hit mathjax">Applications</span> in the management of large-scale data centers and human-in-the-loop service systems are among our chief motivations.
  We quantify the degree to which information constraints <span class="search-hit mathjax">reduce</span> the size of the capacity region in general SPNs, and how such reduction depends on the amount of <span class="search-hit mathjax">memories</span> available to the encoding and allocation policies. Using a novel metric, capacity factor, our main theorem characterizes the reduction in capacity region (under &#34;<span class="search-hit mathjax">optimal</span>&#34; policies) for all non-degenerate channels, and across almost all combinations of <span class="search-hit mathjax">memory</span> sizes. Notably, the theorem demonstrates, in substantial generality, that (1) the presence of a noisy channel always <span class="search-hit mathjax">reduces</span> capacity, (2) more <span class="search-hit mathjax">memory</span> for the allocation policy always <span class="search-hit mathjax">improves</span> capacity, and (3) more <span class="search-hit mathjax">memory</span> for the encoding policy has little to no effect on capacity. Finally, all of our positive (achievability) results are established through constructive, implementable policies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.08365v2-abstract-full').style.display = 'none'; document.getElementById('1904.08365v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 August, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 April, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">48 pages, 5 figures, 1 table</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.05619">arXiv:1904.05619</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.05619">pdf</a>, <a href="https://arxiv.org/ps/1904.05619">ps</a>, <a href="https://arxiv.org/format/1904.05619">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Instrumentation and Methods for Astrophysics">astro-ph.IM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Stochastic LBFGS Algorithm for Radio Interferometric Calibration
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yatawatta%2C+S">Sarod Yatawatta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=De+Clercq%2C+L">Lukas De Clercq</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Spreeuw%2C+H">Hanno Spreeuw</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Diblen%2C+F">Faruk Diblen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.05619v2-abstract-short" style="display: inline;">
        We present a stochastic, limited-<span class="search-hit mathjax">memory</span> Broyden Fletcher Goldfarb Shanno (LBFGS) algorithm that is suitable for handling very large amounts of data. A direct&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.05619v2-abstract-full').style.display = 'inline'; document.getElementById('1904.05619v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.05619v2-abstract-full" style="display: none;">
        We present a stochastic, limited-<span class="search-hit mathjax">memory</span> Broyden Fletcher Goldfarb Shanno (LBFGS) algorithm that is suitable for handling very large amounts of data. A direct <span class="search-hit mathjax">application</span> of this algorithm is radio interferometric calibration of raw data at fine time and frequency resolution. Almost all existing radio interferometric calibration algorithms assume that it is possible to fit the dataset being calibrated into <span class="search-hit mathjax">memory</span>. Therefore, the raw data is averaged in time and frequency to <span class="search-hit mathjax">reduce</span> its size by many orders of magnitude before calibration is performed. However, this averaging is detrimental for the detection of some signals of interest that have narrow bandwidth and time duration such as fast radio bursts (FRBs). Using the proposed algorithm, it is possible to calibrate data at such a fine resolution that they cannot be entirely loaded into <span class="search-hit mathjax">memory</span>, thus preserving such signals. As an additional demonstration, we use the proposed algorithm for training deep neural networks and compare the performance against the mainstream first order <span class="search-hit mathjax">optimization</span> algorithms that are used in deep learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.05619v2-abstract-full').style.display = 'none'; document.getElementById('1904.05619v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 April, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Draft, final version in IEEE Data Science Workshop 2019 proceedings</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.04175">arXiv:1904.04175</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.04175">pdf</a>, <a href="https://arxiv.org/format/1904.04175">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Data-Driven Design for Fourier Ptychographic Microscopy
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kellman%2C+M">Michael Kellman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bostan%2C+E">Emrah Bostan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+M">Michael Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Waller%2C+L">Laura Waller</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.04175v1-abstract-short" style="display: inline;">
        &hellip;captured by an LED array microscope with programmable source patterns. FPM provides simultaneous large field-of-view and high resolution imaging, but at the cost of <span class="search-hit mathjax">reduced</span> temporal resolution, thereby limiting live cell&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.04175v1-abstract-full').style.display = 'inline'; document.getElementById('1904.04175v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.04175v1-abstract-full" style="display: none;">
        Fourier Ptychographic Microscopy (FPM) is a computational imaging method that is able to super-resolve features beyond the diffraction-limit set by the objective lens of a traditional microscope. This is accomplished by using synthetic aperture and phase retrieval algorithms to combine many measurements captured by an LED array microscope with programmable source patterns. FPM provides simultaneous large field-of-view and high resolution imaging, but at the cost of <span class="search-hit mathjax">reduced</span> temporal resolution, thereby limiting live cell <span class="search-hit mathjax">applications</span>. In this work, we learn LED source pattern designs that compress the many required measurements into only a few, with negligible loss in reconstruction quality or resolution. This is accomplished by recasting the super-resolution reconstruction as a Physics-based Neural Network and learning the experimental design to <span class="search-hit mathjax">optimize</span> the network&#39;s overall performance. Specifically, we learn LED patterns for different <span class="search-hit mathjax">applications</span> (e.g. amplitude contrast and quantitative phase imaging) and show that the designs we learn through simulation generalize well in the experimental setting. Further, we discuss a context-specific loss function, practical <span class="search-hit mathjax">memory</span> limitations, and interpretability of our learned designs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.04175v1-abstract-full').style.display = 'none'; document.getElementById('1904.04175v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 9 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.02241">arXiv:1904.02241</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.02241">pdf</a>, <a href="https://arxiv.org/format/1904.02241">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GraphCage: Cache Aware Graph Processing on GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xuhao Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.02241v1-abstract-short" style="display: inline;">
        &hellip;GPUs to accelerate irregular graph algorithms is even more difficult to be efficient, since GPU&#39;s highly structured SIMT architecture is not a natural fit for irregular <span class="search-hit mathjax">applications</span>. With lots of previous efforts spent on subtly mapping graph algorithms onto the GPU, the performance of graph processing on GPUs is still highly&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.02241v1-abstract-full').style.display = 'inline'; document.getElementById('1904.02241v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.02241v1-abstract-full" style="display: none;">
        Efficient Graph processing is challenging because of the irregularity of graph algorithms. Using GPUs to accelerate irregular graph algorithms is even more difficult to be efficient, since GPU&#39;s highly structured SIMT architecture is not a natural fit for irregular <span class="search-hit mathjax">applications</span>. With lots of previous efforts spent on subtly mapping graph algorithms onto the GPU, the performance of graph processing on GPUs is still highly <span class="search-hit mathjax">memory</span>-latency bound, leading to low utilization of compute resources. Random <span class="search-hit mathjax">memory</span> accesses generated by the sparse graph data structure are the major causes of this significant <span class="search-hit mathjax">memory</span> access latency. Simply applying the conventional cache blocking technique proposed for matrix computation have limited benefit due to the significant overhead on the GPU. We propose GraphCage, a cache centric <span class="search-hit mathjax">optimization</span> framework for highly efficient graph processing on GPUs. We first present a throughput-oriented cache blocking scheme (TOCAB) in both push and pull directions. Comparing with conventional cache blocking which suffers repeated accesses when processing large graphs on GPUs, TOCAB is specifically <span class="search-hit mathjax">optimized</span> for the GPU architecture to <span class="search-hit mathjax">reduce</span> this overhead and <span class="search-hit mathjax">improve</span> <span class="search-hit mathjax">memory</span> access efficiency. To integrate our scheme into state-of-the-art implementations without significant overhead, we coordinate TOCAB with load balancing strategies by considering the sparsity of subgraphs. To enable cache blocking for traversal-based algorithms, we consider the benefit and overhead in different iterations with different working set sizes, and apply TOCAB for topology-driven kernels in pull direction. Evaluation shows that GraphCage can <span class="search-hit mathjax">improve</span> performance by 2 ~ 4x compared to hand <span class="search-hit mathjax">optimized</span> implementations and state-of-the-art frameworks (e.g. CuSha and Gunrock), with less <span class="search-hit mathjax">memory</span> consumption than CuSha.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.02241v1-abstract-full').style.display = 'none'; document.getElementById('1904.02241v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 April, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.11749">arXiv:1903.11749</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.11749">pdf</a>, <a href="https://arxiv.org/format/1903.11749">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3308558.3313555">10.1145/3308558.3313555 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Distributed Algorithms for Fully Personalized PageRank on Large Graphs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+W">Wenqing Lin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.11749v1-abstract-short" style="display: inline;">
        Personalized PageRank (PPR) has enormous <span class="search-hit mathjax">applications</span>, such as link prediction and recommendation systems for social networks, which often require the fully PPR to be known. Besides, most of real-life graphs are edge-weighted, e.g., the interaction between users on the Facebook network. However, it is computationally difficult to compute the fully PPR, espec&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.11749v1-abstract-full').style.display = 'inline'; document.getElementById('1903.11749v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.11749v1-abstract-full" style="display: none;">
        Personalized PageRank (PPR) has enormous <span class="search-hit mathjax">applications</span>, such as link prediction and recommendation systems for social networks, which often require the fully PPR to be known. Besides, most of real-life graphs are edge-weighted, e.g., the interaction between users on the Facebook network. However, it is computationally difficult to compute the fully PPR, especially on large graphs, not to mention that most existing approaches do not consider the weights of edges. In particular, the existing approach cannot handle graphs with billion edges on a moderate-size cluster. To address this problem, this paper presents a novel study on the computation of fully edge-weighted PPR on large graphs using the distributed computing framework. Specifically, we employ the Monte Carlo approximation that performs a large number of random walks from each node of the graph, and exploits the parallel pipeline framework to <span class="search-hit mathjax">reduce</span> the overall running time of the fully PPR. Based on that, we develop several <span class="search-hit mathjax">optimization</span> techniques which (i) alleviate the issue of large nodes that could explode the <span class="search-hit mathjax">memory</span> space, (ii) pre-compute short walks for small nodes that largely speedup the computation of random walks, and (iii) <span class="search-hit mathjax">optimize</span> the amount of random walks to compute in each pipeline that significantly <span class="search-hit mathjax">reduces</span> the overhead. With extensive experiments on a variety of real-life graph datasets, we demonstrate that our solution is several orders of magnitude faster than the state-of-the-arts, and meanwhile, largely outperforms the baseline algorithms in terms of accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.11749v1-abstract-full').style.display = 'none'; document.getElementById('1903.11749v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Full Research Paper accepted in the proceedings of the 30th World Wide Web Conference (WWW 2019)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.06631">arXiv:1903.06631</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.06631">pdf</a>, <a href="https://arxiv.org/format/1903.06631">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient <span class="search-hit mathjax">Memory</span> Management for GPU-based Deep Learning Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Junzhe Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yeung%2C+S+H">Sai Ho Yeung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shu%2C+Y">Yao Shu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+B">Bingsheng He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wei Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.06631v1-abstract-short" style="display: inline;">
        GPU (graphics processing unit) has been used for many data-intensive <span class="search-hit mathjax">applications</span>. Among them, deep learning systems are one of the most important consumer systems for GPU nowadays. As deep learning&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.06631v1-abstract-full').style.display = 'inline'; document.getElementById('1903.06631v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.06631v1-abstract-full" style="display: none;">
        GPU (graphics processing unit) has been used for many data-intensive <span class="search-hit mathjax">applications</span>. Among them, deep learning systems are one of the most important consumer systems for GPU nowadays. As deep learning <span class="search-hit mathjax">applications</span> impose deeper and larger models in order to achieve higher accuracy, <span class="search-hit mathjax">memory</span> management becomes an important research topic for deep learning systems, given that GPU has limited <span class="search-hit mathjax">memory</span> size. Many approaches have been proposed towards this issue, e.g., model compression and <span class="search-hit mathjax">memory</span> swapping. However, they either degrade the model accuracy or require a lot of manual intervention. In this paper, we propose two orthogonal approaches to <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">memory</span> cost from the system perspective. Our approaches are transparent to the models, and thus do not affect the model accuracy. They are achieved by exploiting the iterative nature of the training algorithm of deep learning to derive the lifetime and read/write order of all variables. With the lifetime semantics, we are able to implement a <span class="search-hit mathjax">memory</span> pool with minimal fragments. However, the <span class="search-hit mathjax">optimization</span> problem is NP-complete. We propose a heuristic algorithm that <span class="search-hit mathjax">reduces</span> up to 13.3% of <span class="search-hit mathjax">memory</span> compared with Nvidia&#39;s default <span class="search-hit mathjax">memory</span> pool with equal time complexity. With the read/write semantics, the variables that are not in use can be swapped out from GPU to CPU to <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">memory</span> footprint. We propose multiple swapping strategies to <span class="search-hit mathjax">automatically</span> decide which variable to swap and when to swap out (in), which <span class="search-hit mathjax">reduces</span> the <span class="search-hit mathjax">memory</span> cost by up to 34.2% without communication overhead.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.06631v1-abstract-full').style.display = 'none'; document.getElementById('1903.06631v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 February, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.04395">arXiv:1903.04395</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.04395">pdf</a>, <a href="https://arxiv.org/format/1903.04395">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A GraphBLAS Approach for Subgraph Counting
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Langshi Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiayu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Azad%2C+A">Ariful Azad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+L">Lei Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marathe%2C+M">Madhav Marathe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vullikanti%2C+A">Anil Vullikanti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nikolaev%2C+A">Andrey Nikolaev</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Smirnov%2C+E">Egor Smirnov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Israfilov%2C+R">Ruslan Israfilov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+J">Judy Qiu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.04395v1-abstract-short" style="display: inline;">
        &hellip;the occurrences of a subgraph template T in a given network G. The basic problem of computing structural properties such as counting triangles and other subgraphs has found <span class="search-hit mathjax">applications</span> in diverse domains. Recent biological, social, cybersecurity and sensor network&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.04395v1-abstract-full').style.display = 'inline'; document.getElementById('1903.04395v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.04395v1-abstract-full" style="display: none;">
        Subgraph counting aims to count the occurrences of a subgraph template T in a given network G. The basic problem of computing structural properties such as counting triangles and other subgraphs has found <span class="search-hit mathjax">applications</span> in diverse domains. Recent biological, social, cybersecurity and sensor network <span class="search-hit mathjax">applications</span> have motivated solving such problems on massive networks with billions of vertices. The larger subgraph problem is known to be <span class="search-hit mathjax">memory</span> bounded and computationally challenging to scale; the complexity grows both as a function of T and G. In this paper, we study the non-induced tree subgraph counting problem, propose a novel layered softwarehardware co-design approach, and implement a shared-<span class="search-hit mathjax">memory</span> multi-threaded algorithm: 1) <span class="search-hit mathjax">reducing</span> the complexity of the parallel color-<span class="search-hit mathjax">coding</span> algorithm by identifying and pruning redundant graph traversal; 2) achieving a fully-vectorized implementation upon linear algebra kernels inspired by GraphBLAS, which significantly <span class="search-hit mathjax">improves</span> cache usage and maximizes <span class="search-hit mathjax">memory</span> bandwidth utilization. Experiments show that our implementation <span class="search-hit mathjax">improves</span> the overall performance over the state-of-the-art work by orders of magnitude and up to 660x for subgraph templates with size over 12 on a dual-socket Intel(R) Xeon(R) Platinum 8160 server. We believe our approach using GraphBLAS with <span class="search-hit mathjax">optimized</span> sparse linear algebra can be applied to other massive subgraph counting problems and emerging high-<span class="search-hit mathjax">memory</span> bandwidth hardware architectures.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.04395v1-abstract-full').style.display = 'none'; document.getElementById('1903.04395v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.02153">arXiv:1903.02153</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.02153">pdf</a>, <a href="https://arxiv.org/format/1903.02153">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.jpdc.2021.04.005">10.1016/j.jpdc.2021.04.005 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PBBFMM3D: a parallel black-box algorithm for kernel matrix-vector multiplication
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Ruoxi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+J">Jonghyun Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Darve%2C+E">Eric Darve</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.02153v3-abstract-short" style="display: inline;">
        Kernel matrix-vector product is ubiquitous in many science and engineering <span class="search-hit mathjax">applications</span>. However, a naive method requires $O(N^2)$ operations, which becomes prohibitive for large-scale problems. We introduce a parallel method that provably requires $O(N)$ operations to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.02153v3-abstract-full').style.display = 'inline'; document.getElementById('1903.02153v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.02153v3-abstract-full" style="display: none;">
        Kernel matrix-vector product is ubiquitous in many science and engineering <span class="search-hit mathjax">applications</span>. However, a naive method requires $O(N^2)$ operations, which becomes prohibitive for large-scale problems. We introduce a parallel method that provably requires $O(N)$ operations to <span class="search-hit mathjax">reduce</span> the computation cost. The distinct feature of our method is that it requires only the ability to evaluate the kernel function, offering a black-box interface to users. Our parallel approach targets multi-core shared-<span class="search-hit mathjax">memory</span> machines and is implemented using OpenMP. Numerical results demonstrate up to $19\times$ speedup on 32 cores. We also present a real-world <span class="search-hit mathjax">application</span> in geostatistics, where our parallel method was used to deliver fast principle component analysis of covariance matrices.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.02153v3-abstract-full').style.display = 'none'; document.getElementById('1903.02153v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 March, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Journal of Parallel and Distributed Computing Volume 154, August 2021, Pages 64-73
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.07353">arXiv:1902.07353</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.07353">pdf</a>, <a href="https://arxiv.org/format/1902.07353">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        In oder Aus
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Madison%2C+E">Ethan Madison</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zipper%2C+Z">Zachary Zipper</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.07353v1-abstract-short" style="display: inline;">
        Bloom filters are data structures used to determine set membership of elements, with <span class="search-hit mathjax">applications</span> from string matching to networking and security problems. These structures are favored because of their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.07353v1-abstract-full').style.display = 'inline'; document.getElementById('1902.07353v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.07353v1-abstract-full" style="display: none;">
        Bloom filters are data structures used to determine set membership of elements, with <span class="search-hit mathjax">applications</span> from string matching to networking and security problems. These structures are favored because of their <span class="search-hit mathjax">reduced</span> <span class="search-hit mathjax">memory</span> consumption and fast wallclock and asymptotic time bounds. Generally, Bloom filters maintain constant membership query time, making them very fast in their niche. However, they are limited in their lack of a removal operation, as well as by their probabilistic nature. In this paper, we discuss various iterations of and alternatives to the generic Bloom filter that have been researched and implemented to overcome their inherent limitations. Bloom filters, especially when used in conjunction with other data structures, are still powerful and efficient data structures; we further discuss their use in industy and research to <span class="search-hit mathjax">optimize</span> resource utilization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.07353v1-abstract-full').style.display = 'none'; document.getElementById('1902.07353v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 February, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.06570">arXiv:1902.06570</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.06570">pdf</a>, <a href="https://arxiv.org/format/1902.06570">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Binary Debloating for Security via Demand Driven Loading
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mururu%2C+G">Girish Mururu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Porter%2C+C">Chris Porter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barua%2C+P">Prithayan Barua</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pande%2C+S">Santosh Pande</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.06570v1-abstract-short" style="display: inline;">
        Modern <span class="search-hit mathjax">software</span> systems heavily use C/C++ based libraries. Because of the weak&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.06570v1-abstract-full').style.display = 'inline'; document.getElementById('1902.06570v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.06570v1-abstract-full" style="display: none;">
        Modern <span class="search-hit mathjax">software</span> systems heavily use C/C++ based libraries. Because of the weak <span class="search-hit mathjax">memory</span> model of C/C++, libraries may suffer from vulnerabilities which can expose the <span class="search-hit mathjax">applications</span> to potential attacks. For example, a very large number of return oriented <span class="search-hit mathjax">programming</span> gadgets exist in glibc that allow stitching together semantically valid but malicious Turing-complete <span class="search-hit mathjax">programs</span>. In spite of significant advances in attack detection and mitigation, full defense is unrealistic against an ever-growing set of possibilities for generating such malicious <span class="search-hit mathjax">programs</span>.
  In this work, we create a defense mechanism by debloating libraries to <span class="search-hit mathjax">reduce</span> the dynamic functions linked so that the possibilities of constructing malicious <span class="search-hit mathjax">programs</span> diminishes significantly. The key idea is to locate each library call site within an <span class="search-hit mathjax">application</span>, and in each case to load only the set of library functions that will be used at that call site. This approach of demand-driven loading relies on an input-aware oracle that predicts a near-exact set of library functions needed at a given call site during the execution. The predicted functions are loaded just in time, and the complete call chain (of function bodies) inside the library is purged after returning from the library call back into the <span class="search-hit mathjax">application</span>. We present a decision-tree based predictor, which acts as an oracle, and an <span class="search-hit mathjax">optimized</span> runtime system, which works directly with library binaries like GNU libc and libstdc++. We show that on average, the proposed scheme cuts the exposed <span class="search-hit mathjax">code</span> surface of libraries by 97.2%, <span class="search-hit mathjax">reduces</span> ROP gadgets present in linked libraries by 97.9%, achieves a prediction accuracy in most cases of at least 97%, and adds a small runtime overhead of 18% on all libraries (16% for glibc, 2% for others) across all benchmarks of SPEC 2006, suggesting this scheme is practical.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.06570v1-abstract-full').style.display = 'none'; document.getElementById('1902.06570v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 February, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.05462">arXiv:1902.05462</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.05462">pdf</a>, <a href="https://arxiv.org/format/1902.05462">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Redundant Loads: A <span class="search-hit mathjax">Software</span> Inefficiency Indicator
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+P">Pengfei Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+S">Shasha Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+H">Hailong Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chabbi%2C+M">Milind Chabbi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xu Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.05462v1-abstract-short" style="display: inline;">
        Modern <span class="search-hit mathjax">software</span> packages have become increasingly complex with millions of lines of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.05462v1-abstract-full').style.display = 'inline'; document.getElementById('1902.05462v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.05462v1-abstract-full" style="display: none;">
        Modern <span class="search-hit mathjax">software</span> packages have become increasingly complex with millions of lines of <span class="search-hit mathjax">code</span> and references to many external libraries. Redundant operations are a common performance limiter in these <span class="search-hit mathjax">code</span> bases. Missed compiler <span class="search-hit mathjax">optimization</span> opportunities, inappropriate data structure and algorithm choices, and developers&#39; inattention to performance are some common reasons for the existence of redundant operations. Developers mainly depend on compilers to eliminate redundant operations. However, compilers&#39; static analysis often misses <span class="search-hit mathjax">optimization</span> opportunities due to ambiguities and limited analysis scope; <span class="search-hit mathjax">automatic</span> <span class="search-hit mathjax">optimizations</span> to algorithmic and data structural problems are out of scope.
  We develop LoadSpy, a whole-<span class="search-hit mathjax">program</span> profiler to pinpoint redundant <span class="search-hit mathjax">memory</span> load operations, which are often a symptom of many redundant operations. The strength of LoadSpy exists in identifying and quantifying redundant load operations in <span class="search-hit mathjax">programs</span> and associating the redundancies with <span class="search-hit mathjax">program</span> execution contexts and scopes to focus developers&#39; attention on problematic <span class="search-hit mathjax">code</span>. LoadSpy works on fully <span class="search-hit mathjax">optimized</span> binaries, adopts various <span class="search-hit mathjax">optimization</span> techniques to <span class="search-hit mathjax">reduce</span> its overhead, and provides a rich graphic user interface, which make it a complete developer tool. Applying LoadSpy showed that a large fraction of redundant loads is common in modern <span class="search-hit mathjax">software</span> packages despite highest levels of <span class="search-hit mathjax">automatic</span> compiler <span class="search-hit mathjax">optimizations</span>. Guided by LoadSpy, we <span class="search-hit mathjax">optimize</span> several well-known benchmarks and real-world <span class="search-hit mathjax">applications</span>, yielding significant speedups.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.05462v1-abstract-full').style.display = 'none'; document.getElementById('1902.05462v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 February, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper is a full-version of our ICSE paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.10997">arXiv:1901.10997</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.10997">pdf</a>, <a href="https://arxiv.org/format/1901.10997">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hardware-Guided Symbiotic Training for Compact, Accurate, yet Execution-Efficient LSTM
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+H">Hongxu Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+G">Guoyang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yingmin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Che%2C+S">Shuai Che</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Weifeng Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jha%2C+N+K">Niraj K. Jha</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.10997v1-abstract-short" style="display: inline;">
        Many long short-term <span class="search-hit mathjax">memory</span> (LSTM)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.10997v1-abstract-full').style.display = 'inline'; document.getElementById('1901.10997v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.10997v1-abstract-full" style="display: none;">
        Many long short-term <span class="search-hit mathjax">memory</span> (LSTM) <span class="search-hit mathjax">applications</span> need fast yet compact models. Neural network compression approaches, such as the grow-and-prune paradigm, have proved to be promising for cutting down network complexity by skipping insignificant weights. However, current compression strategies are mostly hardware-agnostic and network complexity reduction does not always translate into execution efficiency. In this work, we propose a hardware-guided symbiotic training methodology for compact, accurate, yet execution-efficient inference models. It is based on our observation that hardware may introduce substantial non-monotonic behavior, which we call the latency hysteresis effect, when evaluating network size vs. inference latency. This observation raises question about the mainstream smaller-dimension-is-better compression strategy, which often leads to a sub-<span class="search-hit mathjax">optimal</span> model architecture. By leveraging the hardware-impacted hysteresis effect and sparsity, we are able to achieve the symbiosis of model compactness and accuracy with execution efficiency, thus <span class="search-hit mathjax">reducing</span> LSTM latency while increasing its accuracy. We have evaluated our algorithms on language modeling and speech recognition <span class="search-hit mathjax">applications</span>. Relative to the traditional stacked LSTM architecture obtained for the Penn Treebank dataset, we <span class="search-hit mathjax">reduce</span> the number of parameters by 18.0x (30.5x) and measured run-time latency by up to 2.4x (5.2x) on Nvidia GPUs (Intel Xeon CPUs) without any accuracy degradation. For the DeepSpeech2 architecture obtained for the AN4 dataset, we <span class="search-hit mathjax">reduce</span> the number of parameters by 7.0x (19.4x), word error rate from 12.9% to 9.9% (10.4%), and measured run-time latency by up to 1.7x (2.4x) on Nvidia GPUs (Intel Xeon CPUs). Thus, our method yields compact, accurate, yet execution-efficient inference models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.10997v1-abstract-full').style.display = 'none'; document.getElementById('1901.10997v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 January, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.09290">arXiv:1901.09290</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.09290">pdf</a>, <a href="https://arxiv.org/format/1901.09290">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3295500.3356156">10.1145/3295500.3356156 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PruneTrain: Fast Neural Network Training by Dynamic Sparse Model Reconfiguration
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lym%2C+S">Sangkug Lym</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Choukse%2C+E">Esha Choukse</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zangeneh%2C+S">Siavash Zangeneh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+W">Wei Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sanghavi%2C+S">Sujay Sanghavi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erez%2C+M">Mattan Erez</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.09290v5-abstract-short" style="display: inline;">
        State-of-the-art convolutional neural networks (CNNs) used in vision <span class="search-hit mathjax">applications</span> have large models with numerous weights. Training these models is very compute- and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.09290v5-abstract-full').style.display = 'inline'; document.getElementById('1901.09290v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.09290v5-abstract-full" style="display: none;">
        State-of-the-art convolutional neural networks (CNNs) used in vision <span class="search-hit mathjax">applications</span> have large models with numerous weights. Training these models is very compute- and <span class="search-hit mathjax">memory</span>-resource intensive. Much research has been done on pruning or compressing these models to <span class="search-hit mathjax">reduce</span> the cost of inference, but little work has addressed the costs of training. We focus precisely on accelerating training. We propose PruneTrain, a cost-efficient mechanism that gradually <span class="search-hit mathjax">reduces</span> the training cost during training. PruneTrain uses a structured group-lasso regularization approach that drives the training <span class="search-hit mathjax">optimization</span> toward both high accuracy and small weight values. Small weights can then be periodically removed by reconfiguring the network model to a smaller one. By using a structured-pruning approach and additional reconfiguration techniques we introduce, the pruned model can still be efficiently processed on a GPU accelerator. Overall, PruneTrain achieves a reduction of 39% in the end-to-end training time of ResNet50 for ImageNet by <span class="search-hit mathjax">reducing</span> computation cost by 40% in FLOPs, <span class="search-hit mathjax">memory</span> accesses by 37% for <span class="search-hit mathjax">memory</span> bandwidth bound layers, and the inter-accelerator communication by 55%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.09290v5-abstract-full').style.display = 'none'; document.getElementById('1901.09290v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 December, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 January, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.07827">arXiv:1901.07827</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.07827">pdf</a>, <a href="https://arxiv.org/format/1901.07827">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Compact ConvNets via Structure-Sparsity Regularized Filter Pruning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+S">Shaohui Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+R">Rongrong Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuchao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+C">Cheng Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xuelong Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.07827v2-abstract-short" style="display: inline;">
        The success of convolutional neural networks (CNNs) in computer vision <span class="search-hit mathjax">applications</span> has been accompanied by a significant increase of computation and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.07827v2-abstract-full').style.display = 'inline'; document.getElementById('1901.07827v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.07827v2-abstract-full" style="display: none;">
        The success of convolutional neural networks (CNNs) in computer vision <span class="search-hit mathjax">applications</span> has been accompanied by a significant increase of computation and <span class="search-hit mathjax">memory</span> costs, which prohibits its usage on resource-limited environments such as mobile or embedded devices. To this end, the research of CNN compression has recently become emerging. In this paper, we propose a novel filter pruning scheme, termed structured sparsity regularization (SSR), to simultaneously speedup the computation and <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">memory</span> overhead of CNNs, which can be well supported by various off-the-shelf deep learning libraries. Concretely, the proposed scheme incorporates two different regularizers of structured sparsity into the original objective function of filter pruning, which fully coordinates the global outputs and local pruning operations to adaptively prune filters. We further propose an Alternative Updating with Lagrange Multipliers (AULM) scheme to efficiently solve its <span class="search-hit mathjax">optimization</span>. AULM follows the principle of ADMM and alternates between promoting the structured sparsity of CNNs and <span class="search-hit mathjax">optimizing</span> the recognition loss, which leads to a very efficient solver (2.5x to the most recent work that directly solves the group sparsity-based regularization). Moreover, by imposing the structured sparsity, the online inference is extremely <span class="search-hit mathjax">memory</span>-light, since the number of filters and the output feature maps are simultaneously <span class="search-hit mathjax">reduced</span>. The proposed scheme has been deployed to a variety of state-of-the-art CNN structures including LeNet, AlexNet, VGG, ResNet and GoogLeNet over different datasets. Quantitative results demonstrate that the proposed scheme achieves superior performance over the state-of-the-art methods. We further demonstrate the proposed compression scheme for the task of transfer learning, including domain adaptation and object detection, which also show exciting performance gains over the state-of-the-arts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.07827v2-abstract-full').style.display = 'none'; document.getElementById('1901.07827v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 March, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 January, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1812.07106">arXiv:1812.07106</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1812.07106">pdf</a>, <a href="https://arxiv.org/format/1812.07106">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        E-RNN: Design <span class="search-hit mathjax">Optimization</span> for Efficient Recurrent Neural Networks in FPGAs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhe Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+C">Caiwen Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Siyue Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+W">Wujie Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhuo%2C+Y">Youwei Zhuo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+Q">Qinru Qiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+W">Wenyao Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+X">Xue Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+X">Xuehai Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yanzhi Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1812.07106v1-abstract-short" style="display: inline;">
        Recurrent Neural Networks (RNNs) are becoming increasingly important for time series-related <span class="search-hit mathjax">applications</span> which require efficient and real-time implementations. The two major types are Long Short-Term&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.07106v1-abstract-full').style.display = 'inline'; document.getElementById('1812.07106v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1812.07106v1-abstract-full" style="display: none;">
        Recurrent Neural Networks (RNNs) are becoming increasingly important for time series-related <span class="search-hit mathjax">applications</span> which require efficient and real-time implementations. The two major types are Long Short-Term <span class="search-hit mathjax">Memory</span> (LSTM) and Gated Recurrent Unit (GRU) networks. It is a challenging task to have real-time, efficient, and accurate hardware RNN implementations because of the high sensitivity to imprecision accumulation and the requirement of special activation function implementations.
  A key limitation of the prior works is the lack of a systematic design <span class="search-hit mathjax">optimization</span> framework of RNN model and hardware implementations, especially when the block size (or compression ratio) should be jointly <span class="search-hit mathjax">optimized</span> with RNN type, layer size, etc. In this paper, we adopt the block-circulant matrix-based framework, and present the Efficient RNN (E-RNN) framework for FPGA implementations of the <span class="search-hit mathjax">Automatic</span> Speech Recognition (ASR) <span class="search-hit mathjax">application</span>. The overall goal is to <span class="search-hit mathjax">improve</span> performance/energy efficiency under accuracy requirement. We use the alternating direction method of multipliers (ADMM) technique for more accurate block-circulant training, and present two design explorations providing guidance on block size and <span class="search-hit mathjax">reducing</span> RNN training trials. Based on the two observations, we decompose E-RNN in two phases: Phase I on determining RNN model to <span class="search-hit mathjax">reduce</span> computation and storage subject to accuracy requirement, and Phase II on hardware implementations given RNN model, including processing element design/<span class="search-hit mathjax">optimization</span>, quantization, activation implementation, etc. Experimental results on actual FPGA deployments show that E-RNN achieves a maximum energy efficiency <span class="search-hit mathjax">improvement</span> of 37.4$\times$ compared with ESE, and more than 2$\times$ compared with C-LSTM, under the same accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.07106v1-abstract-full').style.display = 'none'; document.getElementById('1812.07106v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 December, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In The 25th International Symposium on High-Performance Computer Architecture (HPCA 2019)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1812.05955">arXiv:1812.05955</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1812.05955">pdf</a>, <a href="https://arxiv.org/format/1812.05955">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Impact of Traditional Sparse <span class="search-hit mathjax">Optimizations</span> on a Migratory Thread Architecture
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rolinger%2C+T+B">Thomas B. Rolinger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Krieger%2C+C+D">Christopher D. Krieger</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1812.05955v1-abstract-short" style="display: inline;">
        Achieving high performance for sparse <span class="search-hit mathjax">applications</span> is challenging due to irregular access patterns and weak locality. These properties preclude many static&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.05955v1-abstract-full').style.display = 'inline'; document.getElementById('1812.05955v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1812.05955v1-abstract-full" style="display: none;">
        Achieving high performance for sparse <span class="search-hit mathjax">applications</span> is challenging due to irregular access patterns and weak locality. These properties preclude many static <span class="search-hit mathjax">optimizations</span> and degrade cache performance on traditional systems. To address these challenges, novel systems such as the Emu architecture have been proposed. The Emu design uses light-weight migratory threads, narrow <span class="search-hit mathjax">memory</span>, and near-<span class="search-hit mathjax">memory</span> processing capabilities to address weak locality and <span class="search-hit mathjax">reduce</span> the total load on the <span class="search-hit mathjax">memory</span> system. Because the Emu architecture is fundamentally different than cache based hierarchical <span class="search-hit mathjax">memory</span> systems, it is crucial to understand the cost-benefit tradeoffs of standard sparse algorithm <span class="search-hit mathjax">optimizations</span> on Emu hardware. In this work, we explore sparse matrix-vector multiplication (SpMV) on the Emu architecture. We investigate the effects of different sparse <span class="search-hit mathjax">optimizations</span> such as dense vector data layouts, work distributions, and matrix reorderings. Our study finds that initially distributing work evenly across the system is inadequate to maintain load balancing over time due to the migratory nature of Emu threads. In severe cases, matrix sparsity patterns produce hot-spots as many migratory threads converge on a single resource. We demonstrate that known matrix reordering techniques can <span class="search-hit mathjax">improve</span> SpMV performance on the Emu architecture by as much as 70% by encouraging more consistent load balancing. This can be compared with a performance gain of no more than 16% on a cache-<span class="search-hit mathjax">memory</span> based system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.05955v1-abstract-full').style.display = 'none'; document.getElementById('1812.05955v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 December, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8th Workshop on Irregular Applications: Architectures and Algorithms (IA^3) 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1812.04070">arXiv:1812.04070</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1812.04070">pdf</a>, <a href="https://arxiv.org/format/1812.04070">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SIMD-X: <span class="search-hit mathjax">Programming</span> and Processing of Graph Algorithms on GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H+H">H. Howie Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1812.04070v1-abstract-short" style="display: inline;">
        With high computation power and <span class="search-hit mathjax">memory</span> bandwidth, graphics processing units (GPUs) lend themselves to accelerate data-intensive analytics, especially when such&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.04070v1-abstract-full').style.display = 'inline'; document.getElementById('1812.04070v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1812.04070v1-abstract-full" style="display: none;">
        With high computation power and <span class="search-hit mathjax">memory</span> bandwidth, graphics processing units (GPUs) lend themselves to accelerate data-intensive analytics, especially when such <span class="search-hit mathjax">applications</span> fit the single instruction multiple data (SIMD) model. However, graph algorithms such as breadth-first search and k-core, often fail to take full advantage of GPUs, due to irregularity in <span class="search-hit mathjax">memory</span> access and control flow. To address this challenge, we have developed SIMD-X, for <span class="search-hit mathjax">programming</span> and processing of single instruction multiple, complex, data on GPUs. Specifically, the new Active-Compute-Combine (ACC) model not only provides ease of <span class="search-hit mathjax">programming</span> to programmers, but more importantly creates opportunities for system-level <span class="search-hit mathjax">optimizations</span>. To this end, SIMD-X utilizes just-in-time task management which filters out inactive vertices at runtime and intelligently maps various tasks to different amount of GPU cores in pursuit of workload balancing. In addition, SIMD-X leverages push-pull based kernel fusion that, with the help of a new deadlock-free global barrier, <span class="search-hit mathjax">reduces</span> a large number of computation kernels to very few. Using SIMD-X, a user can <span class="search-hit mathjax">program</span> a graph algorithm in tens of lines of <span class="search-hit mathjax">code</span>, while achieving 3?, 6?, 24?, 3? speedup over Gunrock, Galois, CuSha, and Ligra, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.04070v1-abstract-full').style.display = 'none'; document.getElementById('1812.04070v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 December, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.11811">arXiv:1811.11811</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.11811">pdf</a>, <a href="https://arxiv.org/format/1811.11811">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An <span class="search-hit mathjax">Application</span> of Storage-<span class="search-hit mathjax">Optimal</span> MatDot <span class="search-hit mathjax">Codes</span> for <span class="search-hit mathjax">Coded</span> Matrix Multiplication: Fast k-Nearest Neighbors Estimation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sheth%2C+U">Utsav Sheth</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dutta%2C+S">Sanghamitra Dutta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chaudhari%2C+M">Malhar Chaudhari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jeong%2C+H">Haewon Jeong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yaoqing Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kohonen%2C+J">Jukka Kohonen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roos%2C+T">Teemu Roos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Grover%2C+P">Pulkit Grover</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.11811v1-abstract-short" style="display: inline;">
        We propose a novel <span class="search-hit mathjax">application</span> of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.11811v1-abstract-full').style.display = 'inline'; document.getElementById('1811.11811v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.11811v1-abstract-full" style="display: none;">
        We propose a novel <span class="search-hit mathjax">application</span> of <span class="search-hit mathjax">coded</span> computing to the problem of the nearest neighbor estimation using MatDot <span class="search-hit mathjax">Codes</span> [Fahim. et.al. 2017], that are known to be <span class="search-hit mathjax">optimal</span> for matrix multiplication in terms of recovery threshold under storage constraints. In approximate nearest neighbor algorithms, it is common to construct efficient in-<span class="search-hit mathjax">memory</span> indexes to <span class="search-hit mathjax">improve</span> query response time. One such strategy is Multiple Random Projection Trees (MRPT), which <span class="search-hit mathjax">reduces</span> the set of candidate points over which Euclidean distance calculations are performed. However, this may result in a high <span class="search-hit mathjax">memory</span> footprint and possibly paging penalties for large or high-dimensional data. Here we propose two techniques to parallelize MRPT, that exploit data and model parallelism respectively, by dividing both the data storage and the computation efforts among different nodes in a distributed computing cluster. This is especially critical when a single compute node cannot hold the complete dataset in <span class="search-hit mathjax">memory</span>. We also propose a novel <span class="search-hit mathjax">coded</span> computation strategy based on MatDot <span class="search-hit mathjax">codes</span> for the model-parallel architecture that, in a straggler-prone environment, achieves the storage-<span class="search-hit mathjax">optimal</span> recovery threshold, i.e., the number of nodes that are required to serve a query. We experimentally demonstrate that, in the absence of straggling, our distributed approaches require less query time than execution on a single processing node, providing near-linear speedups with respect to the number of worker nodes. Through our experiments on real systems with simulated straggling, we also show that our strategy achieves a faster query execution than the uncoded strategy in a straggler-prone environment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.11811v1-abstract-full').style.display = 'none'; document.getElementById('1811.11811v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication at the IEEE Big Data 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.09721">arXiv:1811.09721</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.09721">pdf</a>, <a href="https://arxiv.org/format/1811.09721">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Costless: <span class="search-hit mathjax">Optimizing</span> Cost of Serverless Computing through Function Fusion and Placement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elgamal%2C+T">Tarek Elgamal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sandur%2C+A">Atul Sandur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nahrstedt%2C+K">Klara Nahrstedt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agha%2C+G">Gul Agha</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.09721v1-abstract-short" style="display: inline;">
        Serverless computing has recently experienced significant adoption by several <span class="search-hit mathjax">applications</span>, especially Internet of Things (IoT)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.09721v1-abstract-full').style.display = 'inline'; document.getElementById('1811.09721v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.09721v1-abstract-full" style="display: none;">
        Serverless computing has recently experienced significant adoption by several <span class="search-hit mathjax">applications</span>, especially Internet of Things (IoT) <span class="search-hit mathjax">applications</span>. In serverless computing, rather than deploying and managing dedicated virtual machines, users are able to deploy individual functions, and pay only for the time that their <span class="search-hit mathjax">code</span> is actually executing. However, since serverless platforms are relatively new, they have a completely different pricing model that depends on the <span class="search-hit mathjax">memory</span>, duration, and the number of executions of a sequence/workflow of functions. In this paper we present an algorithm that <span class="search-hit mathjax">optimizes</span> the price of serverless <span class="search-hit mathjax">applications</span> in AWS Lambda. We first describe the factors affecting price of serverless <span class="search-hit mathjax">applications</span> which include: (1) fusing a sequence of functions, (2) splitting functions across edge and cloud resources, and (3) allocating the <span class="search-hit mathjax">memory</span> for each function. We then present an efficient algorithm to explore different function fusion-placement solutions and find the solution that <span class="search-hit mathjax">optimizes</span> the <span class="search-hit mathjax">application&#39;s</span> price while keeping the latency under a certain threshold. Our results on image processing workflows show that the algorithm can find solutions <span class="search-hit mathjax">optimizing</span> the price by more than 35%-57% with only 5%-15% increase in latency. We also show that our algorithm can find non-trivial <span class="search-hit mathjax">memory</span> configurations that <span class="search-hit mathjax">reduce</span> both latency and price.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.09721v1-abstract-full').style.display = 'none'; document.getElementById('1811.09721v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.08932">arXiv:1811.08932</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.08932">pdf</a>, <a href="https://arxiv.org/format/1811.08932">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.23919/DATE.2019.8714922">10.23919/DATE.2019.8714922 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CapsAcc: An Efficient Hardware Accelerator for CapsuleNets with Data Reuse
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Marchisio%2C+A">Alberto Marchisio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hanif%2C+M+A">Muhammad Abdullah Hanif</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shafique%2C+M">Muhammad Shafique</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.08932v1-abstract-short" style="display: inline;">
        Deep Neural Networks (DNNs) have been widely deployed for many Machine Learning <span class="search-hit mathjax">applications</span>. Recently, CapsuleNets have overtaken traditional DNNs, because of their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.08932v1-abstract-full').style.display = 'inline'; document.getElementById('1811.08932v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.08932v1-abstract-full" style="display: none;">
        Deep Neural Networks (DNNs) have been widely deployed for many Machine Learning <span class="search-hit mathjax">applications</span>. Recently, CapsuleNets have overtaken traditional DNNs, because of their <span class="search-hit mathjax">improved</span> generalization ability due to the multi-dimensional capsules, in contrast to the single-dimensional neurons. Consequently, CapsuleNets also require extremely intense matrix computations, making it a gigantic challenge to achieve high performance. In this paper, we propose CapsAcc, the first specialized CMOS-based hardware architecture to perform CapsuleNets inference with high performance and energy efficiency. State-of-the-art convolutional DNN accelerators would not work efficiently for CapsuleNets, as their designs do not account for key operations involved in CapsuleNets, like squashing and dynamic routing, as well as multi-dimensional matrix processing. Our CapsAcc architecture targets this problem and achieves significant <span class="search-hit mathjax">improvements</span>, when compared to an <span class="search-hit mathjax">optimized</span> GPU implementation. Our architecture exploits the massive parallelism by flexibly feeding the data to a specialized systolic array according to the operations required in different layers. It also avoids extensive load and store operations on the on-chip <span class="search-hit mathjax">memory</span>, by reusing the data when possible. We further <span class="search-hit mathjax">optimize</span> the routing algorithm to <span class="search-hit mathjax">reduce</span> the computations needed at this stage. We synthesized the complete CapsAcc architecture in a 32nm CMOS technology using Synopsys design tools, and evaluated it for the MNIST benchmark (as also done by the original CapsuleNet paper) to ensure consistent and fair comparisons. This work enables highly-efficient CapsuleNets inference on embedded platforms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.08932v1-abstract-full').style.display = 'none'; document.getElementById('1811.08932v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication at Design, Automation and Test in Europe (DATE 2019). Florence, Italy</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.08282">arXiv:1811.08282</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.08282">pdf</a>, <a href="https://arxiv.org/format/1811.08282">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Physics">physics.comp-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Applying the swept rule for solving explicit partial differential equations on heterogeneous computing systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Magee%2C+D+J">Daniel J. Magee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Walker%2C+A+S">Anthony S. Walker</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niemeyer%2C+K+E">Kyle E. Niemeyer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.08282v2-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Applications</span> that exploit the architectural details of high-performance computing (HPC) systems have become increasingly invaluable in academia and industry over the past two decades. The most important hardware development of the last decade in HPC has been the General Purpose Graphics Processing Unit (GPGPU), a class of massively parallel devices that now&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.08282v2-abstract-full').style.display = 'inline'; document.getElementById('1811.08282v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.08282v2-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Applications</span> that exploit the architectural details of high-performance computing (HPC) systems have become increasingly invaluable in academia and industry over the past two decades. The most important hardware development of the last decade in HPC has been the General Purpose Graphics Processing Unit (GPGPU), a class of massively parallel devices that now contributes the majority of computational power in the top 500 supercomputers. As these systems grow, small costs such as latency---due to the fixed cost of <span class="search-hit mathjax">memory</span> accesses and communication---accumulate in a large simulation and become a significant barrier to performance. The swept time-space decomposition rule is a communication-avoiding technique for time-stepping stencil update formulas that attempts to <span class="search-hit mathjax">reduce</span> latency costs. This work extends the swept rule by targeting heterogeneous, CPU/GPU architectures representing current and future HPC systems. We compare our approach to a naive decomposition scheme with two test equations using an MPI+CUDA pattern on 40 processes over two nodes containing one GPU. The swept rule produces a factor of 1.9 to 23 speedup for the heat equation and a factor of 1.1 to 2.0 speedup for the Euler equations, using the same processors and work distribution, and with the best possible configurations. These results show the potential effectiveness of the swept rule for different equations and numerical schemes on massively parallel computing systems that incur substantial latency costs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.08282v2-abstract-full').style.display = 'none'; document.getElementById('1811.08282v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 May, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 November, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">24 pages, 9 figures. Accepted for publication by the Journal of Supercomputing</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.07275">arXiv:1811.07275</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.07275">pdf</a>, <a href="https://arxiv.org/format/1811.07275">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RePr: <span class="search-hit mathjax">Improved</span> Training of Convolutional Filters
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Prakash%2C+A">Aaditya Prakash</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Storer%2C+J">James Storer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Florencio%2C+D">Dinei Florencio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Cha Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.07275v3-abstract-short" style="display: inline;">
        &hellip;by the network&#39;s filters. Innovations in network architecture such as skip/dense connections and Inception units have mitigated this problem to some extent, but these <span class="search-hit mathjax">improvements</span> come with increased computation and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.07275v3-abstract-full').style.display = 'inline'; document.getElementById('1811.07275v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.07275v3-abstract-full" style="display: none;">
        A well-trained Convolutional Neural Network can easily be pruned without significant loss of performance. This is because of unnecessary overlap in the features captured by the network&#39;s filters. Innovations in network architecture such as skip/dense connections and Inception units have mitigated this problem to some extent, but these <span class="search-hit mathjax">improvements</span> come with increased computation and <span class="search-hit mathjax">memory</span> requirements at run-time. We attempt to address this problem from another angle - not by changing the network structure but by altering the training method. We show that by temporarily pruning and then restoring a subset of the model&#39;s filters, and repeating this process cyclically, overlap in the learned features is <span class="search-hit mathjax">reduced</span>, producing <span class="search-hit mathjax">improved</span> generalization. We show that the existing model-pruning criteria are not <span class="search-hit mathjax">optimal</span> for selecting filters to prune in this context and introduce inter-filter orthogonality as the ranking criteria to determine under-expressive filters. Our method is <span class="search-hit mathjax">applicable</span> both to vanilla convolutional networks and more complex modern architectures, and <span class="search-hit mathjax">improves</span> the performance across a variety of tasks, especially when applied to smaller networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.07275v3-abstract-full').style.display = 'none'; document.getElementById('1811.07275v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 November, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.06396">arXiv:1811.06396</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.06396">pdf</a>, <a href="https://arxiv.org/format/1811.06396">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Asynchronous Stochastic Composition <span class="search-hit mathjax">Optimization</span> with Variance Reduction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+S">Shuheng Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+L">Linli Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jingchang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+J">Junliang Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ling%2C+Q">Qing Ling</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.06396v1-abstract-short" style="display: inline;">
        Composition <span class="search-hit mathjax">optimization</span> has drawn a lot of attention in a wide variety of machine learning domains from risk management to reinforcement learning. Existing methods solving the composition&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.06396v1-abstract-full').style.display = 'inline'; document.getElementById('1811.06396v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.06396v1-abstract-full" style="display: none;">
        Composition <span class="search-hit mathjax">optimization</span> has drawn a lot of attention in a wide variety of machine learning domains from risk management to reinforcement learning. Existing methods solving the composition <span class="search-hit mathjax">optimization</span> problem often work in a sequential and single-machine manner, which limits their <span class="search-hit mathjax">applications</span> in large-scale problems. To address this issue, this paper proposes two asynchronous parallel variance <span class="search-hit mathjax">reduced</span> stochastic compositional gradient (AsyVRSC) algorithms that are suitable to handle large-scale data sets. The two algorithms are AsyVRSC-Shared for the shared-<span class="search-hit mathjax">memory</span> architecture and AsyVRSC-Distributed for the master-worker architecture. The embedded variance reduction techniques enable the algorithms to achieve linear convergence rates. Furthermore, AsyVRSC-Shared and AsyVRSC-Distributed enjoy provable linear speedup, when the time delays are bounded by the data dimensionality or the sparsity ratio of the partial gradients, respectively. Extensive experiments are conducted to verify the effectiveness of the proposed algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.06396v1-abstract-full').style.display = 'none'; document.getElementById('1811.06396v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">30 pages, 19 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.05031">arXiv:1811.05031</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.05031">pdf</a>, <a href="https://arxiv.org/format/1811.05031">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation">stat.CO</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1002/WIDM.1305">10.1002/WIDM.1305 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Review of <span class="search-hit mathjax">automatic</span> differentiation and its efficient implementation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Margossian%2C+C+C">Charles C. Margossian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.05031v2-abstract-short" style="display: inline;">
        Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. <span class="search-hit mathjax">Automatic</span> differentiation is a powerful tool to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.05031v2-abstract-full').style.display = 'inline'; document.getElementById('1811.05031v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.05031v2-abstract-full" style="display: none;">
        Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. <span class="search-hit mathjax">Automatic</span> differentiation is a powerful tool to <span class="search-hit mathjax">automate</span> the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of <span class="search-hit mathjax">automatic</span> differentiation however requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to <span class="search-hit mathjax">improve</span> <span class="search-hit mathjax">applicability</span>, run time, and <span class="search-hit mathjax">memory</span> management. Among these techniques are operation overloading, region based <span class="search-hit mathjax">memory</span>, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can <span class="search-hit mathjax">reduce</span> by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open problems include the extension of current packages to provide more specialized routines, and efficient methods to perform higher-order differentiation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.05031v2-abstract-full').style.display = 'none'; document.getElementById('1811.05031v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 January, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 November, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">32 pages, 5 figures, submitted for publication. WIREs Data Mining Knowl Discov, March 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.01472">arXiv:1811.01472</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.01472">pdf</a>, <a href="https://arxiv.org/format/1811.01472">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RePair in Compressed Space and Time
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sakai%2C+K">Kensuke Sakai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ohno%2C+T">Tatsuya Ohno</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goto%2C+K">Keisuke Goto</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Takabatake%2C+Y">Yoshimasa Takabatake</a>, 
      
      <a href="/search/?searchtype=author&amp;query=I%2C+T">Tomohiro I</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sakamoto%2C+H">Hiroshi Sakamoto</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.01472v1-abstract-short" style="display: inline;">
        &hellip;methods, RePair (recursive paring) [Larsson and Moffat, 1999] is notable for achieving good compression ratios in practice. Although the original paper already achieved a time-<span class="search-hit mathjax">optimal</span> algorithm to compute the RePair grammar RePair($T$) in expected $O(N)$ time, the study to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.01472v1-abstract-full').style.display = 'inline'; document.getElementById('1811.01472v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.01472v1-abstract-full" style="display: none;">
        Given a string $T$ of length $N$, the goal of grammar compression is to construct a small context-free grammar generating only $T$. Among existing grammar compression methods, RePair (recursive paring) [Larsson and Moffat, 1999] is notable for achieving good compression ratios in practice. Although the original paper already achieved a time-<span class="search-hit mathjax">optimal</span> algorithm to compute the RePair grammar RePair($T$) in expected $O(N)$ time, the study to <span class="search-hit mathjax">reduce</span> its working space is still active so that it is <span class="search-hit mathjax">applicable</span> to large-scale data. In this paper, we propose the first RePair algorithm working in compressed space, i.e., potentially $o(N)$ space for highly compressible texts. The key idea is to give a new way to restructure an arbitrary grammar $S$ for $T$ into RePair($T$) in compressed space and time. Based on the recompression technique, we propose an algorithm for RePair($T$) in $O(\min(N, nm \log N))$ space and expected $O(\min(N, nm \log N) m)$ time or $O(\min(N, nm \log N) \log \log N)$ time, where $n$ is the size of $S$ and $m$ is the number of variables in RePair($T$). We implemented our algorithm running in $O(\min(N, nm \log N) m)$ time and show it can actually run in compressed space. We also present a new approach to <span class="search-hit mathjax">reduce</span> the peak <span class="search-hit mathjax">memory</span> usage of existing RePair algorithms combining with our algorithms, and show that the new approach outperforms, both in computation time and space, the most space efficient linear-time RePair implementation to date.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.01472v1-abstract-full').style.display = 'none'; document.getElementById('1811.01472v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.09555">arXiv:1810.09555</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.09555">pdf</a>, <a href="https://arxiv.org/format/1810.09555">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ShareJIT: JIT <span class="search-hit mathjax">Code</span> Cache Sharing across Processes and Its Practical Implementation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+X">Xiaoran Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cooper%2C+K">Keith Cooper</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brock%2C+J">Jacob Brock</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+H">Handong Ye</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.09555v1-abstract-short" style="display: inline;">
        Just-in-time (JIT) compilation coupled with <span class="search-hit mathjax">code</span> caching are widely used to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.09555v1-abstract-full').style.display = 'inline'; document.getElementById('1810.09555v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.09555v1-abstract-full" style="display: none;">
        Just-in-time (JIT) compilation coupled with <span class="search-hit mathjax">code</span> caching are widely used to <span class="search-hit mathjax">improve</span> performance in dynamic <span class="search-hit mathjax">programming</span> language implementations. These <span class="search-hit mathjax">code</span> caches, along with the associated profiling data for the hot <span class="search-hit mathjax">code</span>, however, consume significant amounts of <span class="search-hit mathjax">memory</span>. Furthermore, they incur extra JIT compilation time for their creation. On Android, the current standard JIT compiler and its <span class="search-hit mathjax">code</span> caches are not shared among processes---that is, the runtime system maintains a private <span class="search-hit mathjax">code</span> cache, and its associated data, for each runtime process. However, <span class="search-hit mathjax">applications</span> running on the same platform tend to share multiple libraries in common. Sharing cached <span class="search-hit mathjax">code</span> across multiple <span class="search-hit mathjax">applications</span> and multiple processes can lead to a reduction in <span class="search-hit mathjax">memory</span> use. It can directly <span class="search-hit mathjax">reduce</span> compile time. It can also <span class="search-hit mathjax">reduce</span> the cumulative amount of time spent interpreting <span class="search-hit mathjax">code</span>. All three of these effects can <span class="search-hit mathjax">improve</span> actual runtime performance.
  In this paper, we describe ShareJIT, a global <span class="search-hit mathjax">code</span> cache for JITs that can share <span class="search-hit mathjax">code</span> across multiple <span class="search-hit mathjax">applications</span> and multiple processes. We implemented ShareJIT in the context of the Android Runtime (ART), a widely used, state-of-the-art system. To increase sharing, our implementation constrains the amount of context that the JIT compiler can use to <span class="search-hit mathjax">optimize</span> the <span class="search-hit mathjax">code</span>. This exposes a fundamental tradeoff: increased specialization to a single process&#39; context decreases the extent to which the compiled <span class="search-hit mathjax">code</span> can be shared. In ShareJIT, we limit some <span class="search-hit mathjax">optimization</span> to increase shareability. To evaluate the ShareJIT, we tested 8 popular Android apps in a total of 30 experiments. ShareJIT <span class="search-hit mathjax">improved</span> overall performance by 9% on average, while decreasing <span class="search-hit mathjax">memory</span> consumption by 16% on average and JIT compilation time by 37% on average.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.09555v1-abstract-full').style.display = 'none'; document.getElementById('1810.09555v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 October, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">OOPSLA 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.07751">arXiv:1810.07751</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.07751">pdf</a>, <a href="https://arxiv.org/format/1810.07751">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gobieski%2C+G">Graham Gobieski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Beckmann%2C+N">Nathan Beckmann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lucia%2C+B">Brandon Lucia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.07751v2-abstract-short" style="display: inline;">
        Energy-harvesting technology provides a promising platform for future IoT <span class="search-hit mathjax">applications</span>. However, since communication is very expensive in these devices,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.07751v2-abstract-full').style.display = 'inline'; document.getElementById('1810.07751v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.07751v2-abstract-full" style="display: none;">
        Energy-harvesting technology provides a promising platform for future IoT <span class="search-hit mathjax">applications</span>. However, since communication is very expensive in these devices, <span class="search-hit mathjax">applications</span> will require inference &#34;beyond the edge&#34; to avoid wasting precious energy on pointless communication. We show that <span class="search-hit mathjax">application</span> performance is highly sensitive to inference accuracy. Unfortunately, accurate inference requires large amounts of computation and <span class="search-hit mathjax">memory</span>, and energy-harvesting systems are severely resource-constrained. Moreover, energy-harvesting systems operate intermittently, suffering frequent power failures that corrupt results and impede forward progress.
  This paper overcomes these challenges to present the first full-scale demonstration of DNN inference on an energy-harvesting system. We design and implement SONIC, an intermittence-aware <span class="search-hit mathjax">software</span> system with specialized support for DNN inference. SONIC introduces loop continuation, a new technique that dramatically <span class="search-hit mathjax">reduces</span> the cost of guaranteeing correct intermittent execution for loop-heavy <span class="search-hit mathjax">code</span> like DNN inference. To build a complete system, we further present GENESIS, a tool that <span class="search-hit mathjax">automatically</span> compresses networks to <span class="search-hit mathjax">optimally</span> balance inference accuracy and energy, and TAILS, which exploits SIMD hardware available in some microcontrollers to <span class="search-hit mathjax">improve</span> energy efficiency. Both SONIC &amp; TAILS guarantee correct intermittent execution without any hand-tuning or performance loss across different power systems. Across three neural networks on a commercially available microcontroller, SONIC &amp; TAILS <span class="search-hit mathjax">reduce</span> inference energy by 6.9x and 12.2x, respectively, over the state-of-the-art.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.07751v2-abstract-full').style.display = 'none'; document.getElementById('1810.07751v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 September, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.09395">arXiv:1809.09395</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.09395">pdf</a>, <a href="https://arxiv.org/format/1809.09395">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Case for Asymmetric Non-Volatile <span class="search-hit mathjax">Memory</span> Architecture
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+T">Teng Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Mingxing Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+K">Kang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+X">Xuehai Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yongwei Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.09395v2-abstract-short" style="display: inline;">
        The byte-addressable Non-Volatile <span class="search-hit mathjax">Memory</span> (NVM) is a promising technology since it simultaneously provides DRAM-like performance, disk-like capacity, and persistency. The current NVM deployment is symmetric, where NVM devices are directly attached to servers. Due to the higher density, NVM provides larger capacity and can be shared among servers. Unfortunatel&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.09395v2-abstract-full').style.display = 'inline'; document.getElementById('1809.09395v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.09395v2-abstract-full" style="display: none;">
        The byte-addressable Non-Volatile <span class="search-hit mathjax">Memory</span> (NVM) is a promising technology since it simultaneously provides DRAM-like performance, disk-like capacity, and persistency. The current NVM deployment is symmetric, where NVM devices are directly attached to servers. Due to the higher density, NVM provides larger capacity and can be shared among servers. Unfortunately, in the symmetric setting, the availability of NVM devices is affected by the specific machine it is attached to. High availability can be realized by replicating data to NVM on a remote machine. However, it requires full replication of data structure in local <span class="search-hit mathjax">memory</span>, limiting the size of the working set. This paper rethinks NVM deployment and makes a case for the asymmetric NVM architecture, which decouples servers from persistent data storage. In the proposed AsymNVM architecture, NVM devices (back-end nodes) can be shared by multiple servers (front-end nodes) and provide recoverable persistent data structures. The asymmetric architecture is made possible by RDMA, and follows the recent industry trend of resource disaggregation. We build AsymNVM framework based on AsymNVM architecture that implements: 1) high performance persistent data structure update; 2) NVM data management; 3) concurrency control; and 4) crash-consistency and replication. The central idea is to use operation logs to <span class="search-hit mathjax">reduce</span> the stall due to RDMA writes and enable efficient batching and caching in front-end nodes. To evaluation performance, we construct eight widely used data structures and two <span class="search-hit mathjax">applications</span> based on AsymNVM framework, and use traces of industry workloads. In a cluster with ten machines, the results show that AsymNVM achieves comparable performance to the best possible symmetric architecture while avoiding all the drawbacks with disaggregation. Compared to the baseline AsymNVM, speedup brought by the proposed <span class="search-hit mathjax">optimizations</span> is 6~22x.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.09395v2-abstract-full').style.display = 'none'; document.getElementById('1809.09395v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 January, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 September, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 Pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.08628">arXiv:1809.08628</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.08628">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OS Scheduling Algorithms for <span class="search-hit mathjax">Memory</span> Intensive Workloads in Multi-socket Multi-core servers
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Durbhakula%2C+M">Murthy Durbhakula</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.08628v1-abstract-short" style="display: inline;">
        Major chip manufacturers have all introduced multicore microprocessors. Multi-socket systems built from these processors are routinely used for running various server <span class="search-hit mathjax">applications</span>. Depending on the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.08628v1-abstract-full').style.display = 'inline'; document.getElementById('1809.08628v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.08628v1-abstract-full" style="display: none;">
        Major chip manufacturers have all introduced multicore microprocessors. Multi-socket systems built from these processors are routinely used for running various server <span class="search-hit mathjax">applications</span>. Depending on the <span class="search-hit mathjax">application</span> that is run on the system, remote <span class="search-hit mathjax">memory</span> accesses can impact overall performance. This paper presents a new operating system (OS) scheduling <span class="search-hit mathjax">optimization</span> to <span class="search-hit mathjax">reduce</span> the impact of such remote <span class="search-hit mathjax">memory</span> accesses. By observing the pattern of local and remote DRAM accesses for every thread in each scheduling quantum and applying different algorithms, we come up with a new schedule of threads for the next quantum. This new schedule potentially cuts down remote DRAM accesses for the next scheduling quantum and <span class="search-hit mathjax">improves</span> overall performance. We present three such new algorithms of varying complexity followed by an algorithm which is an adaptation of Hungarian algorithm. We used three different synthetic workloads to evaluate the algorithm. We also performed sensitivity analysis with respect to varying DRAM latency. We show that these algorithms can cut down DRAM access latency by up to 55% depending on the algorithm used. The benefit gained from the algorithms is dependent upon their complexity. In general higher the complexity higher is the benefit. Hungarian algorithm results in an <span class="search-hit mathjax">optimal</span> solution. We find that two out of four algorithms provide a good trade-off between performance and complexity for the workloads we studied.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.08628v1-abstract-full').style.display = 'none'; document.getElementById('1809.08628v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.07599">arXiv:1809.07599</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.07599">pdf</a>, <a href="https://arxiv.org/format/1809.07599">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sparsified SGD with <span class="search-hit mathjax">Memory</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Stich%2C+S+U">Sebastian U. Stich</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cordonnier%2C+J">Jean-Baptiste Cordonnier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jaggi%2C+M">Martin Jaggi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.07599v2-abstract-short" style="display: inline;">
        Huge scale machine learning problems are nowadays tackled by distributed <span class="search-hit mathjax">optimization</span> algorithms, i.e. algorithms that leverage the compute power of many devices for training. The communication overhead is a key bottleneck that hinders perfect scalability. Various recent works proposed to use quantization or sparsification techniques to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.07599v2-abstract-full').style.display = 'inline'; document.getElementById('1809.07599v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.07599v2-abstract-full" style="display: none;">
        Huge scale machine learning problems are nowadays tackled by distributed <span class="search-hit mathjax">optimization</span> algorithms, i.e. algorithms that leverage the compute power of many devices for training. The communication overhead is a key bottleneck that hinders perfect scalability. Various recent works proposed to use quantization or sparsification techniques to <span class="search-hit mathjax">reduce</span> the amount of data that needs to be communicated, for instance by only sending the most significant entries of the stochastic gradient (top-k sparsification). Whilst such schemes showed very promising performance in practice, they have eluded theoretical analysis so far.
  In this work we analyze Stochastic Gradient Descent (SGD) with k-sparsification or compression (for instance top-k or random-k) and show that this scheme converges at the same rate as vanilla SGD when equipped with error compensation (keeping track of accumulated errors in <span class="search-hit mathjax">memory</span>). That is, communication can be <span class="search-hit mathjax">reduced</span> by a factor of the dimension of the problem (sometimes even more) whilst still converging at the same rate. We present numerical experiments to illustrate the theoretical findings and the better scalability for distributed <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.07599v2-abstract-full').style.display = 'none'; document.getElementById('1809.07599v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 November, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 September, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">to appear at NIPS 2018</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68W40; 68W15; 90C25; 90C06
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          G.1.6; F.2.1; E.4
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.04570">arXiv:1809.04570</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.04570">pdf</a>, <a href="https://arxiv.org/format/1809.04570">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Blott%2C+M">Michaela Blott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Preusser%2C+T">Thomas Preusser</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fraser%2C+N">Nicholas Fraser</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gambardella%2C+G">Giulio Gambardella</a>, 
      
      <a href="/search/?searchtype=author&amp;query=O%27Brien%2C+K">Kenneth O&#39;Brien</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Umuroglu%2C+Y">Yaman Umuroglu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.04570v1-abstract-short" style="display: inline;">
        &hellip;algorithm, enabling ubiquitous machine vision and intelligent decisions on even embedded computing-systems. While the underlying arithmetic is structurally simple, compute and <span class="search-hit mathjax">memory</span> requirements are challenging. One of the promising opportunities is leveraging&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.04570v1-abstract-full').style.display = 'inline'; document.getElementById('1809.04570v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.04570v1-abstract-full" style="display: none;">
        Convolutional Neural Networks have rapidly become the most successful machine learning algorithm, enabling ubiquitous machine vision and intelligent decisions on even embedded computing-systems. While the underlying arithmetic is structurally simple, compute and <span class="search-hit mathjax">memory</span> requirements are challenging. One of the promising opportunities is leveraging <span class="search-hit mathjax">reduced</span>-precision representations for inputs, activations and model parameters. The resulting scalability in performance, power efficiency and storage footprint provides interesting design compromises in exchange for a small reduction in accuracy. FPGAs are ideal for exploiting low-precision inference engines leveraging custom precisions to achieve the required numerical accuracy for a given <span class="search-hit mathjax">application</span>. In this article, we describe the second generation of the FINN framework, an end-to-end tool which enables design space exploration and <span class="search-hit mathjax">automates</span> the creation of fully customized inference engines on FPGAs. Given a neural network description, the tool <span class="search-hit mathjax">optimizes</span> for given platforms, design targets and a specific precision. We introduce formalizations of resource cost functions and performance predictions, and elaborate on the <span class="search-hit mathjax">optimization</span> algorithms. Finally, we evaluate a selection of <span class="search-hit mathjax">reduced</span> precision neural networks ranging from CIFAR-10 classifiers to YOLO-based object detection on a range of platforms including PYNQ and AWS\,F1, demonstrating new unprecedented measured throughput at 50TOp/s on AWS-F1 and 5TOp/s on embedded devices.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.04570v1-abstract-full').style.display = 'none'; document.getElementById('1809.04570v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">to be published in ACM TRETS Special Edition on Deep Learning</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.04256">arXiv:1808.04256</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.04256">pdf</a>, <a href="https://arxiv.org/format/1808.04256">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TMI.2019.2922960">10.1109/TMI.2019.2922960 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CT Super-resolution GAN Constrained by the Identical, Residual, and Cycle Learning Ensemble(GAN-CIRCLE)
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=You%2C+C">Chenyu You</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+G">Guang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiaoliu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shan%2C+H">Hongming Shan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ju%2C+S">Shenghong Ju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Z">Zhen Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhuiyang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cong%2C+W">Wenxiang Cong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vannier%2C+M+W">Michael W. Vannier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saha%2C+P+K">Punam K. Saha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+G">Ge Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.04256v3-abstract-short" style="display: inline;">
        &hellip;ionizing radiation, an overarching thrust of related technical research is development of novel methods enabling ultrahigh quality imaging with fine structural details while <span class="search-hit mathjax">reducing</span> the X-ray radiation. In this paper, we present a semi-supervised deep learning approach to accurately recover high-resolution (HR) CT images from low-resolution (LR) counterpart&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.04256v3-abstract-full').style.display = 'inline'; document.getElementById('1808.04256v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.04256v3-abstract-full" style="display: none;">
        Computed tomography (CT) is widely used in screening, diagnosis, and image-guided therapy for both clinical and research purposes. Since CT involves ionizing radiation, an overarching thrust of related technical research is development of novel methods enabling ultrahigh quality imaging with fine structural details while <span class="search-hit mathjax">reducing</span> the X-ray radiation. In this paper, we present a semi-supervised deep learning approach to accurately recover high-resolution (HR) CT images from low-resolution (LR) counterparts. Specifically, with the generative adversarial network (GAN) as the building block, we enforce the cycle-consistency in terms of the Wasserstein distance to establish a nonlinear end-to-end mapping from noisy LR input images to denoised and deblurred HR outputs. We also include the joint constraints in the loss function to facilitate structural preservation. In this deep imaging process, we incorporate deep convolutional neural network (CNN), residual learning, and network in network techniques for feature extraction and restoration. In contrast to the current trend of increasing network depth and complexity to boost the CT imaging performance, which limit its real-world <span class="search-hit mathjax">applications</span> by imposing considerable computational and <span class="search-hit mathjax">memory</span> overheads, we apply a parallel $1\times1$ CNN to compress the output of the hidden layer and <span class="search-hit mathjax">optimize</span> the number of layers and the number of filters for each convolutional layer. Quantitative and qualitative evaluations demonstrate that our proposed model is accurate, efficient and robust for super-resolution (SR) image restoration from noisy LR input images. In particular, we validate our composite SR networks on three large-scale CT datasets, and obtain promising results as compared to the other state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.04256v3-abstract-full').style.display = 'none'; document.getElementById('1808.04256v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 August, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          TMI-2019-0250
        

        

        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Transactions on Medical Imaging 2019
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.03843">arXiv:1808.03843</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.03843">pdf</a>, <a href="https://arxiv.org/format/1808.03843">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Matrix Factorization on GPUs with <span class="search-hit mathjax">Memory</span> <span class="search-hit mathjax">Optimization</span> and Approximate Computing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+W">Wei Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+S">Shiyu Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fong%2C+L">Liana Fong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Cheng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zijun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+L">Liangliang Cao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.03843v1-abstract-short" style="display: inline;">
        &hellip;observations, which has shown great promises in the fields of collaborative filtering, data compression, feature extraction, word embedding, etc. While many problem-specific <span class="search-hit mathjax">optimization</span> techniques have been proposed, alternating least square (ALS) remains popular due to its general&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.03843v1-abstract-full').style.display = 'inline'; document.getElementById('1808.03843v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.03843v1-abstract-full" style="display: none;">
        Matrix factorization (MF) discovers latent features from observations, which has shown great promises in the fields of collaborative filtering, data compression, feature extraction, word embedding, etc. While many problem-specific <span class="search-hit mathjax">optimization</span> techniques have been proposed, alternating least square (ALS) remains popular due to its general <span class="search-hit mathjax">applicability</span> e.g. easy to handle positive-unlabeled inputs, fast convergence and parallelization capability. Current MF implementations are either <span class="search-hit mathjax">optimized</span> for a single machine or with a need of a large computer cluster but still are insufficient. This is because a single machine provides limited compute power for large-scale data while multiple machines suffer from the network communication bottleneck.
  To address the aforementioned challenge, accelerating ALS on graphics processing units (GPUs) is a promising direction. We propose the novel approach in enhancing the MF efficiency via both <span class="search-hit mathjax">memory</span> <span class="search-hit mathjax">optimization</span> and approximate computing. The former exploits GPU <span class="search-hit mathjax">memory</span> hierarchy to increase data reuse, while the later <span class="search-hit mathjax">reduces</span> unnecessary computing without hurting the convergence of learning algorithms. Extensive experiments on large-scale datasets show that our solution not only outperforms the competing CPU solutions by a large margin but also has a 2x-4x performance gain compared to the state-of-the-art GPU solutions. Our implementations are open-sourced and publicly available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.03843v1-abstract-full').style.display = 'none'; document.getElementById('1808.03843v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 August, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.06417">arXiv:1807.06417</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.06417">pdf</a>, <a href="https://arxiv.org/format/1807.06417">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tiered Object Storage using Persistent <span class="search-hit mathjax">Memory</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=George%2C+J">Johnu George</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pydipaty%2C+R">Ramdoot Pydipaty</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+X">Xinyuan Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saha%2C+A">Amit Saha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dutta%2C+D">Debo Dutta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+G">Gary Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gangumalla%2C+U">Uma Gangumalla</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.06417v1-abstract-short" style="display: inline;">
        Most data intensive <span class="search-hit mathjax">applications</span> often access only a few fields of the objects they are operating on. Since NVM provides fast, byte-addressable access to durable&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.06417v1-abstract-full').style.display = 'inline'; document.getElementById('1807.06417v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.06417v1-abstract-full" style="display: none;">
        Most data intensive <span class="search-hit mathjax">applications</span> often access only a few fields of the objects they are operating on. Since NVM provides fast, byte-addressable access to durable <span class="search-hit mathjax">memory</span>, it is possible to access various fields of an object stored in NVM directly without incurring any serialization and deserialization cost. This paper proposes a novel tiered object storage model that modifies a data structure such that only a chosen subset of fields of the data structure are stored in NVM, while the remaining fields are stored in a cheaper (and a traditional) storage layer such as HDDs/SSDs. We introduce a novel linear-<span class="search-hit mathjax">programming</span> based <span class="search-hit mathjax">optimization</span> framework for deciding the field placement. Our proof of concept demonstrates that a tiered object storage model <span class="search-hit mathjax">improves</span> the execution time of standard operations by up to 50\% by avoiding the cost of serialization/deserialization and by <span class="search-hit mathjax">reducing</span> the <span class="search-hit mathjax">memory</span> footprint of operations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.06417v1-abstract-full').style.display = 'none'; document.getElementById('1807.06417v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 July, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.03562">arXiv:1807.03562</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.03562">pdf</a>, <a href="https://arxiv.org/format/1807.03562">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DXRAM&#39;s Fault-Tolerance Mechanisms Meet High Speed I/O Devices
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Beineke%2C+K">Kevin Beineke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nothaas%2C+S">Stefan Nothaas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schoettner%2C+M">Michael Schoettner</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.03562v2-abstract-short" style="display: inline;">
        In-<span class="search-hit mathjax">memory</span> key-value stores provide consistent low-latency access to all objects which is important for interactive large-scale&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.03562v2-abstract-full').style.display = 'inline'; document.getElementById('1807.03562v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.03562v2-abstract-full" style="display: none;">
        In-<span class="search-hit mathjax">memory</span> key-value stores provide consistent low-latency access to all objects which is important for interactive large-scale <span class="search-hit mathjax">applications</span> like social media networks or online graph analytics and also opens up new <span class="search-hit mathjax">application</span> areas. But, when storing the data in RAM on thousands of servers one has to consider server failures. Only a few in-<span class="search-hit mathjax">memory</span> key-value stores provide <span class="search-hit mathjax">automatic</span> online recovery of failed servers. The most prominent example of these systems is RAMCloud. Another system with sophisticated fault-tolerance mechanisms is DXRAM which is <span class="search-hit mathjax">optimized</span> for small data objects. In this report, we detail the remote replication process which is based on logs, investigate selection strategies for the reorganization of these logs and evaluate the reorganization performance for sequential, random, zipf and hot-and-cold distributions in DXRAM. This is also the first time DXRAM&#39;s backup system is evaluated with high speed I/O devices, specifically with 56 GBit/s InfiniBand interconnect and PCI-e SSDs. Furthermore, we discuss the copyset replica distribution to <span class="search-hit mathjax">reduce</span> the probability for data loss and the adaptations to the original approach for DXRAM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.03562v2-abstract-full').style.display = 'none'; document.getElementById('1807.03562v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 July, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">21 pages, 20 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.02322">arXiv:1807.02322</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.02322">pdf</a>, <a href="https://arxiv.org/format/1807.02322">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Memory</span> Augmented Policy <span class="search-hit mathjax">Optimization</span> for <span class="search-hit mathjax">Program</span> Synthesis and Semantic Parsing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+C">Chen Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Norouzi%2C+M">Mohammad Norouzi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berant%2C+J">Jonathan Berant</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Le%2C+Q">Quoc Le</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lao%2C+N">Ni Lao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.02322v5-abstract-short" style="display: inline;">
        We present <span class="search-hit mathjax">Memory</span> Augmented Policy&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.02322v5-abstract-full').style.display = 'inline'; document.getElementById('1807.02322v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.02322v5-abstract-full" style="display: none;">
        We present <span class="search-hit mathjax">Memory</span> Augmented Policy <span class="search-hit mathjax">Optimization</span> (MAPO), a simple and novel way to leverage a <span class="search-hit mathjax">memory</span> buffer of promising trajectories to <span class="search-hit mathjax">reduce</span> the variance of policy gradient estimate. MAPO is <span class="search-hit mathjax">applicable</span> to deterministic environments with discrete actions, such as structured prediction and combinatorial <span class="search-hit mathjax">optimization</span> tasks. We express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside the <span class="search-hit mathjax">memory</span> buffer, and a separate expectation over trajectories outside the buffer. To make an efficient algorithm of MAPO, we propose: (1) <span class="search-hit mathjax">memory</span> weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the <span class="search-hit mathjax">memory</span> buffer to scale up training. MAPO <span class="search-hit mathjax">improves</span> the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards. We evaluate MAPO on weakly supervised <span class="search-hit mathjax">program</span> synthesis from natural language (semantic parsing). On the WikiTableQuestions benchmark, we <span class="search-hit mathjax">improve</span> the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision. Our source <span class="search-hit mathjax">code</span> is available at https://github.com/crazydonkey200/neural-symbolic-machines
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.02322v5-abstract-full').style.display = 'none'; document.getElementById('1807.02322v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 July, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 Pages, 4 figures, 7 tables, accepted as a spotlight paper for NeurIPS 2018, camera ready version, fixed a typo in table 4</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1805.00923">arXiv:1805.00923</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1805.00923">pdf</a>, <a href="https://arxiv.org/format/1805.00923">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GraphIt: A High-Performance DSL for Graph Analytics
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yunming Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+M">Mengjiao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baghdadi%2C+R">Riyadh Baghdadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kamil%2C+S">Shoaib Kamil</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shun%2C+J">Julian Shun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amarasinghe%2C+S">Saman Amarasinghe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1805.00923v2-abstract-short" style="display: inline;">
        The performance bottlenecks of graph <span class="search-hit mathjax">applications</span> depend not only on the algorithm and the underlying hardware, but also on the size and structure of the input graph. Programmers must try different combinations of a large set of techniques to develop the best implementation for a specific algorithm and type of graph. Existing graph frameworks lack flexibilit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.00923v2-abstract-full').style.display = 'inline'; document.getElementById('1805.00923v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1805.00923v2-abstract-full" style="display: none;">
        The performance bottlenecks of graph <span class="search-hit mathjax">applications</span> depend not only on the algorithm and the underlying hardware, but also on the size and structure of the input graph. Programmers must try different combinations of a large set of techniques to develop the best implementation for a specific algorithm and type of graph. Existing graph frameworks lack flexibility, supporting only a limited set of <span class="search-hit mathjax">optimizations</span>.
  This paper introduces GraphIt, a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics running on graphs with different sizes and structures. GraphIt separates what is computed (algorithm) from how it is computed (schedule). Programmers specify the algorithm using an algorithm language, and performance <span class="search-hit mathjax">optimizations</span> are specified using a scheduling language. The algorithm language simplifies expressing the algorithms. We formulate graph <span class="search-hit mathjax">optimizations</span>, including edge traversal direction, data layout, parallelization, cache, NUMA, and kernel fusion <span class="search-hit mathjax">optimizations</span>, as tradeoffs among locality, parallelism, and work-efficiency. The scheduling language enables programmers to easily search through this complicated tradeoff space by composing together <span class="search-hit mathjax">optimizations</span>. We also built an autotuner to <span class="search-hit mathjax">automatically</span> find high-performance schedules. The compiler uses a new scheduling representation, the graph iteration space, to model, compose, and ensure the validity of the large number of <span class="search-hit mathjax">optimizations</span>. GraphIt outperforms the next fastest of six state-of-the-art shared-<span class="search-hit mathjax">memory</span> frameworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24 out of 32 experiments by up to 4.8$\times$, and is never more than 43% slower than the fastest framework on the other experiments. GraphIt also <span class="search-hit mathjax">reduces</span> the lines of <span class="search-hit mathjax">code</span> by up to an order of magnitude compared to the next fastest framework.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.00923v2-abstract-full').style.display = 'none'; document.getElementById('1805.00923v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 October, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 May, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Paper Accepted at OOPSLA 2018</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        OOPSLA 2018
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1805.00280">arXiv:1805.00280</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1805.00280">pdf</a>, <a href="https://arxiv.org/format/1805.00280">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Graph Computation for Node2Vec
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+D">Dongyan Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+S">Songjie Niu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Shimin Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1805.00280v1-abstract-short" style="display: inline;">
        &hellip;method for network analysis. However, current solutions cannot run Node2Vec on large-scale graphs with billions of vertices and edges, which are common in real-world <span class="search-hit mathjax">applications</span>. The existing distributed Node2Vec on Spark incurs significant space and time overhead. It runs out of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.00280v1-abstract-full').style.display = 'inline'; document.getElementById('1805.00280v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1805.00280v1-abstract-full" style="display: none;">
        Node2Vec is a state-of-the-art general-purpose feature learning method for network analysis. However, current solutions cannot run Node2Vec on large-scale graphs with billions of vertices and edges, which are common in real-world <span class="search-hit mathjax">applications</span>. The existing distributed Node2Vec on Spark incurs significant space and time overhead. It runs out of <span class="search-hit mathjax">memory</span> even for mid-sized graphs with millions of vertices. Moreover, it considers at most 30 edges for every vertex in generating random walks, causing poor result quality. In this paper, we propose Fast-Node2Vec, a family of efficient Node2Vec random walk algorithms on a Pregel-like graph computation framework. Fast-Node2Vec computes transition probabilities during random walks to <span class="search-hit mathjax">reduce</span> <span class="search-hit mathjax">memory</span> space consumption and computation overhead for large-scale graphs. The Pregel-like scheme avoids space and time overhead of Spark&#39;s read-only RDD structures and shuffle operations. Moreover, we propose a number of <span class="search-hit mathjax">optimization</span> techniques to further <span class="search-hit mathjax">reduce</span> the computation overhead for popular vertices with large degrees. Empirical evaluation show that Fast-Node2Vec is capable of computing Node2Vec on graphs with billions of vertices and edges on a mid-sized machine cluster. Compared to Spark-Node2Vec, Fast-Node2Vec achieves 7.7--122x speedups.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.00280v1-abstract-full').style.display = 'none'; document.getElementById('1805.00280v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 May, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.11265">arXiv:1804.11265</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.11265">pdf</a>, <a href="https://arxiv.org/format/1804.11265">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mosaic: An <span class="search-hit mathjax">Application</span>-Transparent Hardware-<span class="search-hit mathjax">Software</span> Cooperative <span class="search-hit mathjax">Memory</span> Manager for GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ausavarungnirun%2C+R">Rachata Ausavarungnirun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Landgraf%2C+J">Joshua Landgraf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miller%2C+V">Vance Miller</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghose%2C+S">Saugata Ghose</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gandhi%2C+J">Jayneel Gandhi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rossbach%2C+C+J">Christopher J. Rossbach</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mutlu%2C+O">Onur Mutlu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.11265v1-abstract-short" style="display: inline;">
        Modern GPUs face a trade-off on how the page size used for <span class="search-hit mathjax">memory</span> management affects address translation and demand paging. Support for multiple page sizes can help relax the page size trade-off so that address translation and demand paging&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.11265v1-abstract-full').style.display = 'inline'; document.getElementById('1804.11265v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.11265v1-abstract-full" style="display: none;">
        Modern GPUs face a trade-off on how the page size used for <span class="search-hit mathjax">memory</span> management affects address translation and demand paging. Support for multiple page sizes can help relax the page size trade-off so that address translation and demand paging <span class="search-hit mathjax">optimizations</span> work together synergistically. However, existing page coalescing and splintering policies require costly base page migrations that undermine the benefits multiple page sizes provide. In this paper, we observe that GPGPU <span class="search-hit mathjax">applications</span> present an opportunity to support multiple page sizes without costly data migration, as the <span class="search-hit mathjax">applications</span> perform most of their <span class="search-hit mathjax">memory</span> allocation en masse (i.e., they allocate a large number of base pages at once). We show that this en masse allocation allows us to create intelligent <span class="search-hit mathjax">memory</span> allocation policies which ensure that base pages that are contiguous in virtual <span class="search-hit mathjax">memory</span> are allocated to contiguous physical <span class="search-hit mathjax">memory</span> pages. As a result, coalescing and splintering operations no longer need to migrate base pages.
  We introduce Mosaic, a GPU <span class="search-hit mathjax">memory</span> manager that provides <span class="search-hit mathjax">application</span>-transparent support for multiple page sizes. Mosaic uses base pages to transfer data over the system I/O bus, and allocates physical <span class="search-hit mathjax">memory</span> in a way that (1) preserves base page contiguity and (2) ensures that a large page frame contains pages from only a single <span class="search-hit mathjax">memory</span> protection domain. This mechanism allows the TLB to use large pages, <span class="search-hit mathjax">reducing</span> address translation overhead. During data transfer, this mechanism enables the GPU to transfer only the base pages that are needed by the <span class="search-hit mathjax">application</span> over the system I/O bus, keeping demand paging overhead low.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.11265v1-abstract-full').style.display = 'none'; document.getElementById('1804.11265v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 April, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.10541">arXiv:1804.10541</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.10541">pdf</a>, <a href="https://arxiv.org/format/1804.10541">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A matrix-free approach to parallel and <span class="search-hit mathjax">memory</span>-efficient deformable image registration
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nig%2C+L">Lars KÃ¶nig</a>, 
      
      <a href="/search/?searchtype=author&amp;query=R%C3%BChaak%2C+J">Jan RÃ¼haak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Derksen%2C+A">Alexander Derksen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lellmann%2C+J">Jan Lellmann</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.10541v1-abstract-short" style="display: inline;">
        We present a novel computational approach to fast and <span class="search-hit mathjax">memory</span>-efficient deformable image registration. In the variational registration model, the computation of the objective function derivatives is the computationally most expensive operation, both in terms of runtime and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.10541v1-abstract-full').style.display = 'inline'; document.getElementById('1804.10541v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.10541v1-abstract-full" style="display: none;">
        We present a novel computational approach to fast and <span class="search-hit mathjax">memory</span>-efficient deformable image registration. In the variational registration model, the computation of the objective function derivatives is the computationally most expensive operation, both in terms of runtime and <span class="search-hit mathjax">memory</span> requirements. In order to target this bottleneck, we analyze the matrix structure of gradient and Hessian computations for the case of the normalized gradient fields distance measure and curvature regularization. Based on this analysis, we derive equivalent matrix-free closed-form expressions for derivative computations, eliminating the need for storing intermediate results and the costs of sparse matrix arithmetic. This has further benefits: (1) matrix computations can be fully parallelized, (2) <span class="search-hit mathjax">memory</span> complexity for derivative computation is <span class="search-hit mathjax">reduced</span> from linear to constant, and (3) overall computation times are substantially <span class="search-hit mathjax">reduced</span>.
  In comparison with an <span class="search-hit mathjax">optimized</span> matrix-based reference implementation, the CPU implementation achieves speedup factors between 3.1 and 9.7, and we are able to handle substantially higher resolutions. Using a GPU implementation, we achieve an additional speedup factor of up to 9.2.
  Furthermore, we evaluated the approach on real-world medical datasets. On ten publicly available lung CT images from the DIR-Lab 4DCT dataset, we achieve the best mean landmark error of 0.93 mm compared to other submissions on the DIR-Lab website, with an average runtime of only 9.23 s. Complete non-rigid registration of full-size 3D thorax-abdomen CT volumes from oncological follow-up is achieved in 12.6 s. The experimental results show that the proposed matrix-free algorithm enables the use of variational registration models also in <span class="search-hit mathjax">applications</span> which were previously impractical due to <span class="search-hit mathjax">memory</span> or runtime restrictions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.10541v1-abstract-full').style.display = 'none'; document.getElementById('1804.10541v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 April, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication in SIAM Journal on Scientific Computing (SISC)</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          92C55; 65K10; 65Y05
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.10001">arXiv:1804.10001</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.10001">pdf</a>, <a href="https://arxiv.org/format/1804.10001">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Profile-guided <span class="search-hit mathjax">memory</span> <span class="search-hit mathjax">optimization</span> for deep neural networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sekiyama%2C+T">Taro Sekiyama</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Imamichi%2C+T">Takashi Imamichi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Imai%2C+H">Haruki Imai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raymond%2C+R">Rudy Raymond</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.10001v1-abstract-short" style="display: inline;">
        Recent years have seen deep neural networks (DNNs) becoming wider and deeper to achieve better performance in many <span class="search-hit mathjax">applications</span> of AI. Such DNNs however require huge amounts of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.10001v1-abstract-full').style.display = 'inline'; document.getElementById('1804.10001v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.10001v1-abstract-full" style="display: none;">
        Recent years have seen deep neural networks (DNNs) becoming wider and deeper to achieve better performance in many <span class="search-hit mathjax">applications</span> of AI. Such DNNs however require huge amounts of <span class="search-hit mathjax">memory</span> to store weights and intermediate results (e.g., activations, feature maps, etc.) in propagation. This requirement makes it difficult to run the DNNs on devices with limited, hard-to-extend <span class="search-hit mathjax">memory</span>, degrades the running time performance, and restricts the design of network models. We address this challenge by developing a novel profile-guided <span class="search-hit mathjax">memory</span> <span class="search-hit mathjax">optimization</span> to efficiently and quickly allocate <span class="search-hit mathjax">memory</span> blocks during the propagation in DNNs. The <span class="search-hit mathjax">optimization</span> utilizes a simple and fast heuristic algorithm based on the two-dimensional rectangle packing problem. Experimenting with well-known neural network models, we confirm that our method not only <span class="search-hit mathjax">reduces</span> the <span class="search-hit mathjax">memory</span> consumption by up to $49.5\%$ but also accelerates training and inference by up to a factor of four thanks to the rapidity of the <span class="search-hit mathjax">memory</span> allocation and the ability to use larger mini-batch sizes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.10001v1-abstract-full').style.display = 'none'; document.getElementById('1804.10001v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 April, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 9 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.08378">arXiv:1804.08378</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.08378">pdf</a>, <a href="https://arxiv.org/format/1804.08378">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BrainSlug: Transparent Acceleration of Deep Learning Through Depth-First Parallelism
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Weber%2C+N">Nicolas Weber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schmidt%2C+F">Florian Schmidt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niepert%2C+M">Mathias Niepert</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huici%2C+F">Felipe Huici</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.08378v1-abstract-short" style="display: inline;">
        Neural network frameworks such as PyTorch and TensorFlow are the workhorses of numerous machine learning <span class="search-hit mathjax">applications</span> ranging from object recognition to machine translation. While these frameworks are versatile and straightforward to use, the training of and inference in deep neural networks is resource (energy, compute, and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.08378v1-abstract-full').style.display = 'inline'; document.getElementById('1804.08378v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.08378v1-abstract-full" style="display: none;">
        Neural network frameworks such as PyTorch and TensorFlow are the workhorses of numerous machine learning <span class="search-hit mathjax">applications</span> ranging from object recognition to machine translation. While these frameworks are versatile and straightforward to use, the training of and inference in deep neural networks is resource (energy, compute, and <span class="search-hit mathjax">memory</span>) intensive. In contrast to recent works focusing on algorithmic enhancements, we introduce BrainSlug, a framework that transparently accelerates neural network workloads by changing the default layer-by-layer processing to a depth-first approach, <span class="search-hit mathjax">reducing</span> the amount of data required by the computations and thus <span class="search-hit mathjax">improving</span> the performance of the available hardware caches. BrainSlug achieves performance <span class="search-hit mathjax">improvements</span> of up to 41.1% on CPUs and 35.7% on GPUs. These <span class="search-hit mathjax">optimizations</span> come at zero cost to the user as they do not require hardware changes and only need tiny adjustments to the <span class="search-hit mathjax">software</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.08378v1-abstract-full').style.display = 'none'; document.getElementById('1804.08378v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Technical Report, 13 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.04612">arXiv:1804.04612</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.04612">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Comprehensive Study on the <span class="search-hit mathjax">Applications</span> of Machine Learning for the Medical Diagnosis and Prognosis of Asthma
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kukreja%2C+S">Saksham Kukreja</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.04612v1-abstract-short" style="display: inline;">
        &hellip;almost all deaths are avoidable. Most of these deaths occur because the patients are unaware of their asthmatic morbidity. If detected early, asthmatic mortality rate can be <span class="search-hit mathjax">reduced</span> by 78%, provided that the patients carry appropriate medication for the same and/or are in lose vicinity to medical equipment like nebulizers. This study focuses on the developm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.04612v1-abstract-full').style.display = 'inline'; document.getElementById('1804.04612v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.04612v1-abstract-full" style="display: none;">
        An estimated 300 million people worldwide suffer from asthma, and this number is expected to increase to 400 million by 2025. Approximately 250,000 people die prematurely each year from asthma out of which, almost all deaths are avoidable. Most of these deaths occur because the patients are unaware of their asthmatic morbidity. If detected early, asthmatic mortality rate can be <span class="search-hit mathjax">reduced</span> by 78%, provided that the patients carry appropriate medication for the same and/or are in lose vicinity to medical equipment like nebulizers. This study focuses on the development and valuation of algorithms to diagnose asthma through symptom intensive questionary, clinical data and medical reports. Machine Learning Algorithms like Back-propagation model, Context Sensitive Auto-Associative <span class="search-hit mathjax">Memory</span> Neural Network Model, C4.5 Algorithm, Bayesian Network and Particle Swarm <span class="search-hit mathjax">Optimization</span> have been employed for the diagnosis of asthma and later a comparison is made between their respective prospects. All algorithms received an accuracy of over 80%. However, the use of Auto Associative <span class="search-hit mathjax">Memory</span> Model (on a layered Artificial Neural Network) displayed much better results. It reached to an accuracy of over 90% and an inconclusive diagnosis rate of less than 1% when trained with adequate data. In the end, naÃ¯ve mobile based <span class="search-hit mathjax">applications</span> were developed on Android and iOS that made use of the self-training auto associative <span class="search-hit mathjax">memory</span> model to achieve an accuracy of nearly 94.2%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.04612v1-abstract-full').style.display = 'none'; document.getElementById('1804.04612v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">27 pages. arXiv admin note: text overlap with arXiv:1505.01345 by other authors without attribution</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1803.09948">arXiv:1803.09948</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1803.09948">pdf</a>, <a href="https://arxiv.org/format/1803.09948">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Extreme Scale FMM-Accelerated Boundary Integral Equation Solver for Wave Scattering
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abduljabbar%2C+M">Mustafa Abduljabbar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Farhan%2C+M+A">Mohammed Al Farhan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Al-Harthi%2C+N">Noha Al-Harthi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Rui Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yokota%2C+R">Rio Yokota</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bagci%2C+H">Hakan Bagci</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keyes%2C+D">David Keyes</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1803.09948v1-abstract-short" style="display: inline;">
        Algorithmic and architecture-oriented <span class="search-hit mathjax">optimizations</span> are essential for achieving performance worthy of anticipated energy-austere exascale systems. In this paper, we present an extreme scale FMM-accelerated boundary integral equation solver for wave scattering, which uses FMM as a matrix-vector multiplication inside the GMRES iterative method. Our FMM Helmhol&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.09948v1-abstract-full').style.display = 'inline'; document.getElementById('1803.09948v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1803.09948v1-abstract-full" style="display: none;">
        Algorithmic and architecture-oriented <span class="search-hit mathjax">optimizations</span> are essential for achieving performance worthy of anticipated energy-austere exascale systems. In this paper, we present an extreme scale FMM-accelerated boundary integral equation solver for wave scattering, which uses FMM as a matrix-vector multiplication inside the GMRES iterative method. Our FMM Helmholtz kernels treat nontrivial singular and near-field integration points. We implement highly <span class="search-hit mathjax">optimized</span> kernels for both shared and distributed <span class="search-hit mathjax">memory</span>, targeting emerging Intel extreme performance HPC architectures. We extract the potential thread- and data-level parallelism of the key Helmholtz kernels of FMM. Our <span class="search-hit mathjax">application</span> <span class="search-hit mathjax">code</span> is well <span class="search-hit mathjax">optimized</span> to exploit the AVX-512 SIMD units of Intel Skylake and Knights Landing architectures. We provide different performance models for tuning the task-based tree traversal implementation of FMM, and develop <span class="search-hit mathjax">optimal</span> architecture-specific and algorithm aware partitioning, load balancing, and communication <span class="search-hit mathjax">reducing</span> mechanisms to scale up to 6,144 compute nodes of a Cray XC40 with 196,608 hardware cores. With shared <span class="search-hit mathjax">memory</span> <span class="search-hit mathjax">optimizations</span>, we achieve roughly 77% of peak single precision floating point performance of a 56-core Skylake processor, and on average 60% of peak single precision floating point performance of a 72-core KNL. These numbers represent nearly 5.4x and 10x speedup on Skylake and KNL, respectively, compared to the baseline scalar <span class="search-hit mathjax">code</span>. With distributed <span class="search-hit mathjax">memory</span> <span class="search-hit mathjax">optimizations</span>, on the other hand, we report near-<span class="search-hit mathjax">optimal</span> efficiency in the weak scalability study with respect to both the logarithmic communication complexity as well as the theoretical scaling complexity of FMM. In addition, we exhibit up to 85% efficiency in strong scaling. We compute in excess of 2 billion DoF on the full-scale of the Cray XC40 supercomputer.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.09948v1-abstract-full').style.display = 'none'; document.getElementById('1803.09948v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 March, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1803.06089">arXiv:1803.06089</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1803.06089">pdf</a>, <a href="https://arxiv.org/format/1803.06089">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Distributed Caching for Complex Querying of Raw Arrays
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+W">Weijie Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rusu%2C+F">Florin Rusu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+B">Bin Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+K">Kesheng Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ho%2C+A+Y+Q">Anna Y. Q. Ho</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nugent%2C+P">Peter Nugent</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1803.06089v1-abstract-short" style="display: inline;">
        As <span class="search-hit mathjax">applications</span> continue to generate multi-dimensional data at exponentially increasing rates, fast analytics to extract meaningful results is becoming extremely important. The database community has developed array databases that alleviate this problem through a series of techniques. In-situ mechanisms provide direct access to raw data in the original forma&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.06089v1-abstract-full').style.display = 'inline'; document.getElementById('1803.06089v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1803.06089v1-abstract-full" style="display: none;">
        As <span class="search-hit mathjax">applications</span> continue to generate multi-dimensional data at exponentially increasing rates, fast analytics to extract meaningful results is becoming extremely important. The database community has developed array databases that alleviate this problem through a series of techniques. In-situ mechanisms provide direct access to raw data in the original format---without loading and partitioning. Parallel processing scales to the largest datasets. In-<span class="search-hit mathjax">memory</span> caching <span class="search-hit mathjax">reduces</span> latency when the same data are accessed across a workload of queries. However, we are not aware of any work on distributed caching of multi-dimensional raw arrays. In this paper, we introduce a distributed framework for cost-based caching of multi-dimensional arrays in native format. Given a set of files that contain portions of an array and an online query workload, the framework computes an effective caching plan in two stages. First, the plan identifies the cells to be cached locally from each of the input files by continuously refining an evolving R-tree index. In the second stage, an <span class="search-hit mathjax">optimal</span> assignment of cells to nodes that collocates dependent cells in order to minimize the overall data transfer is determined. We design cache eviction and placement heuristic algorithms that consider the historical query workload. A thorough experimental evaluation over two real datasets in three file formats confirms the superiority -- by as much as two orders of magnitude -- of the proposed framework over existing techniques in terms of cache overhead and workload execution time.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.06089v1-abstract-full').style.display = 'none'; document.getElementById('1803.06089v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1803.05392">arXiv:1803.05392</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1803.05392">pdf</a>, <a href="https://arxiv.org/format/1803.05392">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.artint.2020.103248">10.1016/j.artint.2020.103248 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Automated</span> Construction of Bounded-Loss Imperfect-Recall Abstractions in Extensive-Form Games
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cermak%2C+J">Jiri Cermak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lisy%2C+V">Viliam Lisy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bosansky%2C+B">Branislav Bosansky</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1803.05392v2-abstract-short" style="display: inline;">
        Extensive-form games (EFGs) model finite sequential interactions between players. The amount of <span class="search-hit mathjax">memory</span> required to represent these games is the main bottleneck of algorithms for computing&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.05392v2-abstract-full').style.display = 'inline'; document.getElementById('1803.05392v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1803.05392v2-abstract-full" style="display: none;">
        Extensive-form games (EFGs) model finite sequential interactions between players. The amount of <span class="search-hit mathjax">memory</span> required to represent these games is the main bottleneck of algorithms for computing <span class="search-hit mathjax">optimal</span> strategies and the size of these strategies is often impractical for real-world <span class="search-hit mathjax">applications</span>. A common approach to tackle the <span class="search-hit mathjax">memory</span> bottleneck is to use information abstraction that removes parts of information available to players thus <span class="search-hit mathjax">reducing</span> the number of decision points in the game. However, existing information-abstraction techniques are either specific for a particular domain, they do not provide any quality guarantees, or they are <span class="search-hit mathjax">applicable</span> to very small subclasses of EFGs. We present domain-independent abstraction methods for creating imperfect recall abstractions in extensive-form games that allow computing strategies that are (near) <span class="search-hit mathjax">optimal</span> in the original game. To this end, we introduce two novel algorithms, FPIRA and CFR+IRA, based on fictitious play and counterfactual regret minimization. These algorithms can start with an arbitrary domain specific, or the coarsest possible, abstraction of the original game. The algorithms iteratively detect the missing information they require for computing a strategy for the abstract game that is (near) <span class="search-hit mathjax">optimal</span> in the original game. This information is then included back into the abstract game. Moreover, our algorithms are able to exploit imperfect-recall abstractions that allow players to forget even history of their own actions. However, the algorithms require traversing the complete unabstracted game tree. We experimentally show that our algorithms can closely approximate Nash equilibrium of large games using abstraction with as little as 0.9% of information sets of the original game. Moreover, the results suggest that <span class="search-hit mathjax">memory</span> savings increase with the increasing size of the original games.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.05392v2-abstract-full').style.display = 'none'; document.getElementById('1803.05392v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 April, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 March, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2018.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Cermak, Jiri, Viliam Lisy, and Branislav Bosansky. "<span class="search-hit mathjax">Automated</span> construction of bounded-loss
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.10280">arXiv:1802.10280</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.10280">pdf</a>, <a href="https://arxiv.org/format/1802.10280">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Escoin: Efficient Sparse Convolutional Neural Network Inference on GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xuhao Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.10280v2-abstract-short" style="display: inline;">
        Deep neural networks have achieved remarkable accuracy in many artificial intelligence <span class="search-hit mathjax">applications</span>, e.g. computer vision, at the cost of a large number of parameters and high computational complexity. Weight pruning can compress DNN models by removing redundant parameters in the networks, but it brings sparsity in the weight matrix, and therefore makes the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.10280v2-abstract-full').style.display = 'inline'; document.getElementById('1802.10280v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.10280v2-abstract-full" style="display: none;">
        Deep neural networks have achieved remarkable accuracy in many artificial intelligence <span class="search-hit mathjax">applications</span>, e.g. computer vision, at the cost of a large number of parameters and high computational complexity. Weight pruning can compress DNN models by removing redundant parameters in the networks, but it brings sparsity in the weight matrix, and therefore makes the computation inefficient on GPUs. Although pruning can remove more than 80% of the weights, it actually hurts inference performance (speed) when running models on GPUs.
  Two major problems cause this unsatisfactory performance on GPUs. First, lowering convolution onto matrix multiplication <span class="search-hit mathjax">reduces</span> data reuse opportunities and wastes <span class="search-hit mathjax">memory</span> bandwidth. Second, the sparsity brought by pruning makes the computation irregular, which leads to inefficiency when running on massively parallel GPUs. To overcome these two limitations, we propose Escort, an efficient sparse convolutional neural networks on GPUs. Instead of using the lowering method, we choose to compute the sparse convolutions directly. We then orchestrate the parallelism and locality for the direct sparse convolution kernel, and apply customized <span class="search-hit mathjax">optimization</span> techniques to further <span class="search-hit mathjax">improve</span> performance. Evaluation on NVIDIA GPUs show that Escort can <span class="search-hit mathjax">improve</span> sparse convolution speed by 2.63x and 3.07x, and inference speed by 1.43x and 1.69x, compared to CUBLAS and CUSPARSE respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.10280v2-abstract-full').style.display = 'none'; document.getElementById('1802.10280v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 April, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 February, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.03749">arXiv:1802.03749</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.03749">pdf</a>, <a href="https://arxiv.org/format/1802.03749">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Locality <span class="search-hit mathjax">Optimized</span> Unstructured Mesh Algorithms on GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sulyok%2C+A+A">AndrÃ¡s Attila Sulyok</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Balogh%2C+G+D">GÃ¡bor DÃ¡niel Balogh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Reguly%2C+I+Z">IstvÃ¡n ZoltÃ¡n Reguly</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mudalige%2C+G+R">Gihan R. Mudalige</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.03749v4-abstract-short" style="display: inline;">
        Unstructured-mesh based numerical algorithms such as finite volume and finite element algorithms form an important class of <span class="search-hit mathjax">applications</span> for many scientific and engineering domains. The key difficulty in achieving higher performance from these&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.03749v4-abstract-full').style.display = 'inline'; document.getElementById('1802.03749v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.03749v4-abstract-full" style="display: none;">
        Unstructured-mesh based numerical algorithms such as finite volume and finite element algorithms form an important class of <span class="search-hit mathjax">applications</span> for many scientific and engineering domains. The key difficulty in achieving higher performance from these <span class="search-hit mathjax">applications</span> is the indirect accesses that lead to data-races when parallelized. Current methods for handling such data-races lead to <span class="search-hit mathjax">reduced</span> parallelism and suboptimal performance. Particularly on modern many-core architectures, such as GPUs, that has increasing core/thread counts, <span class="search-hit mathjax">reducing</span> data movement and exploiting <span class="search-hit mathjax">memory</span> locality is vital for gaining good performance.
  In this work we present novel locality-exploiting <span class="search-hit mathjax">optimizations</span> for the efficient execution of unstructured-mesh algorithms on GPUs. Building on a two-layered coloring strategy for handling data races, we introduce novel reordering and partitioning techniques to further <span class="search-hit mathjax">improve</span> efficient execution. The new <span class="search-hit mathjax">optimizations</span> are then applied to several well established unstructured-mesh <span class="search-hit mathjax">applications</span>, investigating their performance on NVIDIA&#39;s latest P100 and V100 GPUs. We demonstrate significant speedups ($1.1\text{--}1.75\times$) compared to the state-of-the-art. A range of performance metrics are benchmarked including runtime, <span class="search-hit mathjax">memory</span> transactions, achieved bandwidth performance, GPU occupancy and data reuse factors and are used to understand and explain the key factors impacting performance. The <span class="search-hit mathjax">optimized</span> algorithms are implemented as an open-source <span class="search-hit mathjax">software</span> library and we illustrate its use for <span class="search-hit mathjax">improving</span> performance of existing or new unstructured-mesh <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.03749v4-abstract-full').style.display = 'none'; document.getElementById('1802.03749v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 July, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 February, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Number of pages: 36 Number of figures: 21 Submitted to JPDC</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.02733">arXiv:1802.02733</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.02733">pdf</a>, <a href="https://arxiv.org/format/1802.02733">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        From Hashing to CNNs: Training BinaryWeight Networks via Hashing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Q">Qinghao Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+P">Peisong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+J">Jian Cheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.02733v1-abstract-short" style="display: inline;">
        &hellip;convolutional neural networks (CNNs) have shown appealing performance on various computer vision tasks in recent years. This motivates people to deploy CNNs to realworld <span class="search-hit mathjax">applications</span>. However, most of state-of-art CNNs require large&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.02733v1-abstract-full').style.display = 'inline'; document.getElementById('1802.02733v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.02733v1-abstract-full" style="display: none;">
        Deep convolutional neural networks (CNNs) have shown appealing performance on various computer vision tasks in recent years. This motivates people to deploy CNNs to realworld <span class="search-hit mathjax">applications</span>. However, most of state-of-art CNNs require large <span class="search-hit mathjax">memory</span> and computational resources, which hinders the deployment on mobile devices. Recent studies show that low-bit weight representation can <span class="search-hit mathjax">reduce</span> much storage and <span class="search-hit mathjax">memory</span> demand, and also can achieve efficient network inference. To achieve this goal, we propose a novel approach named BWNH to train Binary Weight Networks via Hashing. In this paper, we first reveal the strong connection between inner-product preserving hashing and binary weight networks, and show that training binary weight networks can be intrinsically regarded as a hashing problem. Based on this perspective, we propose an alternating <span class="search-hit mathjax">optimization</span> method to learn the hash <span class="search-hit mathjax">codes</span> instead of directly learning binary weights. Extensive experiments on CIFAR10, CIFAR100 and ImageNet demonstrate that our proposed BWNH outperforms current state-of-art by a large margin.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.02733v1-abstract-full').style.display = 'none'; document.getElementById('1802.02733v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 February, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.02474">arXiv:1802.02474</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.02474">pdf</a>, <a href="https://arxiv.org/format/1802.02474">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        High-level python abstractions for <span class="search-hit mathjax">optimal</span> checkpointing in inversion problems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kukreja%2C+N">Navjot Kukreja</a>, 
      
      <a href="/search/?searchtype=author&amp;query=H%C3%BCckelheim%2C+J">Jan HÃ¼ckelheim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lange%2C+M">Michael Lange</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Louboutin%2C+M">Mathias Louboutin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Walther%2C+A">Andrea Walther</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Funke%2C+S+W">Simon W. Funke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gorman%2C+G">Gerard Gorman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.02474v1-abstract-short" style="display: inline;">
        Inversion and PDE-constrained <span class="search-hit mathjax">optimization</span> problems often rely on solving the adjoint problem to calculate the gradient of the objec- tive function. This requires storing large amounts of intermediate data, setting a limit to the largest problem that might be solved with a given amount of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.02474v1-abstract-full').style.display = 'inline'; document.getElementById('1802.02474v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.02474v1-abstract-full" style="display: none;">
        Inversion and PDE-constrained <span class="search-hit mathjax">optimization</span> problems often rely on solving the adjoint problem to calculate the gradient of the objec- tive function. This requires storing large amounts of intermediate data, setting a limit to the largest problem that might be solved with a given amount of <span class="search-hit mathjax">memory</span> available. Checkpointing is an approach that can <span class="search-hit mathjax">reduce</span> the amount of <span class="search-hit mathjax">memory</span> required by redoing parts of the computation instead of storing intermediate results. The Revolve checkpointing algorithm o ers an <span class="search-hit mathjax">optimal</span> schedule that trades computational cost for smaller <span class="search-hit mathjax">memory</span> footprints. Integrat- ing Revolve into a modern python HPC <span class="search-hit mathjax">code</span> and combining it with <span class="search-hit mathjax">code</span> generation is not straightforward. We present an API that makes checkpointing accessible from a DSL-based <span class="search-hit mathjax">code</span> generation environment along with some initial performance gures with a focus on seismic <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.02474v1-abstract-full').style.display = 'none'; document.getElementById('1802.02474v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.09413">arXiv:1801.09413</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.09413">pdf</a>, <a href="https://arxiv.org/format/1801.09413">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Join Query <span class="search-hit mathjax">Optimization</span> Techniques for Complex Event Processing <span class="search-hit mathjax">Applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kolchinsky%2C+I">Ilya Kolchinsky</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schuster%2C+A">Assaf Schuster</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.09413v2-abstract-short" style="display: inline;">
        Complex event processing (CEP) is a prominent technology used in many modern <span class="search-hit mathjax">applications</span> for monitoring and tracking events of interest in massive data streams. CEP engines inspect real-time information flows and attempt to detect combinations of occurrences matching predefined patterns. This is done by combining basic data items, also called primitive even&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.09413v2-abstract-full').style.display = 'inline'; document.getElementById('1801.09413v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.09413v2-abstract-full" style="display: none;">
        Complex event processing (CEP) is a prominent technology used in many modern <span class="search-hit mathjax">applications</span> for monitoring and tracking events of interest in massive data streams. CEP engines inspect real-time information flows and attempt to detect combinations of occurrences matching predefined patterns. This is done by combining basic data items, also called primitive events, according to a pattern detection plan, in a manner similar to the execution of multi-join queries in traditional data management systems. Despite this similarity, little work has been done on utilizing existing join <span class="search-hit mathjax">optimization</span> methods to <span class="search-hit mathjax">improve</span> the performance of CEP-based systems. In this paper, we provide the first theoretical and experimental study of the relationship between these two research areas. We formally prove that the CEP Plan Generation problem is equivalent to the Join Query Plan Generation problem for a restricted class of patterns and can be <span class="search-hit mathjax">reduced</span> to it for a considerably wider range of classes. This result implies the NP-completeness of the CEP Plan Generation problem. We further show how join query <span class="search-hit mathjax">optimization</span> techniques developed over the last decades can be adapted and utilized to provide practically efficient solutions for complex event detection. Our experiments demonstrate the superiority of these techniques over existing strategies for CEP <span class="search-hit mathjax">optimization</span> in terms of throughput, latency, and <span class="search-hit mathjax">memory</span> consumption.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.09413v2-abstract-full').style.display = 'none'; document.getElementById('1801.09413v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 January, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.07237">arXiv:1801.07237</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.07237">pdf</a>, <a href="https://arxiv.org/format/1801.07237">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Smoke: Fine-grained Lineage at Interactive Speed
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Psallidas%2C+F">Fotis Psallidas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+E">Eugene Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.07237v1-abstract-short" style="display: inline;">
        &hellip;both traditional (e.g., debugging, auditing, data integration, and security) and emergent (e.g., interactive visualizations, iterative analytics, explanations, and cleaning) <span class="search-hit mathjax">applications</span>. The core, long-standing problem that lineage systems need to address---and the main focus of this paper---is to capture the relationships between input and output data item&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.07237v1-abstract-full').style.display = 'inline'; document.getElementById('1801.07237v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.07237v1-abstract-full" style="display: none;">
        Data lineage describes the relationship between individual input and output data items of a workflow, and has served as an integral ingredient for both traditional (e.g., debugging, auditing, data integration, and security) and emergent (e.g., interactive visualizations, iterative analytics, explanations, and cleaning) <span class="search-hit mathjax">applications</span>. The core, long-standing problem that lineage systems need to address---and the main focus of this paper---is to capture the relationships between input and output data items across a workflow with the goal to streamline queries over lineage. Unfortunately, current lineage systems either incur high lineage capture overheads, or lineage query processing costs, or both. As a result, <span class="search-hit mathjax">applications</span>, that in principle can express their logic declaratively in lineage terms, resort to hand-tuned implementations. To this end, we introduce Smoke, an in-<span class="search-hit mathjax">memory</span> database engine that neither lineage capture overhead nor lineage query processing needs to be compromised. To do so, Smoke introduces tight integration of the lineage capture logic into physical database operators; efficient, write-<span class="search-hit mathjax">optimized</span> lineage representations for storage; and <span class="search-hit mathjax">optimizations</span> when future lineage queries are known up-front. Our experiments on microbenchmarks and realistic workloads show that Smoke <span class="search-hit mathjax">reduces</span> the lineage capture overhead and streamlines lineage queries by multiple orders of magnitude compared to state-of-the-art alternatives. Our experiments on real-world <span class="search-hit mathjax">applications</span> highlight that Smoke can meet the latency requirements of interactive visualizations (e.g., &lt;150ms) and outperform hand-written implementations of data profiling primitives.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.07237v1-abstract-full').style.display = 'none'; document.getElementById('1801.07237v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 January, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.06601">arXiv:1801.06601</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.06601">pdf</a>, <a href="https://arxiv.org/format/1801.06601">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lai%2C+L">Liangzhen Lai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Suda%2C+N">Naveen Suda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chandra%2C+V">Vikas Chandra</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.06601v1-abstract-short" style="display: inline;">
        Deep Neural Networks are becoming increasingly popular in always-on IoT edge devices performing data analytics right at the source, <span class="search-hit mathjax">reducing</span> latency as well as energy consumption for data communication. This paper presents CMSIS-NN, efficient kernels developed to maximize the performance and minimize the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.06601v1-abstract-full').style.display = 'inline'; document.getElementById('1801.06601v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.06601v1-abstract-full" style="display: none;">
        Deep Neural Networks are becoming increasingly popular in always-on IoT edge devices performing data analytics right at the source, <span class="search-hit mathjax">reducing</span> latency as well as energy consumption for data communication. This paper presents CMSIS-NN, efficient kernels developed to maximize the performance and minimize the <span class="search-hit mathjax">memory</span> footprint of neural network (NN) <span class="search-hit mathjax">applications</span> on Arm Cortex-M processors targeted for intelligent IoT edge devices. Neural network inference based on CMSIS-NN kernels achieves 4.6X <span class="search-hit mathjax">improvement</span> in runtime/throughput and 4.9X <span class="search-hit mathjax">improvement</span> in energy efficiency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.06601v1-abstract-full').style.display = 'none'; document.getElementById('1801.06601v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 January, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28memory%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=200"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28memory%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28memory%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=200"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>
