<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.2.1/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.2.1/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.2.1/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.2.1/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.2.1/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.2.1/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.2.1/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.2.1/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.2.1/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

    <!-- Pendo -->
    <script>
     (function(apiKey){
         (function(p,e,n,d,o){var v,w,x,y,z;o=p[d]=p[d]||{};o._q=[];
             v=['initialize','identify','updateOptions','pageLoad'];for(w=0,x=v.length;w<x;++w)(function(m){
                 o[m]=o[m]||function(){o._q[m===v[0]?'unshift':'push']([m].concat([].slice.call(arguments,0)));};})(v[w]);
             y=e.createElement(n);y.async=!0;y.src='https://content.analytics.arxiv.org/agent/static/'+apiKey+'/pendo.js';
             z=e.getElementsByTagName(n)[0];z.parentNode.insertBefore(y,z);})(window,document,'script','pendo');

         // Call this whenever information about your visitors becomes available
         // Please use Strings, Numbers, or Bools for value types.
         pendo.initialize({
             visitor: {
                 id:              'VISITOR-UNIQUE-ID'   // Required if user is logged in
                 // email:        // Recommended if using Pendo Feedback, or NPS Email
                 // full_name:    // Recommended if using Pendo Feedback
                 // role:         // Optional

                 // You can add any additional visitor level key-values here,
                 // as long as it's not one of the above reserved names.
             },

             account: {
                 id:           'ACCOUNT-UNIQUE-ID' // Highly recommended
                 // name:         // Optional
                 // is_paying:    // Recommended if using Pendo Feedback
                 // monthly_value:// Recommended if using Pendo Feedback
                 // planLevel:    // Optional
                 // planPrice:    // Optional
                 // creationDate: // Optional

                 // You can add any additional account level key-values here,
                 // as long as it's not one of the above reserved names.
             }
         });
     })('d6494389-b427-4103-7c76-03182ecc8e60');
    </script>
    <!-- End Pendo -->


  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.2.1/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo"><img src="https://static.arxiv.org/static/base/0.17.2.1/images/arxiv-logo-web.svg" alt="arXiv" aria-label="logo" width="85" /></a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;200 of 1,439 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first">order: -announced_date_first; size: 200; classification: Computer Science (cs); include_cross_list: True; terms: AND all=(code OR program OR software OR application) AND (optimize OR optimizing OR optimization OR improve OR improving OR improvement OR automated OR automatically OR reduce OR reducing) AND (performance OR efficient OR efficiency OR effective OR effectiveness OR accuracy OR precision)</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=200">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="(code OR program OR software OR application) AND (optimize OR optimizing OR optimization OR improve OR improving OR improvement OR automated OR automatically OR reduce OR reducing) AND (performance OR efficient OR efficiency OR effective OR effectiveness OR accuracy OR precision)"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option value="50">50</option><option value="100">100</option><option selected value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=200"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=200"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=400"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=600"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=800"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.03484">arXiv:2107.03484</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.03484">pdf</a>, <a href="https://arxiv.org/format/2107.03484">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Overview of Low latency for Wireless Communications: an Evolutionary Perspective
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+X">Xin Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huo%2C+Y">Yan Huo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.03484v1-abstract-short" style="display: inline;">
        Ultra-low latency supported by the fifth generation (5G) give impetus to the prosperity of many wireless network <span class="search-hit mathjax">applications</span>, such as autonomous driving, robotics, telepresence, virtual reality and so on. Ultra-low latency is not achieved in a moment, but requires long-term evolution of network structure and key enabling communication technologies. In this&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.03484v1-abstract-full').style.display = 'inline'; document.getElementById('2107.03484v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.03484v1-abstract-full" style="display: none;">
        Ultra-low latency supported by the fifth generation (5G) give impetus to the prosperity of many wireless network <span class="search-hit mathjax">applications</span>, such as autonomous driving, robotics, telepresence, virtual reality and so on. Ultra-low latency is not achieved in a moment, but requires long-term evolution of network structure and key enabling communication technologies. In this paper, we provide an evolutionary overview of low latency in mobile communication systems, including two different evolutionary perspectives: 1) network architecture; 2) physical layer air interface technologies. We firstly describe in detail the evolution of communication network architecture from the second generation (2G) to 5G, highlighting the key points <span class="search-hit mathjax">reducing</span> latency. Moreover, we review the evolution of key enabling technologies in the physical layer from 2G to 5G, which is also aimed at <span class="search-hit mathjax">reducing</span> latency. We also discussed the challenges and future research directions for low latency in network architecture and physical layer.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.03484v1-abstract-full').style.display = 'none'; document.getElementById('2107.03484v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.03467">arXiv:2107.03467</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.03467">pdf</a>, <a href="https://arxiv.org/format/2107.03467">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Empirical Analysis of VM Startup Times in Public IaaS Clouds: An Extended Report
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+J">Jianwei Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+T">Ting Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+I+K">In Kee Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.03467v1-abstract-short" style="display: inline;">
        VM startup time is an essential factor in designing elastic cloud <span class="search-hit mathjax">applications</span>. For example, a cloud&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.03467v1-abstract-full').style.display = 'inline'; document.getElementById('2107.03467v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.03467v1-abstract-full" style="display: none;">
        VM startup time is an essential factor in designing elastic cloud <span class="search-hit mathjax">applications</span>. For example, a cloud <span class="search-hit mathjax">application</span> with autoscaling can <span class="search-hit mathjax">reduce</span> under- and over-provisioning of VM instances with a <span class="search-hit mathjax">precise</span> estimation of VM startup time, and in turn, it is likely to guarantee the <span class="search-hit mathjax">application&#39;s</span> <span class="search-hit mathjax">performance</span> and <span class="search-hit mathjax">improve</span> the cost <span class="search-hit mathjax">efficiency</span>. However, VM startup time has been little studied, and available measurement results <span class="search-hit mathjax">performed</span> previously did not consider various configurations of VMs for modern cloud <span class="search-hit mathjax">applications</span>. In this work, we <span class="search-hit mathjax">perform</span> comprehensive measurements and analysis of VM startup time from two major cloud providers, namely Amazon Web Services (AWS) and Google Cloud Platform (GCP). With three months of measurements, we collected more than 300,000 data points from each provider by applying a set of configurations, including 11+ VM types, four different data center locations, four VM image sizes, two OS types, and two purchase models (e.g., spot/preemptible VMs vs. on-demand VMs). With extensive analysis, we found that VM startup time can vary significantly because of several important factors, such as VM image sizes, data center locations, VM types, and OS types. Moreover, by comparing with previous measurement results, we confirm that cloud providers (specifically AWS) made significant <span class="search-hit mathjax">improvements</span> for the VM startup times and currently have much quicker VM startup times than in the past.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.03467v1-abstract-full').style.display = 'none'; document.getElementById('2107.03467v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.03096">arXiv:2107.03096</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.03096">pdf</a>, <a href="https://arxiv.org/format/2107.03096">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        R2F: A Remote Retraining Framework for AIoT Processors with Computing Errors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+D">Dawen Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+M">Meng He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Cheng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Ying Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+L">Long Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Huawei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiaowei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+K">Kwang-Ting Cheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.03096v1-abstract-short" style="display: inline;">
        &hellip;processors like CPUs and GPUs in a server. Applying the offline trained neural network models to the edge accelerators with errors directly may lead to considerable prediction <span class="search-hit mathjax">accuracy</span> loss.
  To address the problem, we propose a remote retraining framework (R2F) for remote AIoT processors with computing errors. It takes the remote AIoT processor with soft e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.03096v1-abstract-full').style.display = 'inline'; document.getElementById('2107.03096v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.03096v1-abstract-full" style="display: none;">
        AIoT processors fabricated with newer technology nodes suffer rising soft errors due to the shrinking transistor sizes and lower power supply. Soft errors on the AIoT processors particularly the deep learning accelerators (DLAs) with massive computing may cause substantial computing errors. These computing errors are difficult to be captured by the conventional training on general purposed processors like CPUs and GPUs in a server. Applying the offline trained neural network models to the edge accelerators with errors directly may lead to considerable prediction <span class="search-hit mathjax">accuracy</span> loss.
  To address the problem, we propose a remote retraining framework (R2F) for remote AIoT processors with computing errors. It takes the remote AIoT processor with soft errors in the training loop such that the on-site computing errors can be learned with the <span class="search-hit mathjax">application</span> data on the server and the retrained models can be resilient to the soft errors. Meanwhile, we propose an <span class="search-hit mathjax">optimized</span> partial TMR strategy to enhance the retraining. According to our experiments, R2F enables elastic design trade-offs between the model <span class="search-hit mathjax">accuracy</span> and the <span class="search-hit mathjax">performance</span> penalty. The top-5 model <span class="search-hit mathjax">accuracy</span> can be <span class="search-hit mathjax">improved</span> by 1.93%-13.73% with 0%-200% <span class="search-hit mathjax">performance</span> penalty at high fault error rate. In addition, we notice that the retraining requires massive data transmission and even dominates the training time, and propose a sparse increment compression approach for the data transmission <span class="search-hit mathjax">optimization</span>, which <span class="search-hit mathjax">reduces</span> the retraining time by 38%-88% on average with negligible <span class="search-hit mathjax">accuracy</span> loss over a straightforward remote retraining.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.03096v1-abstract-full').style.display = 'none'; document.getElementById('2107.03096v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.02244">arXiv:2107.02244</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.02244">pdf</a>, <a href="https://arxiv.org/format/2107.02244">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3452296.3472903">10.1145/3452296.3472903 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Lucid: A Language for Control in the Data Plane
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sonchack%2C+J">John Sonchack</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Loehr%2C+D">Devon Loehr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rexford%2C+J">Jennifer Rexford</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Walker%2C+D">David Walker</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.02244v1-abstract-short" style="display: inline;">
        Programmable switch hardware makes it possible to move fine-grained control logic inside the network data plane, <span class="search-hit mathjax">improving</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.02244v1-abstract-full').style.display = 'inline'; document.getElementById('2107.02244v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.02244v1-abstract-full" style="display: none;">
        Programmable switch hardware makes it possible to move fine-grained control logic inside the network data plane, <span class="search-hit mathjax">improving</span> <span class="search-hit mathjax">performance</span> for a wide range of <span class="search-hit mathjax">applications</span>. However, <span class="search-hit mathjax">applications</span> with integrated control are inherently hard to write in existing data-plane <span class="search-hit mathjax">programming</span> languages such as P4. This paper presents Lucid, a language that raises the level of abstraction for putting control functionality in the data plane. Lucid introduces abstractions that make it easy to write sophisticated data-plane <span class="search-hit mathjax">applications</span> with interleaved packet-handling and control logic, specialized type and syntax systems that prevent programmer bugs related to data-plane state, and an open-sourced compiler that translates Lucid <span class="search-hit mathjax">programs</span> into P4 <span class="search-hit mathjax">optimized</span> for the Intel Tofino. These features make Lucid general and easy to use, as we demonstrate by writing a suite of ten different data-plane <span class="search-hit mathjax">applications</span> in Lucid. Working prototypes take well under an hour to write, even for a programmer without prior Tofino experience, have around 10x fewer lines of <span class="search-hit mathjax">code</span> compared to P4, and compile <span class="search-hit mathjax">efficiently</span> to real hardware. In a stateful firewall written in Lucid, we find that moving control from a switch&#39;s CPU to its data-plane processor using Lucid <span class="search-hit mathjax">reduces</span> the latency of <span class="search-hit mathjax">performance</span>-sensitive operations by over 300X.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.02244v1-abstract-full').style.display = 'none'; document.getElementById('2107.02244v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages plus 5 pages references/appendix. 17 figures. To appear in SIGCOMM 2021</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          C.2.1
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.01598">arXiv:2107.01598</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.01598">pdf</a>, <a href="https://arxiv.org/format/2107.01598">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Domain Adaptation for Sentiment Analysis Using Increased Intraclass Separation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rostami%2C+M">Mohammad Rostami</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Galstyan%2C+A">Aram Galstyan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.01598v1-abstract-short" style="display: inline;">
        Sentiment analysis is a costly yet necessary task for enterprises to study the opinions of their customers to <span class="search-hit mathjax">improve</span> their products and to determine&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.01598v1-abstract-full').style.display = 'inline'; document.getElementById('2107.01598v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.01598v1-abstract-full" style="display: none;">
        Sentiment analysis is a costly yet necessary task for enterprises to study the opinions of their customers to <span class="search-hit mathjax">improve</span> their products and to determine <span class="search-hit mathjax">optimal</span> marketing strategies. Due to the existence of a wide range of domains across different products and services, cross-domain sentiment analysis methods have received significant attention. These methods mitigate the domain gap between different <span class="search-hit mathjax">applications</span> by training cross-domain generalizable classifiers which help to relax the need for data annotation for each domain. Most existing methods focus on learning domain-agnostic representations that are invariant with respect to both the source and the target domains. As a result, a classifier that is trained using the source domain annotated data would generalize well in a related target domain. We introduce a new domain adaptation method which induces large margins between different classes in an embedding space. This embedding space is trained to be domain-agnostic by matching the data distributions across the domains. Large intraclass margins in the source domain help to <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">effect</span> of &#34;domain shift&#34; on the classifier <span class="search-hit mathjax">performance</span> in the target domain. Theoretical and empirical analysis are provided to demonstrate that the proposed method is <span class="search-hit mathjax">effective</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.01598v1-abstract-full').style.display = 'none'; document.getElementById('2107.01598v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.01405">arXiv:2107.01405</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.01405">pdf</a>, <a href="https://arxiv.org/format/2107.01405">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Fuzzy Scheduling Strategy for Deadline-Based Workflow <span class="search-hit mathjax">Applications</span> in Uncertain Edge-Cloud Environments
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+B">Bing Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+C">Chaowei Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xing Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+N+N">Neal N. Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hua%2C+P">Peisong Hua</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+Q">Qiang Shen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.01405v1-abstract-short" style="display: inline;">
        Workflow scheduling is critical to <span class="search-hit mathjax">performing</span> many practical workflow&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.01405v1-abstract-full').style.display = 'inline'; document.getElementById('2107.01405v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.01405v1-abstract-full" style="display: none;">
        Workflow scheduling is critical to <span class="search-hit mathjax">performing</span> many practical workflow <span class="search-hit mathjax">applications</span>. Scheduling based on edge-cloud computing can help addressing the high complexity of workflow <span class="search-hit mathjax">applications</span>, while decreasing the data transmission delay. However, due to the nature of heterogeneous resources in edge-cloud environments and the complicated data dependencies between the tasks in such a workflow, significant challenges for workflow scheduling remain, including the selection of an <span class="search-hit mathjax">optimal</span> tasks-servers solution from the possible numerous combinations. Existing studies are mainly done subject to rigorous conditions without fluctuations, ignoring the fact that workflow scheduling is typically present in uncertain environments. In this study, we focus on <span class="search-hit mathjax">reducing</span> the execution cost of workflow <span class="search-hit mathjax">applications</span> mainly caused by task computation and data transmission, while satisfying the workflow deadline in uncertain edge-cloud environments. The Triangular Fuzzy Numbers (TFNs) are adopted to represent task processing time and data transferring time. A cost-driven fuzzy scheduling strategy based on an Adaptive Discrete Particle Swarm <span class="search-hit mathjax">Optimization</span> (ADPSO) algorithm is proposed, which is employed the operators of Genetic Algorithm (GA). This strategy introduces the randomly two-point crossover operator, neighborhood mutation operator, and adaptive multipoint mutation operator of GA to <span class="search-hit mathjax">effectively</span> avoid converging on local optima. The experimental results show that our strategy can <span class="search-hit mathjax">effectively</span> <span class="search-hit mathjax">reduce</span> the workflow execution cost in uncertain edge-cloud environments, compared with other benchmark solutions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.01405v1-abstract-full').style.display = 'none'; document.getElementById('2107.01405v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.01093">arXiv:2107.01093</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.01093">pdf</a>, <a href="https://arxiv.org/format/2107.01093">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Model Checking C++ <span class="search-hit mathjax">Programs</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Monteiro%2C+F+R">Felipe R. Monteiro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gadelha%2C+M+R">Mikhail R. Gadelha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cordeiro%2C+L+C">Lucas C. Cordeiro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.01093v1-abstract-short" style="display: inline;">
        In the last three decades, memory safety issues in system <span class="search-hit mathjax">programming</span> languages such as C or C++ have been one of the significant sources of security vulnerabilities. However, there exist only a few attempts with limited success to cope with the complexity of C++&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.01093v1-abstract-full').style.display = 'inline'; document.getElementById('2107.01093v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.01093v1-abstract-full" style="display: none;">
        In the last three decades, memory safety issues in system <span class="search-hit mathjax">programming</span> languages such as C or C++ have been one of the significant sources of security vulnerabilities. However, there exist only a few attempts with limited success to cope with the complexity of C++ <span class="search-hit mathjax">program</span> verification. Here we describe and evaluate a novel verification approach based on bounded model checking (BMC) and satisfiability modulo theories (SMT) to verify C++ <span class="search-hit mathjax">programs</span> formally. Our verification approach analyzes bounded C++ <span class="search-hit mathjax">programs</span> by encoding into SMT various sophisticated features that the C++ <span class="search-hit mathjax">programming</span> language offers, such as templates, inheritance, polymorphism, exception handling, and the Standard C++ Libraries. We formalize these features within our formal verification framework using a decidable fragment of first-order logic and then show how state-of-the-art SMT solvers can <span class="search-hit mathjax">efficiently</span> handle that. We implemented our verification approach on top of ESBMC. We compare ESBMC to LLBMC and DIVINE, which are state-of-the-art verifiers to check C++ <span class="search-hit mathjax">programs</span> directly from the LLVM bitcode. Experimental results show that ESBMC can handle a wide range of C++ <span class="search-hit mathjax">programs</span>, presenting a higher number of correct verification results. At the same time, it <span class="search-hit mathjax">reduces</span> the verification time if compared to LLBMC and DIVINE tools. Additionally, ESBMC has been applied to a commercial C++ <span class="search-hit mathjax">application</span> in the telecommunication domain and successfully detected arithmetic overflow errors, potentially leading to security vulnerabilities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.01093v1-abstract-full').style.display = 'none'; document.getElementById('2107.01093v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">30 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.01025">arXiv:2107.01025</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.01025">pdf</a>, <a href="https://arxiv.org/format/2107.01025">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Structure-aware reinforcement learning for node-overload protection in mobile edge computing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jitani%2C+A">Anirudha Jitani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahajan%2C+A">Aditya Mahajan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Z">Zhongwen Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abou-zeid%2C+H">Hatem Abou-zeid</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fapi%2C+E+T">Emmanuel T. Fapi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Purmehdi%2C+H">Hakimeh Purmehdi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.01025v1-abstract-short" style="display: inline;">
        Mobile Edge Computing (MEC) refers to the concept of placing computational capability and <span class="search-hit mathjax">applications</span> at the edge of the network, providing benefits such as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.01025v1-abstract-full').style.display = 'inline'; document.getElementById('2107.01025v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.01025v1-abstract-full" style="display: none;">
        Mobile Edge Computing (MEC) refers to the concept of placing computational capability and <span class="search-hit mathjax">applications</span> at the edge of the network, providing benefits such as <span class="search-hit mathjax">reduced</span> latency in handling client requests, <span class="search-hit mathjax">reduced</span> network congestion, and <span class="search-hit mathjax">improved</span> <span class="search-hit mathjax">performance</span> of <span class="search-hit mathjax">applications</span>. The <span class="search-hit mathjax">performance</span> and reliability of MEC are degraded significantly when one or several edge servers in the cluster are overloaded. Especially when a server crashes due to the overload, it causes service failures in MEC. In this work, an adaptive admission control policy to prevent edge node from getting overloaded is presented. This approach is based on a recently-proposed low complexity RL (Reinforcement Learning) algorithm called SALMUT (Structure-Aware Learning for Multiple Thresholds), which exploits the structure of the <span class="search-hit mathjax">optimal</span> admission control policy in multi-class queues for an average-cost setting. We extend the framework to work for node overload-protection problem in a discounted-cost setting. The proposed solution is validated using several scenarios mimicking real-world deployments in two different settings - computer simulations and a docker testbed. Our empirical evaluations show that the total discounted cost incurred by SALMUT is similar to state-of-the-art deep RL algorithms such as PPO (Proximal Policy <span class="search-hit mathjax">Optimization</span>) and A2C (Advantage Actor Critic) but requires an order of magnitude less time to train, outputs easily interpretable policy, and can be deployed in an online manner.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.01025v1-abstract-full').style.display = 'none'; document.getElementById('2107.01025v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.00822">arXiv:2107.00822</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.00822">pdf</a>, <a href="https://arxiv.org/format/2107.00822">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        F-LOAM: Fast LiDAR Odometry And Mapping
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Han Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chen Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chun-Lin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lihua Xie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.00822v1-abstract-short" style="display: inline;">
        Simultaneous Localization and Mapping (SLAM) has wide robotic <span class="search-hit mathjax">applications</span> such as autonomous driving and unmanned aerial vehicles. Both computational&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00822v1-abstract-full').style.display = 'inline'; document.getElementById('2107.00822v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.00822v1-abstract-full" style="display: none;">
        Simultaneous Localization and Mapping (SLAM) has wide robotic <span class="search-hit mathjax">applications</span> such as autonomous driving and unmanned aerial vehicles. Both computational <span class="search-hit mathjax">efficiency</span> and localization <span class="search-hit mathjax">accuracy</span> are of great importance towards a good SLAM system. Existing works on LiDAR based SLAM often formulate the problem as two modules: scan-to-scan match and scan-to-map refinement. Both modules are solved by iterative calculation which are computationally expensive. In this paper, we propose a general solution that aims to provide a computationally <span class="search-hit mathjax">efficient</span> and accurate framework for LiDAR based SLAM. Specifically, we adopt a non-iterative two-stage distortion compensation method to <span class="search-hit mathjax">reduce</span> the computational cost. For each scan input, the edge and planar features are extracted and matched to a local edge map and a local plane map separately, where the local smoothness is also considered for iterative pose <span class="search-hit mathjax">optimization</span>. Thorough experiments are <span class="search-hit mathjax">performed</span> to evaluate its <span class="search-hit mathjax">performance</span> in challenging scenarios, including localization for a warehouse <span class="search-hit mathjax">Automated</span> Guided Vehicle (AGV) and a public dataset on autonomous driving. The proposed method achieves a competitive localization <span class="search-hit mathjax">accuracy</span> with a processing rate of more than 10 Hz in the public dataset evaluation, which provides a good trade-off between <span class="search-hit mathjax">performance</span> and computational cost for practical <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00822v1-abstract-full').style.display = 'none'; document.getElementById('2107.00822v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.00615">arXiv:2107.00615</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.00615">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Medical Physics">physics.med-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A linear phase evolution model for reduction of temporal unwrapping and field estimation errors in multi-echo GRE
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Paul%2C+J+S">Joseph Suresh Paul</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Madhusoodhanan%2C+S">Sreekanth Madhusoodhanan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.00615v1-abstract-short" style="display: inline;">
        This article aims at developing a model based <span class="search-hit mathjax">optimization</span> for reduction of temporal unwrapping and field estimation errors in multi-echo acquisition of Gradient Echo sequence. Using the assumption that the phase is linear along the temporal dimension, the field estimation is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00615v1-abstract-full').style.display = 'inline'; document.getElementById('2107.00615v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.00615v1-abstract-full" style="display: none;">
        This article aims at developing a model based <span class="search-hit mathjax">optimization</span> for reduction of temporal unwrapping and field estimation errors in multi-echo acquisition of Gradient Echo sequence. Using the assumption that the phase is linear along the temporal dimension, the field estimation is <span class="search-hit mathjax">performed</span> by <span class="search-hit mathjax">application</span> of unity rank approximation to the Hankel matrix formed using the complex exponential of the channel combined phase at each echo time. For the purpose of maintaining consistency with the observed complex data, the linear phase evolution model is formulated as an <span class="search-hit mathjax">optimization</span> problem with a cost function that involves a fidelity term and a unity rank prior, implemented using alternating minimization. Itoh s algorithm applied to the multi-echo phase estimated from this linear phase evolution model is able to <span class="search-hit mathjax">reduce</span> the unwrapping errors as compared to the unwrapping when directly applied to the measured phase. Secondly, the <span class="search-hit mathjax">improved</span> <span class="search-hit mathjax">accuracy</span> of the frequency fit in comparison to estimation using weighted least-square regression and penalized maximum likelihood is demonstrated using numerical simulation of field perturbation due to magnetic susceptibility <span class="search-hit mathjax">effect</span>. It is shown that the field can be estimated with 80 percent reduction in mean absolute error in comparison to wLSR and 66 percent reduction with respect to penalized maximum likelihood. The <span class="search-hit mathjax">improvement</span> in <span class="search-hit mathjax">performance</span> becomes more pronounced with increasing strengths of field gradient magnitudes and echo spacing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00615v1-abstract-full').style.display = 'none'; document.getElementById('2107.00615v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">29pages, 8 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          J.2
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.00465">arXiv:2107.00465</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.00465">pdf</a>, <a href="https://arxiv.org/format/2107.00465">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Physics-Informed Neural Networks for Minimising Worst-Case Violations in DC <span class="search-hit mathjax">Optimal</span> Power Flow
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nellikkath%2C+R">Rahul Nellikkath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatzivasileiadis%2C+S">Spyros Chatzivasileiadis</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.00465v1-abstract-short" style="display: inline;">
        Physics-informed neural networks exploit the existing models of the underlying physical systems to generate higher <span class="search-hit mathjax">accuracy</span> results with fewer data. Such approaches can help drastically&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00465v1-abstract-full').style.display = 'inline'; document.getElementById('2107.00465v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.00465v1-abstract-full" style="display: none;">
        Physics-informed neural networks exploit the existing models of the underlying physical systems to generate higher <span class="search-hit mathjax">accuracy</span> results with fewer data. Such approaches can help drastically <span class="search-hit mathjax">reduce</span> the computation time and generate a good estimate of computationally intensive processes in power systems, such as dynamic security assessment or <span class="search-hit mathjax">optimal</span> power flow. Combined with the extraction of worst-case guarantees for the neural network <span class="search-hit mathjax">performance</span>, such neural networks can be applied in safety-critical <span class="search-hit mathjax">applications</span> in power systems and build a high level of trust among power system operators. This paper takes the first step and applies, for the first time to our knowledge, Physics-Informed Neural Networks with Worst-Case Guarantees for the DC <span class="search-hit mathjax">Optimal</span> Power Flow problem. We look for guarantees related to (i) maximum constraint violations, (ii) maximum distance between predicted and <span class="search-hit mathjax">optimal</span> decision variables, and (iii) maximum sub-<span class="search-hit mathjax">optimality</span> in the entire input domain. In a range of PGLib-OPF networks, we demonstrate how physics-informed neural networks can be supplied with worst-case guarantees and how they can lead to <span class="search-hit mathjax">reduced</span> worst-case violations compared with conventional neural networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00465v1-abstract-full').style.display = 'none'; document.getElementById('2107.00465v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The <span class="search-hit mathjax">code</span> to reproduce all simulation results is available online in https://github.com/RahulNellikkath/Physics-Informed-Neural-Network-for-DC-OPF</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.15878">arXiv:2106.15878</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.15878">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Formal Languages and Automata Theory">cs.FL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards establishing formal verification and inductive <span class="search-hit mathjax">code</span> synthesis in the PLC domain
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wei%C3%9F%2C+M">Matthias Weiß</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marks%2C+P">Philipp Marks</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maschler%2C+B">Benjamin Maschler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=White%2C+D">Dustin White</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kesseli%2C+P">Pascal Kesseli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Weyrich%2C+M">Michael Weyrich</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.15878v1-abstract-short" style="display: inline;">
        Nowadays, formal methods are used in various areas for the verification of <span class="search-hit mathjax">programs</span> or for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.15878v1-abstract-full').style.display = 'inline'; document.getElementById('2106.15878v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.15878v1-abstract-full" style="display: none;">
        Nowadays, formal methods are used in various areas for the verification of <span class="search-hit mathjax">programs</span> or for <span class="search-hit mathjax">code</span> generation from models in order to increase the quality of <span class="search-hit mathjax">software</span> and to <span class="search-hit mathjax">reduce</span> costs. However, there are still fields in which formal methods have not been widely adopted, despite the large set of possible benefits offered. This is the case for the area of programmable logic controllers (PLC). This article aims to evaluate the potential of formal methods in the context of PLC development. For this purpose, the general concepts of formal methods are first introduced and then transferred to the PLC area, resulting in an engineering-oriented description of the technology that is based on common concepts from PLC development. Based on this description, PLC professionals with varying degrees of experience were interviewed for their perspective on the topic and to identify possible use cases within the PLC domain. The survey results indicate the technology&#39;s high potential in the PLC area, either as a tool to directly support the developer or as a key element within a model-based systems engineering toolchain. The evaluation of the survey results is <span class="search-hit mathjax">performed</span> with the aid of a demo <span class="search-hit mathjax">application</span> that communicates with the Totally Integrated <span class="search-hit mathjax">Automation</span> Portal from Siemens and generates <span class="search-hit mathjax">programs</span> via Fastsynth, a model-based open source <span class="search-hit mathjax">code</span> generator. Benchmarks based on an industry-related PLC project show satisfactory synthesis times and a successful integration into the workflow of a PLC developer.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.15878v1-abstract-full').style.display = 'none'; document.getElementById('2106.15878v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 6 figures, 1 table. Accepted for publication at IEEE INDIN 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.15284">arXiv:2106.15284</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.15284">pdf</a>, <a href="https://arxiv.org/format/2106.15284">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NMPO: Near-Memory Computing Profiling and Offloading
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Corda%2C+S">Stefano Corda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumaraswamy%2C+M">Madhurya Kumaraswamy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Awan%2C+A+J">Ahsan Javed Awan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jordans%2C+R">Roel Jordans</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+A">Akash Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Corporaal%2C+H">Henk Corporaal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.15284v1-abstract-short" style="display: inline;">
        Real-world <span class="search-hit mathjax">applications</span> are now processing big-data sets, often bottlenecked by the data movement between the compute units and the main memory. Near-memory computing (NMC), a modern data-centric computational paradigm, can alleviate these bottlenecks, thereby&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.15284v1-abstract-full').style.display = 'inline'; document.getElementById('2106.15284v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.15284v1-abstract-full" style="display: none;">
        Real-world <span class="search-hit mathjax">applications</span> are now processing big-data sets, often bottlenecked by the data movement between the compute units and the main memory. Near-memory computing (NMC), a modern data-centric computational paradigm, can alleviate these bottlenecks, thereby <span class="search-hit mathjax">improving</span> the <span class="search-hit mathjax">performance</span> of <span class="search-hit mathjax">applications</span>. The lack of NMC system availability makes simulators the primary evaluation tool for <span class="search-hit mathjax">performance</span> estimation. However, simulators are usually time-consuming, and methods that can <span class="search-hit mathjax">reduce</span> this overhead would accelerate the early-stage design process of NMC systems. This work proposes Near-Memory computing Profiling and Offloading (NMPO), a high-level framework capable of predicting NMC offloading suitability employing an ensemble machine learning model. NMPO predicts NMC suitability with an <span class="search-hit mathjax">accuracy</span> of 85.6% and, compared to prior works, can <span class="search-hit mathjax">reduce</span> the prediction time by using hardware-dependent <span class="search-hit mathjax">applications</span> features by up to 3 order of magnitude.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.15284v1-abstract-full').style.display = 'none'; document.getElementById('2106.15284v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Euromicro Conference on Digital System Design 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.14830">arXiv:2106.14830</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.14830">pdf</a>, <a href="https://arxiv.org/format/2106.14830">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        THUE: Discovering Top-K High Utility Episodes
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+S">Shicheng Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jiahui Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gan%2C+W">Wensheng Gan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+G">Guoting Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goyal%2C+V">Vikram Goyal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.14830v1-abstract-short" style="display: inline;">
        Episode discovery from an event is a popular framework for data mining tasks and has many real-world <span class="search-hit mathjax">applications</span>. An episode is a partially ordered set of objects (e.g., item, node), and each object is associated with an event type. This episode can also be considered as a complex event sub-sequence. High-utility episode mining is an interesting utility-dri&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14830v1-abstract-full').style.display = 'inline'; document.getElementById('2106.14830v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.14830v1-abstract-full" style="display: none;">
        Episode discovery from an event is a popular framework for data mining tasks and has many real-world <span class="search-hit mathjax">applications</span>. An episode is a partially ordered set of objects (e.g., item, node), and each object is associated with an event type. This episode can also be considered as a complex event sub-sequence. High-utility episode mining is an interesting utility-driven mining task in the real world. Traditional episode mining algorithms, by setting a threshold, usually return a huge episode that is neither intuitive nor saves time. In general, finding a suitable threshold in a pattern-mining algorithm is a trivial and time-consuming task. In this paper, we propose a novel algorithm, called Top-K High Utility Episode (THUE) mining within the complex event sequence, which redefines the previous mining task by obtaining the K highest episodes. We introduce several threshold-raising strategies and <span class="search-hit mathjax">optimize</span> the episode-weighted utilization upper bounds to speed up the mining process and <span class="search-hit mathjax">effectively</span> <span class="search-hit mathjax">reduce</span> the memory cost. Finally, the experimental results on both real-life and synthetic datasets reveal that the THUE algorithm can offer six to eight orders of magnitude running time <span class="search-hit mathjax">performance</span> <span class="search-hit mathjax">improvement</span> over the state-of-the-art algorithm and has low memory consumption.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14830v1-abstract-full').style.display = 'none'; document.getElementById('2106.14830v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint. 6 figures, 9 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.14776">arXiv:2106.14776</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.14776">pdf</a>, <a href="https://arxiv.org/format/2106.14776">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-objective Evolutionary Approach for <span class="search-hit mathjax">Efficient</span> Kernel Size and Shape for CNN
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Ziwei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Trefzer%2C+M+A">Martin A. Trefzer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bale%2C+S+J">Simon J. Bale</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tyrrell%2C+A+M">Andy M. Tyrrell</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.14776v1-abstract-short" style="display: inline;">
        &hellip;such as VGGNet and ResNet, have become increasingly accurate, these networks are computationally expensive involving billions of arithmetic operations and parameters. To <span class="search-hit mathjax">improve</span> the classification&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14776v1-abstract-full').style.display = 'inline'; document.getElementById('2106.14776v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.14776v1-abstract-full" style="display: none;">
        While state-of-the-art development in CNN topology, such as VGGNet and ResNet, have become increasingly accurate, these networks are computationally expensive involving billions of arithmetic operations and parameters. To <span class="search-hit mathjax">improve</span> the classification <span class="search-hit mathjax">accuracy</span>, state-of-the-art CNNs usually involve large and complex convolutional layers. However, for certain <span class="search-hit mathjax">applications</span>, e.g. Internet of Things (IoT), where such CNNs are to be implemented on resource-constrained platforms, the CNN architectures have to be small and <span class="search-hit mathjax">efficient</span>. To deal with this problem, <span class="search-hit mathjax">reducing</span> the resource consumption in convolutional layers has become one of the most significant solutions. In this work, a multi-objective optimisation approach is proposed to trade-off between the amount of computation and network <span class="search-hit mathjax">accuracy</span> by using Multi-Objective Evolutionary Algorithms (MOEAs). The number of convolution kernels and the size of these kernels are proportional to computational resource consumption of CNNs. Therefore, this paper considers optimising the computational resource consumption by <span class="search-hit mathjax">reducing</span> the size and number of kernels in convolutional layers. Additionally, the use of unconventional kernel shapes has been investigated and results show these clearly outperform the commonly used square convolution kernels. The main contributions of this paper are therefore a methodology to significantly <span class="search-hit mathjax">reduce</span> computational cost of CNNs, based on unconventional kernel shapes, and provide different trade-offs for specific use cases. The experimental results further demonstrate that the proposed method achieves large <span class="search-hit mathjax">improvements</span> in resource consumption with no significant reduction in network <span class="search-hit mathjax">performance</span>. Compared with the benchmark CNN, the best trade-off architecture shows a reduction in multiplications of up to 6X and with slight increase in classification <span class="search-hit mathjax">accuracy</span> on CIFAR-10 dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14776v1-abstract-full').style.display = 'none'; document.getElementById('2106.14776v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages paper, plus 17 papers supplementary materials</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.14156">arXiv:2106.14156</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.14156">pdf</a>, <a href="https://arxiv.org/format/2106.14156">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Post-Training Quantization for Vision Transformer
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhenhua Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yunhe Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+K">Kai Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+S">Siwei Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+W">Wen Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.14156v1-abstract-short" style="display: inline;">
        Recently, transformer has achieved remarkable <span class="search-hit mathjax">performance</span> on a variety of computer vision&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14156v1-abstract-full').style.display = 'inline'; document.getElementById('2106.14156v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.14156v1-abstract-full" style="display: none;">
        Recently, transformer has achieved remarkable <span class="search-hit mathjax">performance</span> on a variety of computer vision <span class="search-hit mathjax">applications</span>. Compared with mainstream convolutional neural networks, vision transformers are often of sophisticated architectures for extracting powerful feature representations, which are more difficult to be developed on mobile devices. In this paper, we present an <span class="search-hit mathjax">effective</span> post-training quantization algorithm for <span class="search-hit mathjax">reducing</span> the memory storage and computational costs of vision transformers. Basically, the quantization task can be regarded as finding the <span class="search-hit mathjax">optimal</span> low-bit quantization intervals for weights and inputs, respectively. To preserve the functionality of the attention mechanism, we introduce a ranking loss into the conventional quantization objective that aims to keep the relative order of the self-attention results after quantization. Moreover, we thoroughly analyze the relationship between quantization loss of different layers and the feature diversity, and explore a mixed-<span class="search-hit mathjax">precision</span> quantization scheme by exploiting the nuclear norm of each attention map and output feature. The <span class="search-hit mathjax">effectiveness</span> of the proposed method is verified on several benchmark models and datasets, which outperforms the state-of-the-art post-training quantization algorithms. For instance, we can obtain an 81.29\% top-1 <span class="search-hit mathjax">accuracy</span> using DeiT-B model on ImageNet dataset with about 8-bit quantization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14156v1-abstract-full').style.display = 'none'; document.getElementById('2106.14156v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.13669">arXiv:2106.13669</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.13669">pdf</a>, <a href="https://arxiv.org/format/2106.13669">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-player Multi-armed Bandits with Collision-Dependent Reward Distributions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+C">Chengshuai Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+C">Cong Shen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.13669v1-abstract-short" style="display: inline;">
        &hellip;where the reward distribution changes if a collision occurs on the arm. Existing literature always assumes a zero reward for involved players if collision happens, but for <span class="search-hit mathjax">applications</span> such as cognitive radio, the more realistic scenario is that collision&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.13669v1-abstract-full').style.display = 'inline'; document.getElementById('2106.13669v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.13669v1-abstract-full" style="display: none;">
        We study a new stochastic multi-player multi-armed bandits (MP-MAB) problem, where the reward distribution changes if a collision occurs on the arm. Existing literature always assumes a zero reward for involved players if collision happens, but for <span class="search-hit mathjax">applications</span> such as cognitive radio, the more realistic scenario is that collision <span class="search-hit mathjax">reduces</span> the mean reward but not necessarily to zero. We focus on the more practical no-sensing setting where players do not perceive collisions directly, and propose the Error-Correction Collision Communication (EC3) algorithm that models implicit communication as a reliable communication over noisy channel problem, for which random <span class="search-hit mathjax">coding</span> error exponent is used to establish the <span class="search-hit mathjax">optimal</span> regret that no communication protocol can beat. Finally, <span class="search-hit mathjax">optimizing</span> the tradeoff between <span class="search-hit mathjax">code</span> length and decoding error rate leads to a regret that approaches the centralized MP-MAB regret, which represents a natural lower bound. Experiments with practical error-correction <span class="search-hit mathjax">codes</span> on both synthetic and real-world datasets demonstrate the superiority of EC3. In particular, the results show that the choice of <span class="search-hit mathjax">coding</span> schemes has a profound impact on the regret <span class="search-hit mathjax">performance</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.13669v1-abstract-full').style.display = 'none'; document.getElementById('2106.13669v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 14 figures. Accepted to IEEE Transactions on Signal Processing</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.12387">arXiv:2106.12387</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.12387">pdf</a>, <a href="https://arxiv.org/format/2106.12387">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fairness in Cardiac MR Image Analysis: An Investigation of Bias Due to Data Imbalance in Deep Learning Based Segmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Puyol-Anton%2C+E">Esther Puyol-Anton</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ruijsink%2C+B">Bram Ruijsink</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Piechnik%2C+S+K">Stefan K. Piechnik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Neubauer%2C+S">Stefan Neubauer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petersen%2C+S+E">Steffen E. Petersen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Razavi%2C+R">Reza Razavi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=King%2C+A+P">Andrew P. King</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.12387v2-abstract-short" style="display: inline;">
        &hellip;to assessing AI algorithms for potential bias based on demographic characteristics such as race and gender, and the development of algorithms to address this bias. Most <span class="search-hit mathjax">applications</span> to date have been in computer vision, although some work in healthcare has started to emerge. The use of deep learning (DL) in cardiac MR segmentation has led to impressive resul&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.12387v2-abstract-full').style.display = 'inline'; document.getElementById('2106.12387v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.12387v2-abstract-full" style="display: none;">
        The subject of &#34;fairness&#34; in artificial intelligence (AI) refers to assessing AI algorithms for potential bias based on demographic characteristics such as race and gender, and the development of algorithms to address this bias. Most <span class="search-hit mathjax">applications</span> to date have been in computer vision, although some work in healthcare has started to emerge. The use of deep learning (DL) in cardiac MR segmentation has led to impressive results in recent years, and such techniques are starting to be translated into clinical practice. However, no work has yet investigated the fairness of such models. In this work, we <span class="search-hit mathjax">perform</span> such an analysis for racial/gender groups, focusing on the problem of training data imbalance, using a nnU-Net model trained and evaluated on cine short axis cardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from 6 different racial groups. We find statistically significant differences in Dice <span class="search-hit mathjax">performance</span> between different racial groups. To <span class="search-hit mathjax">reduce</span> the racial bias, we investigated three strategies: (1) stratified batch sampling, in which batch sampling is stratified to ensure balance between racial groups; (2) fair meta-learning for segmentation, in which a DL classifier is trained to classify race and jointly <span class="search-hit mathjax">optimized</span> with the segmentation model; and (3) protected group models, in which a different segmentation model is trained for each racial group. We also compared the results to the scenario where we have a perfectly balanced database. To assess fairness we used the standard deviation (SD) and skewed error ratio (SER) of the average Dice values. Our results demonstrate that the racial bias results from the use of imbalanced training data, and that all proposed bias mitigation strategies <span class="search-hit mathjax">improved</span> fairness, with the best SD and SER resulting from the use of protected group models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.12387v2-abstract-full').style.display = 'none'; document.getElementById('2106.12387v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">MICCAI 2021 conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.12089">arXiv:2106.12089</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.12089">pdf</a>, <a href="https://arxiv.org/format/2106.12089">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Structured in Space, Randomized in Time: Leveraging Dropout in RNNs for <span class="search-hit mathjax">Efficient</span> Training
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sarma%2C+A">Anup Sarma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+S">Sonali Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+H">Huaipan Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+R">Rui Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kandemir%2C+M+T">Mahmut T Kandemir</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+C+R">Chita R Das</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.12089v1-abstract-short" style="display: inline;">
        &hellip;Long Short-Term Memory (LSTM) variants, have been widely used as a deep learning tool for tackling sequence-based learning tasks in text and speech. Training of such LSTM <span class="search-hit mathjax">applications</span> is computationally intensive due to the recurrent nature of hidden state computation that repeats for each time step. While sparsity in Deep Neural Nets has been widely seen as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.12089v1-abstract-full').style.display = 'inline'; document.getElementById('2106.12089v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.12089v1-abstract-full" style="display: none;">
        Recurrent Neural Networks (RNNs), more specifically their Long Short-Term Memory (LSTM) variants, have been widely used as a deep learning tool for tackling sequence-based learning tasks in text and speech. Training of such LSTM <span class="search-hit mathjax">applications</span> is computationally intensive due to the recurrent nature of hidden state computation that repeats for each time step. While sparsity in Deep Neural Nets has been widely seen as an opportunity for <span class="search-hit mathjax">reducing</span> computation time in both training and inference phases, the usage of non-ReLU activation in LSTM RNNs renders the opportunities for such dynamic sparsity associated with neuron activation and gradient values to be limited or non-existent. In this work, we identify dropout induced sparsity for LSTMs as a suitable mode of computation reduction. Dropout is a widely used regularization mechanism, which randomly drops computed neuron values during each iteration of training. We propose to structure dropout patterns, by dropping out the same set of physical neurons within a batch, resulting in column (row) level hidden state sparsity, which are well amenable to computation reduction at run-time in general-purpose SIMD hardware as well as systolic arrays. We conduct our experiments for three representative NLP tasks: language modelling on the PTB dataset, OpenNMT based machine translation using the IWSLT De-En and En-Vi datasets, and named entity recognition sequence labelling using the CoNLL-2003 shared task. We demonstrate that our proposed approach can be used to translate dropout-based computation reduction into <span class="search-hit mathjax">reduced</span> training time, with <span class="search-hit mathjax">improvement</span> ranging from 1.23x to 1.64x, without sacrificing the target metric.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.12089v1-abstract-full').style.display = 'none'; document.getElementById('2106.12089v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.12007">arXiv:2106.12007</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.12007">pdf</a>, <a href="https://arxiv.org/format/2106.12007">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TPDS.2021.3090334">10.1109/TPDS.2021.3090334 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Energy hardware and workload aware job scheduling towards interconnected HPC environments
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=D%27Amico%2C+M">Marco D&#39;Amico</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Corbalan%2C+J">Julita Corbalan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.12007v1-abstract-short" style="display: inline;">
        New HPC machines are getting close to the exascale. Power consumption for those machines has been increasing, and researchers are studying ways to <span class="search-hit mathjax">reduce</span> it. A second trend is HPC machines&#39; growing complexity, with increasing heterogeneous hardware components and different clusters architectures cooperating in the same machine. We refer to these environm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.12007v1-abstract-full').style.display = 'inline'; document.getElementById('2106.12007v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.12007v1-abstract-full" style="display: none;">
        New HPC machines are getting close to the exascale. Power consumption for those machines has been increasing, and researchers are studying ways to <span class="search-hit mathjax">reduce</span> it. A second trend is HPC machines&#39; growing complexity, with increasing heterogeneous hardware components and different clusters architectures cooperating in the same machine. We refer to these environments with the term heterogeneous multi-cluster environments. With the aim of <span class="search-hit mathjax">optimizing</span> <span class="search-hit mathjax">performance</span> and energy consumption in these environments, this paper proposes an Energy-Aware-Multi-Cluster (EAMC) job scheduling policy. EAMC-policy is able to <span class="search-hit mathjax">optimize</span> the scheduling and placement of jobs by predicting <span class="search-hit mathjax">performance</span> and energy consumption of arriving jobs for different hardware architectures and processor frequencies, <span class="search-hit mathjax">reducing</span> workload&#39;s energy consumption, makespan, and response time. The policy assigns a different priority to each job-resource combination so that the most <span class="search-hit mathjax">efficient</span> ones are favored, while less <span class="search-hit mathjax">efficient</span> ones are still considered on a variable degree, <span class="search-hit mathjax">reducing</span> response time and increasing cluster utilization. We implemented EAMC-policy in Slurm, and we evaluated a scenario in which two CPU clusters collaborate in the same machine. Simulations of workloads running <span class="search-hit mathjax">applications</span> modeled from real-world show a reduction of response time and makespan by up to 25% and 6% while saving up to 20% of total energy consumed when compared to policies minimizing runtime, and by 49%, 26%, and 6% compared to policies minimizing energy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.12007v1-abstract-full').style.display = 'none'; document.getElementById('2106.12007v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Transactions on Parallel and Distributed Systems 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.11851">arXiv:2106.11851</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.11851">pdf</a>, <a href="https://arxiv.org/format/2106.11851">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stochastic Polyak Stepsize with a Moving Target
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gower%2C+R+M">Robert M. Gower</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Defazio%2C+A">Aaron Defazio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rabbat%2C+M">Michael Rabbat</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.11851v1-abstract-short" style="display: inline;">
        We propose a new stochastic gradient method that uses recorded past loss values to <span class="search-hit mathjax">reduce</span> the variance. Our method can be interpreted as a new stochastic variant of the Polyak Stepsize that converges globally without assuming interpolation. Our method introduces auxiliary variables, one for each data point, that track the loss value for each data point. We p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.11851v1-abstract-full').style.display = 'inline'; document.getElementById('2106.11851v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.11851v1-abstract-full" style="display: none;">
        We propose a new stochastic gradient method that uses recorded past loss values to <span class="search-hit mathjax">reduce</span> the variance. Our method can be interpreted as a new stochastic variant of the Polyak Stepsize that converges globally without assuming interpolation. Our method introduces auxiliary variables, one for each data point, that track the loss value for each data point. We provide a global convergence theory for our method by showing that it can be interpreted as a special variant of online SGD. The new method only stores a single scalar per data point, opening up new <span class="search-hit mathjax">applications</span> for variance reduction where memory is the bottleneck.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.11851v1-abstract-full').style.display = 'none'; document.getElementById('2106.11851v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">41 pages, 13 figures, 1 table</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          90C53; 74S60; 90C06; 62L20; 68W20; 15B52; 65Y20; 68W40
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          G.1.6
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.11756">arXiv:2106.11756</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.11756">pdf</a>, <a href="https://arxiv.org/format/2106.11756">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Trinity: A No-<span class="search-hit mathjax">Code</span> AI platform for complex spatial datasets
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Iyer%2C+C+V+K">C. V. Krishnakumar Iyer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hou%2C+F">Feili Hou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Henry Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yonghong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oh%2C+K">Kay Oh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ganguli%2C+S">Swetava Ganguli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pandey%2C+V">Vipul Pandey</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.11756v5-abstract-short" style="display: inline;">
        We present a no-<span class="search-hit mathjax">code</span> Artificial Intelligence (AI) platform called Trinity with the main design goal of enabling both machine learning researchers and non-technical geospatial domain experts to experiment with domain-specific signals and datasets for solving a variety of complex problems on their own. This versatility to solve diverse problems is achieved by&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.11756v5-abstract-full').style.display = 'inline'; document.getElementById('2106.11756v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.11756v5-abstract-full" style="display: none;">
        We present a no-<span class="search-hit mathjax">code</span> Artificial Intelligence (AI) platform called Trinity with the main design goal of enabling both machine learning researchers and non-technical geospatial domain experts to experiment with domain-specific signals and datasets for solving a variety of complex problems on their own. This versatility to solve diverse problems is achieved by transforming complex Spatio-temporal datasets to make them consumable by standard deep learning models, in this case, Convolutional Neural Networks (CNNs), and giving the ability to formulate disparate problems in a standard way, eg. semantic segmentation. With an intuitive user interface, a feature store that hosts derivatives of complex feature engineering, a deep learning kernel, and a scalable data processing mechanism, Trinity provides a powerful platform for domain experts to share the stage with scientists and engineers in solving business-critical problems. It enables quick prototyping, rapid experimentation and <span class="search-hit mathjax">reduces</span> the time to production by standardizing model building and deployment. In this paper, we present our motivation behind Trinity and its design along with showcasing sample <span class="search-hit mathjax">applications</span> to motivate the idea of lowering the bar to using AI.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.11756v5-abstract-full').style.display = 'none'; document.getElementById('2106.11756v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.11396">arXiv:2106.11396</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.11396">pdf</a>, <a href="https://arxiv.org/ps/2106.11396">ps</a>, <a href="https://arxiv.org/format/2106.11396">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BiAdam: Fast Adaptive Bilevel <span class="search-hit mathjax">Optimization</span> Methods
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+F">Feihu Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">Heng Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.11396v2-abstract-short" style="display: inline;">
        Bilevel <span class="search-hit mathjax">optimization</span> recently has attracted increased interest in machine learning due to its many&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.11396v2-abstract-full').style.display = 'inline'; document.getElementById('2106.11396v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.11396v2-abstract-full" style="display: none;">
        Bilevel <span class="search-hit mathjax">optimization</span> recently has attracted increased interest in machine learning due to its many <span class="search-hit mathjax">applications</span> such as hyper-parameter <span class="search-hit mathjax">optimization</span> and policy <span class="search-hit mathjax">optimization</span>. Although some methods recently have been proposed to solve the bilevel problems, these methods do not consider using adaptive learning rates. To fill this gap, in the paper, we propose a class of fast and <span class="search-hit mathjax">effective</span> adaptive methods for solving bilevel <span class="search-hit mathjax">optimization</span> problems that the outer problem is possibly nonconvex and the inner problem is strongly-convex. Specifically, we propose a fast single-loop BiAdam algorithm based on the basic momentum technique, which achieves a sample complexity of $\tilde{O}(ε^{-4})$ for finding an $ε$-stationary point. At the same time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by using variance <span class="search-hit mathjax">reduced</span> technique, which reaches the best known sample complexity of $\tilde{O}(ε^{-3})$. To further <span class="search-hit mathjax">reduce</span> computation in estimating derivatives, we propose a fast single-loop stochastic approximated BiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still achieves a sample complexity of $\tilde{O}(ε^{-4})$ without large batches. We further present an accelerated version of saBiAdam algorithm (VR-saBiAdam), which also reaches the best known sample complexity of $\tilde{O}(ε^{-3})$. We apply the unified adaptive matrices to our methods as the SUPER-ADAM \citep{huang2021super}, which including many types of adaptive learning rates. Moreover, our framework can flexibly use the momentum and variance <span class="search-hit mathjax">reduced</span> techniques. In particular, we provide a useful convergence analysis framework for both the constrained and unconstrained bilevel <span class="search-hit mathjax">optimization</span>. To the best of our knowledge, we first study the adaptive bilevel <span class="search-hit mathjax">optimization</span> methods with adaptive learning rates.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.11396v2-abstract-full').style.display = 'none'; document.getElementById('2106.11396v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">66 pages, 2 tables. We add the detailed proofs</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.10423">arXiv:2106.10423</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.10423">pdf</a>, <a href="https://arxiv.org/format/2106.10423">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Speed Control and Energy Replenishment <span class="search-hit mathjax">Optimization</span> for UAV-assisted IoT Data Collection with Deep Reinforcement Transfer Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+N+H">Nam H. Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoang%2C+D+T">Dinh Thai Hoang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+D+N">Diep N. Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Van+Huynh%2C+N">Nguyen Van Huynh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dutkiewicz%2C+E">Eryk Dutkiewicz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.10423v1-abstract-short" style="display: inline;">
        Unmanned aerial vehicle (UAV)-assisted data collection has been emerging as a prominent <span class="search-hit mathjax">application</span> due to its flexibility, mobility, and low operational cost. However, under the dynamic and uncertainty of IoT data collection and energy replenishment processes,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.10423v1-abstract-full').style.display = 'inline'; document.getElementById('2106.10423v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.10423v1-abstract-full" style="display: none;">
        Unmanned aerial vehicle (UAV)-assisted data collection has been emerging as a prominent <span class="search-hit mathjax">application</span> due to its flexibility, mobility, and low operational cost. However, under the dynamic and uncertainty of IoT data collection and energy replenishment processes, <span class="search-hit mathjax">optimizing</span> the <span class="search-hit mathjax">performance</span> for UAV collectors is a very challenging task. Thus, this paper introduces a novel framework that jointly <span class="search-hit mathjax">optimizes</span> the flying speed and energy replenishment for each UAV to significantly <span class="search-hit mathjax">improve</span> the data collection <span class="search-hit mathjax">performance</span>. Specifically, we first develop a Markov decision process to help the UAV <span class="search-hit mathjax">automatically</span> and dynamically make <span class="search-hit mathjax">optimal</span> decisions under the dynamics and uncertainties of the environment. We then propose a highly-<span class="search-hit mathjax">effective</span> reinforcement learning algorithm leveraging deep Q-learning, double deep Q-learning, and a deep dueling neural network architecture to quickly obtain the UAV&#39;s <span class="search-hit mathjax">optimal</span> policy. The core ideas of this algorithm are to estimate the state values and action advantages separately and simultaneously and to employ double estimators for estimating the action values. Thus, these proposed techniques can stabilize the learning process and <span class="search-hit mathjax">effectively</span> address the overestimation problem of conventional Q-learning algorithms. To further <span class="search-hit mathjax">reduce</span> the learning time as well as significantly <span class="search-hit mathjax">improve</span> learning quality, we develop advanced transfer learning techniques to allow UAVs to ``share&#39;&#39; and ``transfer&#39;&#39; learning knowledge. Extensive simulations demonstrate that our proposed solution can <span class="search-hit mathjax">improve</span> the average data collection <span class="search-hit mathjax">performance</span> of the system up to 200% compared with those of current methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.10423v1-abstract-full').style.display = 'none'; document.getElementById('2106.10423v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.10022">arXiv:2106.10022</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.10022">pdf</a>, <a href="https://arxiv.org/format/2106.10022">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Local AdaGrad-Type Algorithm for Stochastic Convex-Concave Minimax Problems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liao%2C+L">Luofeng Liao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+L">Li Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+J">Jia Duan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kolar%2C+M">Mladen Kolar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tao%2C+D">Dacheng Tao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.10022v1-abstract-short" style="display: inline;">
        Large scale convex-concave minimax problems arise in numerous <span class="search-hit mathjax">applications</span>, including game theory, robust training, and training of generative adversarial networks. Despite their wide&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.10022v1-abstract-full').style.display = 'inline'; document.getElementById('2106.10022v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.10022v1-abstract-full" style="display: none;">
        Large scale convex-concave minimax problems arise in numerous <span class="search-hit mathjax">applications</span>, including game theory, robust training, and training of generative adversarial networks. Despite their wide <span class="search-hit mathjax">applicability</span>, solving such problems <span class="search-hit mathjax">efficiently</span> and <span class="search-hit mathjax">effectively</span> is challenging in the presence of large amounts of data using existing stochastic minimax methods. We study a class of stochastic minimax methods and develop a communication-<span class="search-hit mathjax">efficient</span> distributed stochastic extragradient algorithm, LocalAdaSEG, with an adaptive learning rate suitable for solving convex-concave minimax problem in the Parameter-Server model. LocalAdaSEG has three main features: (i) periodic communication strategy <span class="search-hit mathjax">reduces</span> the communication cost between workers and the server; (ii) an adaptive learning rate that is computed locally and allows for tuning-free implementation; and (iii) theoretically, a nearly linear speed-up with respect to the dominant variance term, arising from estimation of the stochastic gradient, is proven in both the smooth and nonsmooth convex-concave settings. LocalAdaSEG is used to solve a stochastic bilinear game, and train generative adversarial network. We compare LocalAdaSEG against several existing <span class="search-hit mathjax">optimizers</span> for minimax problems and demonstrate its efficacy through several experiments in both the homogeneous and heterogeneous settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.10022v1-abstract-full').style.display = 'none'; document.getElementById('2106.10022v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">24 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.09884">arXiv:2106.09884</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.09884">pdf</a>, <a href="https://arxiv.org/ps/2106.09884">ps</a>, <a href="https://arxiv.org/format/2106.09884">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Batch Multi-Fidelity Bayesian <span class="search-hit mathjax">Optimization</span> with Deep Auto-Regressive Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Shibo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kirby%2C+R+M">Robert M. Kirby</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhe%2C+S">Shandian Zhe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.09884v1-abstract-short" style="display: inline;">
        Bayesian <span class="search-hit mathjax">optimization</span> (BO) is a powerful approach for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.09884v1-abstract-full').style.display = 'inline'; document.getElementById('2106.09884v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.09884v1-abstract-full" style="display: none;">
        Bayesian <span class="search-hit mathjax">optimization</span> (BO) is a powerful approach for <span class="search-hit mathjax">optimizing</span> black-box, expensive-to-evaluate functions. To enable a flexible trade-off between the cost and <span class="search-hit mathjax">accuracy</span>, many <span class="search-hit mathjax">applications</span> allow the function to be evaluated at different fidelities. In order to <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">optimization</span> cost while maximizing the benefit-cost ratio, in this paper, we propose Batch Multi-fidelity Bayesian <span class="search-hit mathjax">Optimization</span> with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of Bayesian neural networks to construct a fully auto-regressive model, which is expressive enough to capture strong yet complex relationships across all the fidelities, so as to <span class="search-hit mathjax">improve</span> the surrogate learning and <span class="search-hit mathjax">optimization</span> <span class="search-hit mathjax">performance</span>. Furthermore, to enhance the quality and diversity of queries, we develop a simple yet <span class="search-hit mathjax">efficient</span> batch querying method, without any combinatorial search over the fidelities. We propose a batch acquisition function based on Max-value Entropy Search (MES) principle, which penalizes highly correlated queries and encourages diversity. We use posterior samples and moment matching to fulfill <span class="search-hit mathjax">efficient</span> computation of the acquisition function and conduct alternating <span class="search-hit mathjax">optimization</span> over every fidelity-input pair, which guarantees an <span class="search-hit mathjax">improvement</span> at each step. We demonstrate the advantage of our approach on four real-world hyperparameter <span class="search-hit mathjax">optimization</span> <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.09884v1-abstract-full').style.display = 'none'; document.getElementById('2106.09884v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.09877">arXiv:2106.09877</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.09877">pdf</a>, <a href="https://arxiv.org/format/2106.09877">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HIFIR: Hybrid Incomplete Factorization with Iterative Refinement for Preconditioning Ill-conditioned and Singular Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Qiao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiao%2C+X">Xiangmin Jiao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.09877v1-abstract-short" style="display: inline;">
        We introduce a <span class="search-hit mathjax">software</span> package called HIFIR for preconditioning sparse, unsymmetric, ill-conditioned, and potentially singular systems. HIFIR computes a hybrid incomplete factorization, which combines multilevel incomplete LU factorization with a truncated, rank-revealing QR factorization on the final Schur complement. This novel hybridization is based on t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.09877v1-abstract-full').style.display = 'inline'; document.getElementById('2106.09877v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.09877v1-abstract-full" style="display: none;">
        We introduce a <span class="search-hit mathjax">software</span> package called HIFIR for preconditioning sparse, unsymmetric, ill-conditioned, and potentially singular systems. HIFIR computes a hybrid incomplete factorization, which combines multilevel incomplete LU factorization with a truncated, rank-revealing QR factorization on the final Schur complement. This novel hybridization is based on the new theory of approximate generalized inverse and $ε$-<span class="search-hit mathjax">accuracy</span>. It enables near-<span class="search-hit mathjax">optimal</span> preconditioners for consistent systems and enables flexible GMRES to solve inconsistent systems when coupled with iterative refinement. In this paper, we focus on some practical algorithmic and <span class="search-hit mathjax">software</span> issues of HIFIR. In particular, we introduce a new inverse-based rook pivoting into ILU, which <span class="search-hit mathjax">improves</span> the robustness and the overall <span class="search-hit mathjax">efficiency</span> for some ill-conditioned systems by significantly <span class="search-hit mathjax">reducing</span> the size of the final Schur complement for some systems. We also describe the <span class="search-hit mathjax">software</span> design of HIFIR in terms of its <span class="search-hit mathjax">efficient</span> data structures for supporting rook pivoting in a multilevel setting, its template-based generic <span class="search-hit mathjax">programming</span> interfaces for mixed-<span class="search-hit mathjax">precision</span> real and complex values in C++, and its user-friendly high-level interfaces in MATLAB and Python. We demonstrate the <span class="search-hit mathjax">effectiveness</span> of HIFIR for ill-conditioned or singular systems arising from several <span class="search-hit mathjax">applications</span>, including the Helmholtz equation, linear elasticity, stationary incompressible Navier--Stokes equations, and time-dependent advection-diffusion equation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.09877v1-abstract-full').style.display = 'none'; document.getElementById('2106.09877v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ACM Transactions on Mathematical <span class="search-hit mathjax">Software</span> (TOMS)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.09375">arXiv:2106.09375</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.09375">pdf</a>, <a href="https://arxiv.org/format/2106.09375">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Recovery under Side Constraints
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ardah%2C+K">Khaled Ardah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haardt%2C+M">Martin Haardt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tianyi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matter%2C+F">Frederic Matter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pesavento%2C+M">Marius Pesavento</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pfetsch%2C+M+E">Marc E. Pfetsch</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.09375v1-abstract-short" style="display: inline;">
        This paper addresses sparse signal reconstruction under various types of structural side constraints with <span class="search-hit mathjax">applications</span> in multi-antenna systems. Side constraints may result from prior information on the measurement system and the sparse signal structure. They may involve the structure of the sensing matrix, the structure of the non-zero support values, the t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.09375v1-abstract-full').style.display = 'inline'; document.getElementById('2106.09375v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.09375v1-abstract-full" style="display: none;">
        This paper addresses sparse signal reconstruction under various types of structural side constraints with <span class="search-hit mathjax">applications</span> in multi-antenna systems. Side constraints may result from prior information on the measurement system and the sparse signal structure. They may involve the structure of the sensing matrix, the structure of the non-zero support values, the temporal structure of the sparse representationvector, and the nonlinear measurement structure. First, we demonstrate how a priori information in form of structural side constraints influence recovery guarantees (null space properties) using L1-minimization. Furthermore, for constant modulus signals, signals with row-, block- and rank-sparsity, as well as non-circular signals, we illustrate how structural prior information can be used to devise <span class="search-hit mathjax">efficient</span> algorithms with <span class="search-hit mathjax">improved</span> recovery <span class="search-hit mathjax">performance</span> and <span class="search-hit mathjax">reduced</span> computational complexity. Finally, we address the measurement system design for linear and nonlinear measurements of sparse signals. Moreover, we discuss the linear mixing matrix design based on coherence minimization. Then we extend our focus to nonlinear measurement systems where we design parallel <span class="search-hit mathjax">optimization</span> algorithms to <span class="search-hit mathjax">efficiently</span> compute stationary points in the sparse phase retrieval problem with and without dictionary learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.09375v1-abstract-full').style.display = 'none'; document.getElementById('2106.09375v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.08482">arXiv:2106.08482</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.08482">pdf</a>, <a href="https://arxiv.org/format/2106.08482">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Minimizing Communication while Maximizing <span class="search-hit mathjax">Performance</span> in Multi-Agent Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Vijay%2C+V+K">Varun Kumar Vijay</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sheikh%2C+H">Hassam Sheikh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Majumdar%2C+S">Somdeb Majumdar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Phielipp%2C+M">Mariano Phielipp</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.08482v2-abstract-short" style="display: inline;">
        Inter-agent communication can significantly increase <span class="search-hit mathjax">performance</span> in multi-agent tasks that require co-ordination to achieve a shared goal. Prior work has shown that it is possible to learn inter-agent communication protocols using multi-agent reinforcement learning and message-passing network architectures. However, these models use an unconstrained broadcas&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08482v2-abstract-full').style.display = 'inline'; document.getElementById('2106.08482v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.08482v2-abstract-full" style="display: none;">
        Inter-agent communication can significantly increase <span class="search-hit mathjax">performance</span> in multi-agent tasks that require co-ordination to achieve a shared goal. Prior work has shown that it is possible to learn inter-agent communication protocols using multi-agent reinforcement learning and message-passing network architectures. However, these models use an unconstrained broadcast communication model, in which an agent communicates with all other agents at every step, even when the task does not require it. In real-world <span class="search-hit mathjax">applications</span>, where communication may be limited by system constraints like bandwidth, power and network capacity, one might need to <span class="search-hit mathjax">reduce</span> the number of messages that are sent. In this work, we explore a simple method of minimizing communication while maximizing <span class="search-hit mathjax">performance</span> in multi-task learning: simultaneously <span class="search-hit mathjax">optimizing</span> a task-specific objective and a communication penalty. We show that the objectives can be <span class="search-hit mathjax">optimized</span> using Reinforce and the Gumbel-Softmax reparameterization. We introduce two techniques to stabilize training: 50% training and message forwarding. Training with the communication penalty on only 50% of the episodes prevents our models from turning off their outgoing messages. Second, repeating messages received previously helps models retain information, and further <span class="search-hit mathjax">improves</span> <span class="search-hit mathjax">performance</span>. With these techniques, we show that we can <span class="search-hit mathjax">reduce</span> communication by 75% with no loss of <span class="search-hit mathjax">performance</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08482v2-abstract-full').style.display = 'none'; document.getElementById('2106.08482v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.08301">arXiv:2106.08301</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.08301">pdf</a>, <a href="https://arxiv.org/format/2106.08301">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Efficient</span> Micro-Structured Weight Unification and Pruning for Neural Network Compression
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+S">Sheng Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+W">Wei Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+K">Kaidi Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yanzhi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Songnan Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.08301v2-abstract-short" style="display: inline;">
        Compressing Deep Neural Network (DNN) models to alleviate the storage and computation requirements is essential for practical <span class="search-hit mathjax">applications</span>, especially for resource limited devices. Although capable of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08301v2-abstract-full').style.display = 'inline'; document.getElementById('2106.08301v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.08301v2-abstract-full" style="display: none;">
        Compressing Deep Neural Network (DNN) models to alleviate the storage and computation requirements is essential for practical <span class="search-hit mathjax">applications</span>, especially for resource limited devices. Although capable of <span class="search-hit mathjax">reducing</span> a reasonable amount of model parameters, previous unstructured or structured weight pruning methods can hardly truly accelerate inference, either due to the poor hardware compatibility of the unstructured sparsity or due to the low sparse rate of the structurally pruned network. Aiming at <span class="search-hit mathjax">reducing</span> both storage and computation, as well as preserving the original task <span class="search-hit mathjax">performance</span>, we propose a generalized weight unification framework at a hardware compatible micro-structured level to achieve high amount of compression and acceleration. Weight coefficients of a selected micro-structured block are unified to <span class="search-hit mathjax">reduce</span> the storage and computation of the block without changing the neuron connections, which turns to a micro-structured pruning special case when all unified coefficients are set to zero, where neuron connections (hence storage and computation) are completely removed. In addition, we developed an <span class="search-hit mathjax">effective</span> training framework based on the alternating direction method of multipliers (ADMM), which converts our complex constrained <span class="search-hit mathjax">optimization</span> into separately solvable subproblems. Through iteratively <span class="search-hit mathjax">optimizing</span> the subproblems, the desired micro-structure can be ensured with high compression ratio and low <span class="search-hit mathjax">performance</span> degradation. We extensively evaluated our method using a variety of benchmark models and datasets for different <span class="search-hit mathjax">applications</span>. Experimental results demonstrate state-of-the-art <span class="search-hit mathjax">performance</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08301v2-abstract-full').style.display = 'none'; document.getElementById('2106.08301v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 3 figures and 5 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.08253">arXiv:2106.08253</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.08253">pdf</a>, <a href="https://arxiv.org/ps/2106.08253">ps</a>, <a href="https://arxiv.org/format/2106.08253">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Code</span> Generation Based on Deep Learning: a Brief Review
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Q">Qihao Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wenjie Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.08253v4-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Automatic</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08253v4-abstract-full').style.display = 'inline'; document.getElementById('2106.08253v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.08253v4-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Automatic</span> <span class="search-hit mathjax">software</span> development has been a research hot spot in the field of <span class="search-hit mathjax">software</span> engineering (SE) in the past decade. In particular, deep learning (DL) has been applied and achieved a lot of progress in various SE tasks. Among all <span class="search-hit mathjax">applications</span>, <span class="search-hit mathjax">automatic</span> <span class="search-hit mathjax">code</span> generation by machines as a general concept, including <span class="search-hit mathjax">code</span> completion and <span class="search-hit mathjax">code</span> synthesis, is a common expectation in the field of SE, which may greatly <span class="search-hit mathjax">reduce</span> the development burden of the <span class="search-hit mathjax">software</span> developers and <span class="search-hit mathjax">improves</span> the <span class="search-hit mathjax">efficiency</span> and quality of the <span class="search-hit mathjax">software</span> development process to a certain extent. <span class="search-hit mathjax">Code</span> completion is an important part of modern integrated development environments (IDEs). <span class="search-hit mathjax">Code</span> completion technology <span class="search-hit mathjax">effectively</span> helps programmers complete <span class="search-hit mathjax">code</span> class names, method names, and key-words, etc., which <span class="search-hit mathjax">improves</span> the <span class="search-hit mathjax">efficiency</span> of <span class="search-hit mathjax">program</span> development and <span class="search-hit mathjax">reduces</span> spelling errors in the <span class="search-hit mathjax">coding</span> process. Such tools use static analysis on the <span class="search-hit mathjax">code</span> and provide candidates for completion arranged in alphabetical order. <span class="search-hit mathjax">Code</span> synthesis is implemented from two aspects, one based on input-output samples and the other based on functionality description. In this study, we introduce existing techniques of these two aspects and the corresponding DL techniques, and present some possible future research directions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08253v4-abstract-full').style.display = 'none'; document.getElementById('2106.08253v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.08122">arXiv:2106.08122</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.08122">pdf</a>, <a href="https://arxiv.org/format/2106.08122">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sequence-Level Training for Non-Autoregressive Neural Machine Translation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shao%2C+C">Chenze Shao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+Y">Yang Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jinchao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+F">Fandong Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jie Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.08122v1-abstract-short" style="display: inline;">
        &hellip;tasks. However, the word-by-word generation manner determined by the autoregressive mechanism leads to high translation latency of the NMT and restricts its low-latency <span class="search-hit mathjax">applications</span>. Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive mechanism and achieves significant decoding speedup through generating target words independently&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08122v1-abstract-full').style.display = 'inline'; document.getElementById('2106.08122v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.08122v1-abstract-full" style="display: none;">
        In recent years, Neural Machine Translation (NMT) has achieved notable results in various translation tasks. However, the word-by-word generation manner determined by the autoregressive mechanism leads to high translation latency of the NMT and restricts its low-latency <span class="search-hit mathjax">applications</span>. Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive mechanism and achieves significant decoding speedup through generating target words independently and simultaneously. Nevertheless, NAT still takes the word-level cross-entropy loss as the training objective, which is not <span class="search-hit mathjax">optimal</span> because the output of NAT cannot be properly evaluated due to the multimodality problem. In this paper, we propose using sequence-level training objectives to train NAT models, which evaluate the NAT outputs as a whole and correlates well with the real translation quality. Firstly, we propose training NAT models to <span class="search-hit mathjax">optimize</span> sequence-level evaluation metrics (e.g., BLEU) based on several novel reinforcement algorithms customized for NAT, which outperforms the conventional method by <span class="search-hit mathjax">reducing</span> the variance of gradient estimation. Secondly, we introduce a novel training objective for NAT models, which aims to minimize the Bag-of-Ngrams (BoN) difference between the model output and the reference sentence. The BoN training objective is differentiable and can be calculated <span class="search-hit mathjax">efficiently</span> without doing any approximations. Finally, we apply a three-stage training strategy to combine these two methods to train the NAT model. We validate our approach on four translation tasks (WMT14 En$\leftrightarrow$De, WMT16 En$\leftrightarrow$Ro), which shows that our approach largely outperforms NAT baselines and achieves remarkable <span class="search-hit mathjax">performance</span> on all translation tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08122v1-abstract-full').style.display = 'none'; document.getElementById('2106.08122v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.07827">arXiv:2106.07827</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.07827">pdf</a>, <a href="https://arxiv.org/format/2106.07827">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Improving</span> the compromise between <span class="search-hit mathjax">accuracy</span>, interpretability and personalization of rule-based machine learning in medical problems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Valente%2C+F">Francisco Valente</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paredes%2C+S">Simao Paredes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Henriques%2C+J">Jorge Henriques</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.07827v1-abstract-short" style="display: inline;">
        One of the key challenges when developing a predictive model is the capability to describe the domain knowledge and the cause-<span class="search-hit mathjax">effect</span> relationships in a simple way. Decision rules are a useful and important methodology in this context, justifying their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.07827v1-abstract-full').style.display = 'inline'; document.getElementById('2106.07827v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.07827v1-abstract-full" style="display: none;">
        One of the key challenges when developing a predictive model is the capability to describe the domain knowledge and the cause-<span class="search-hit mathjax">effect</span> relationships in a simple way. Decision rules are a useful and important methodology in this context, justifying their <span class="search-hit mathjax">application</span> in several areas, in particular in clinical practice. Several machine-learning classifiers have exploited the advantageous properties of decision rules to build intelligent prediction models, namely decision trees and ensembles of trees (ETs). However, such methodologies usually suffer from a trade-off between interpretability and predictive <span class="search-hit mathjax">performance</span>. Some procedures consider a simplification of ETs, using heuristic approaches to select an <span class="search-hit mathjax">optimal</span> <span class="search-hit mathjax">reduced</span> set of decision rules. In this paper, we introduce a novel step to those methodologies. We create a new component to predict if a given rule will be correct or not for a particular patient, which introduces personalization into the procedure. Furthermore, the validation results using three public clinical datasets show that it also allows to increase the predictive <span class="search-hit mathjax">performance</span> of the selected set of rules, <span class="search-hit mathjax">improving</span> the mentioned trade-off.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.07827v1-abstract-full').style.display = 'none'; document.getElementById('2106.07827v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.07537">arXiv:2106.07537</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.07537">pdf</a>, <a href="https://arxiv.org/format/2106.07537">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Wasserstein Minimax Framework for Mixed Linear Regression
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Diamandis%2C+T">Theo Diamandis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Eldar%2C+Y+C">Yonina C. Eldar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fallah%2C+A">Alireza Fallah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Farnia%2C+F">Farzan Farnia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ozdaglar%2C+A">Asuman Ozdaglar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.07537v2-abstract-short" style="display: inline;">
        &hellip;distributions are commonly used to model clustered data in statistical learning tasks. In this paper, we consider the Mixed Linear Regression (MLR) problem. We propose an <span class="search-hit mathjax">optimal</span> transport-based framework for MLR problems, Wasserstein Mixed Linear Regression (WMLR), which minimizes the Wasserstein distance between the learned and target mixture regression mo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.07537v2-abstract-full').style.display = 'inline'; document.getElementById('2106.07537v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.07537v2-abstract-full" style="display: none;">
        Multi-modal distributions are commonly used to model clustered data in statistical learning tasks. In this paper, we consider the Mixed Linear Regression (MLR) problem. We propose an <span class="search-hit mathjax">optimal</span> transport-based framework for MLR problems, Wasserstein Mixed Linear Regression (WMLR), which minimizes the Wasserstein distance between the learned and target mixture regression models. Through a model-based duality analysis, WMLR <span class="search-hit mathjax">reduces</span> the underlying MLR task to a nonconvex-concave minimax <span class="search-hit mathjax">optimization</span> problem, which can be provably solved to find a minimax stationary point by the Gradient Descent Ascent (GDA) algorithm. In the special case of mixtures of two linear regression models, we show that WMLR enjoys global convergence and generalization guarantees. We prove that WMLR&#39;s sample complexity grows linearly with the dimension of data. Finally, we discuss the <span class="search-hit mathjax">application</span> of WMLR to the federated learning task where the training samples are collected by multiple agents in a network. Unlike the Expectation Maximization algorithm, WMLR directly extends to the distributed, federated learning setting. We support our theoretical results through several numerical experiments, which highlight our framework&#39;s ability to handle the federated learning setting with mixture models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.07537v2-abstract-full').style.display = 'none'; document.getElementById('2106.07537v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in 38th International Conference on Machine Learning (ICML 2021)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.07520">arXiv:2106.07520</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.07520">pdf</a>, <a href="https://arxiv.org/format/2106.07520">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        JUGE: An Infrastructure for Benchmarking Java Unit Test Generators
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Devroey%2C+X">Xavier Devroey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gambi%2C+A">Alessio Gambi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Galeotti%2C+J+P">Juan Pablo Galeotti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Just%2C+R">René Just</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kifetew%2C+F">Fitsum Kifetew</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Panichella%2C+A">Annibale Panichella</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Panichella%2C+S">Sebastiano Panichella</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.07520v1-abstract-short" style="display: inline;">
        Researchers and practitioners have designed and implemented various <span class="search-hit mathjax">automated</span> test case generators to support&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.07520v1-abstract-full').style.display = 'inline'; document.getElementById('2106.07520v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.07520v1-abstract-full" style="display: none;">
        Researchers and practitioners have designed and implemented various <span class="search-hit mathjax">automated</span> test case generators to support <span class="search-hit mathjax">effective</span> <span class="search-hit mathjax">software</span> testing. Such generators exist for various languages (e.g., Java, C#, or Python) and for various platforms (e.g., desktop, web, or mobile <span class="search-hit mathjax">applications</span>). Such generators exhibit varying <span class="search-hit mathjax">effectiveness</span> and <span class="search-hit mathjax">efficiency</span>, depending on the testing goals they aim to satisfy (e.g., unit-testing of libraries vs. system-testing of entire <span class="search-hit mathjax">applications</span>) and the underlying techniques they implement. In this context, practitioners need to be able to compare different generators to identify the most suited one for their requirements, while researchers seek to identify future research directions. This can be achieved through the systematic execution of large-scale evaluations of different generators. However, the execution of such empirical evaluations is not trivial and requires a substantial effort to collect benchmarks, setup the evaluation infrastructure, and collect and analyse the results. In this paper, we present our JUnit Generation benchmarking infrastructure (JUGE) supporting generators (e.g., search-based, random-based, symbolic execution, etc.) seeking to <span class="search-hit mathjax">automate</span> the production of unit tests for various purposes (e.g., validation, regression testing, fault localization, etc.). The primary goal is to <span class="search-hit mathjax">reduce</span> the overall effort, ease the comparison of several generators, and enhance the knowledge transfer between academia and industry by standardizing the evaluation and comparison process. Since 2013, eight editions of a unit testing tool competition, co-located with the Search-Based <span class="search-hit mathjax">Software</span> Testing Workshop, have taken place and used and updated JUGE. As a result, an increasing amount of tools (over ten) from both academia and industry have been evaluated on JUGE, matured over the years, and allowed the identification of future research directions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.07520v1-abstract-full').style.display = 'none'; document.getElementById('2106.07520v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.07243">arXiv:2106.07243</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.07243">pdf</a>, <a href="https://arxiv.org/ps/2106.07243">ps</a>, <a href="https://arxiv.org/format/2106.07243">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Compressed Gradient Tracking for Decentralized <span class="search-hit mathjax">Optimization</span> Over General Directed Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Z">Zhuoqing Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+L">Lei Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pu%2C+S">Shi Pu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+M">Ming Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.07243v1-abstract-short" style="display: inline;">
        In this paper, we propose two communication-<span class="search-hit mathjax">efficient</span> algorithms for decentralized&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.07243v1-abstract-full').style.display = 'inline'; document.getElementById('2106.07243v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.07243v1-abstract-full" style="display: none;">
        In this paper, we propose two communication-<span class="search-hit mathjax">efficient</span> algorithms for decentralized <span class="search-hit mathjax">optimization</span> over a multi-agent network with general directed network topology. In the first part, we consider a novel communication-<span class="search-hit mathjax">efficient</span> gradient tracking based method, termed Compressed Push-Pull (CPP), which combines the Push-Pull method with communication compression. We show that CPP is <span class="search-hit mathjax">applicable</span> to a general class of unbiased compression operators and achieves linear convergence for strongly convex and smooth objective functions. In the second part, we propose a broadcast-like version of CPP (B-CPP), which also achieves linear convergence rate under the same conditions for the objective functions. B-CPP can be applied in an asynchronous broadcast setting and further <span class="search-hit mathjax">reduce</span> communication costs compared to CPP. Numerical experiments complement the theoretical analysis and confirm the <span class="search-hit mathjax">effectiveness</span> of the proposed methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.07243v1-abstract-full').style.display = 'none'; document.getElementById('2106.07243v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">working paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.04916">arXiv:2106.04916</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.04916">pdf</a>, <a href="https://arxiv.org/format/2106.04916">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Erratum: Leveraging Flexible Tree Matching to Repair Broken Locators in Web <span class="search-hit mathjax">Automation</span> Scripts
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Brisset%2C+S">Sacha Brisset</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rouvoy%2C+R">Romain Rouvoy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Seinturier%2C+L">Lionel Seinturier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pawlak%2C+R">Renaud Pawlak</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.04916v1-abstract-short" style="display: inline;">
        Web <span class="search-hit mathjax">applications</span> are constantly evolving to integrate new features and fix reported bugs. Even an imperceptible change can sometimes entail significant modifications of the Document Object Model (DOM), which is the underlying model used by browsers to render all the elements included in a web&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04916v1-abstract-full').style.display = 'inline'; document.getElementById('2106.04916v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.04916v1-abstract-full" style="display: none;">
        Web <span class="search-hit mathjax">applications</span> are constantly evolving to integrate new features and fix reported bugs. Even an imperceptible change can sometimes entail significant modifications of the Document Object Model (DOM), which is the underlying model used by browsers to render all the elements included in a web <span class="search-hit mathjax">application</span>. Scripts that interact with web <span class="search-hit mathjax">applications</span> (e.g. web test scripts, crawlers, or robotic process <span class="search-hit mathjax">automation</span>) rely on this continuously evolving DOM which means they are often particularly fragile. More <span class="search-hit mathjax">precisely</span>, the major cause of breakages observed in <span class="search-hit mathjax">automation</span> scripts are element locators, which are identifiers used by <span class="search-hit mathjax">automation</span> scripts to navigate across the DOM. When the DOM evolves, these identifiers tend to break, thus causing the related scripts to no longer locate the intended target elements. For this reason, several contributions explored the idea of <span class="search-hit mathjax">automatically</span> repairing broken locators on a page. These works attempt to repair a given broken locator by scanning all elements in the new DOM to find the most similar one. Unfortunately, this approach fails to scale when the complexity of web pages grows, leading either to long computation times or incorrect element repairs. This article, therefore, adopts a different perspective on this problem by introducing a new locator repair solution that leverages tree matching algorithms to relocate broken locators. This solution, named Erratum, implements a holistic approach to <span class="search-hit mathjax">reduce</span> the element search space, which greatly eases the locator repair task and drastically <span class="search-hit mathjax">improves</span> repair <span class="search-hit mathjax">accuracy</span>. We compare the robustness of Erratum on a large-scale benchmark composed of realistic and synthetic mutations applied to popular web <span class="search-hit mathjax">applications</span> currently deployed in production. Our empirical results demonstrate that Erratum outperforms the <span class="search-hit mathjax">accuracy</span> of WATER, a state-of-the-art solution, by 67%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04916v1-abstract-full').style.display = 'none'; document.getElementById('2106.04916v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.04729">arXiv:2106.04729</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.04729">pdf</a>, <a href="https://arxiv.org/format/2106.04729">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Drones for Medical Delivery Considering Different Demands Classes: A Markov Decision Process Approach for Managing Health Centers Dispatching Medical Products
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Asadi%2C+A">Amin Asadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pinkley%2C+S+N">Sarah Nurre Pinkley</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.04729v1-abstract-short" style="display: inline;">
        We consider the problem of <span class="search-hit mathjax">optimizing</span> the distribution operations of a hub using drones to deliver medical supplies to different geographic regions. Drones are an innovative method with many benefits including low-contact delivery thereby&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04729v1-abstract-full').style.display = 'inline'; document.getElementById('2106.04729v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.04729v1-abstract-full" style="display: none;">
        We consider the problem of <span class="search-hit mathjax">optimizing</span> the distribution operations of a hub using drones to deliver medical supplies to different geographic regions. Drones are an innovative method with many benefits including low-contact delivery thereby <span class="search-hit mathjax">reducing</span> the spread of pandemic and vaccine-preventable diseases. While we focus on medical supply delivery for this work, it is <span class="search-hit mathjax">applicable</span> to drone delivery for many other <span class="search-hit mathjax">applications</span>, including food, postal items, and e-commerce delivery. In this paper, our goal is to address drone delivery challenges by <span class="search-hit mathjax">optimizing</span> the distribution operations at a drone hub that dispatch drones to different geographic locations generating stochastic demands for medical supplies. By considering different geographic locations, we consider different classes of demand that require different flight ranges, which is directly related to the amount of charge held in a drone battery. We classify the stochastic demands based on their distance from the drone hub, use a Markov decision process to model the problem, and <span class="search-hit mathjax">perform</span> computational tests using realistic data representing a prominent drone delivery company. We solve the problem using a reinforcement learning method and show its high <span class="search-hit mathjax">performance</span> compared with the exact solution found using dynamic <span class="search-hit mathjax">programming</span>. Finally, we analyze the results and provide insights for managing the drone hub operations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04729v1-abstract-full').style.display = 'none'; document.getElementById('2106.04729v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.04618">arXiv:2106.04618</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.04618">pdf</a>, <a href="https://arxiv.org/format/2106.04618">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EXPObench: Benchmarking Surrogate-based Optimisation Algorithms on Expensive Black-box Functions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bliek%2C+L">Laurens Bliek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guijt%2C+A">Arthur Guijt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Karlsson%2C+R">Rickard Karlsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verwer%2C+S">Sicco Verwer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Weerdt%2C+M">Mathijs de Weerdt</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.04618v1-abstract-short" style="display: inline;">
        &hellip;the literature, these algorithms are usually evaluated with synthetic benchmarks which are well established but have no expensive objective, and only on one or two real-life <span class="search-hit mathjax">applications</span> which vary wildly between papers. There is a clear lack of standardisation when it comes to benchmarking surrogate algorithms on real-life, expensive, black-box objective fu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04618v1-abstract-full').style.display = 'inline'; document.getElementById('2106.04618v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.04618v1-abstract-full" style="display: none;">
        Surrogate algorithms such as Bayesian optimisation are especially designed for black-box optimisation problems with expensive objectives, such as hyperparameter tuning or simulation-based optimisation. In the literature, these algorithms are usually evaluated with synthetic benchmarks which are well established but have no expensive objective, and only on one or two real-life <span class="search-hit mathjax">applications</span> which vary wildly between papers. There is a clear lack of standardisation when it comes to benchmarking surrogate algorithms on real-life, expensive, black-box objective functions. This makes it very difficult to draw conclusions on the <span class="search-hit mathjax">effect</span> of algorithmic contributions. A new benchmark library, EXPObench, provides first steps towards such a standardisation. The library is used to provide an extensive comparison of six different surrogate algorithms on four expensive optimisation problems from different real-life <span class="search-hit mathjax">applications</span>. This has led to new insights regarding the relative importance of exploration, the evaluation time of the objective, and the used model. A further contribution is that we make the algorithms and benchmark problem instances publicly available, contributing to more uniform analysis of surrogate algorithms. Most importantly, we include the <span class="search-hit mathjax">performance</span> of the six algorithms on all evaluated problem instances. This results in a unique new dataset that lowers the bar for researching new methods as the number of expensive evaluations required for comparison is significantly <span class="search-hit mathjax">reduced</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04618v1-abstract-full').style.display = 'none'; document.getElementById('2106.04618v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.04508">arXiv:2106.04508</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.04508">pdf</a>, <a href="https://arxiv.org/format/2106.04508">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Energy-<span class="search-hit mathjax">Efficient</span> Adaptive System Reconfiguration for Dynamic Deadlines in Autonomous Driving
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yi%2C+S">Saehanseul Yi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+T">Tae-Wook Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Jong-Chan Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dutt%2C+N">Nikil Dutt</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.04508v1-abstract-short" style="display: inline;">
        The increasing computing demands of autonomous driving <span class="search-hit mathjax">applications</span> make energy&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04508v1-abstract-full').style.display = 'inline'; document.getElementById('2106.04508v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.04508v1-abstract-full" style="display: none;">
        The increasing computing demands of autonomous driving <span class="search-hit mathjax">applications</span> make energy <span class="search-hit mathjax">optimizations</span> critical for <span class="search-hit mathjax">reducing</span> battery capacity and vehicle weight. Current energy <span class="search-hit mathjax">optimization</span> methods typically target traditional real-time systems with static deadlines, resulting in conservative energy savings that are unable to exploit additional energy <span class="search-hit mathjax">optimizations</span> due to dynamic deadlines arising from the vehicle&#39;s change in velocity and driving context. We present an adaptive system <span class="search-hit mathjax">optimization</span> and reconfiguration approach that dynamically adapts the scheduling parameters and processor speeds to satisfy dynamic deadlines while consuming as little energy as possible. Our experimental results with an autonomous driving task set from Bosch and real-world driving data show energy reductions up to 46.4% on average in typical dynamic driving scenarios compared with traditional static energy <span class="search-hit mathjax">optimization</span> methods, demonstrating great potential for dynamic energy <span class="search-hit mathjax">optimization</span> gains by exploiting dynamic deadlines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04508v1-abstract-full').style.display = 'none'; document.getElementById('2106.04508v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE ISORC 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.04217">arXiv:2106.04217</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.04217">pdf</a>, <a href="https://arxiv.org/format/2106.04217">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Sparse Training for Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sokar%2C+G">Ghada Sokar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mocanu%2C+E">Elena Mocanu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mocanu%2C+D+C">Decebal Constantin Mocanu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pechenizkiy%2C+M">Mykola Pechenizkiy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stone%2C+P">Peter Stone</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.04217v1-abstract-short" style="display: inline;">
        &hellip;has achieved significant success in many decision-making tasks in various fields. However, it requires a large training time of dense neural networks to obtain a good <span class="search-hit mathjax">performance</span>. This hinders its&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04217v1-abstract-full').style.display = 'inline'; document.getElementById('2106.04217v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.04217v1-abstract-full" style="display: none;">
        Deep reinforcement learning has achieved significant success in many decision-making tasks in various fields. However, it requires a large training time of dense neural networks to obtain a good <span class="search-hit mathjax">performance</span>. This hinders its <span class="search-hit mathjax">applicability</span> on low-resource devices where memory and computation are strictly constrained. In a step towards enabling deep reinforcement learning agents to be applied to low-resource devices, in this work, we propose for the first time to dynamically train deep reinforcement learning agents with sparse neural networks from scratch. We adopt the evolution principles of dynamic sparse training in the reinforcement learning paradigm and introduce a training algorithm that <span class="search-hit mathjax">optimizes</span> the sparse topology and the weight values jointly to dynamically fit the incoming data. Our approach is easy to be integrated into existing deep reinforcement learning algorithms and has many favorable advantages. First, it allows for significant compression of the network size which <span class="search-hit mathjax">reduces</span> the memory and computation costs substantially. This would accelerate not only the agent inference but also its training process. Second, it speeds up the agent learning process and allows for <span class="search-hit mathjax">reducing</span> the number of required training steps. Third, it can achieve higher <span class="search-hit mathjax">performance</span> than training the dense counterpart network. We evaluate our approach on OpenAI gym continuous control tasks. The experimental results show the <span class="search-hit mathjax">effectiveness</span> of our approach in achieving higher <span class="search-hit mathjax">performance</span> than one of the state-of-art baselines with a 50\% reduction in the network size and floating-point operations (FLOPs). Moreover, our proposed approach can reach the same <span class="search-hit mathjax">performance</span> achieved by the dense network with a 40-50\% reduction in the number of training steps.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04217v1-abstract-full').style.display = 'none'; document.getElementById('2106.04217v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.03799">arXiv:2106.03799</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.03799">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deterministic Iteratively Built KD-Tree with KNN Search for Exact <span class="search-hit mathjax">Applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Naim%2C+A">Aryan Naim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bowkett%2C+J">Joseph Bowkett</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Karumanchi%2C+S">Sisir Karumanchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tavallali%2C+P">Peyman Tavallali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kennedy%2C+B">Brett Kennedy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.03799v1-abstract-short" style="display: inline;">
        K-Nearest Neighbors (KNN) search is a fundamental algorithm in artificial intelligence <span class="search-hit mathjax">software</span> with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03799v1-abstract-full').style.display = 'inline'; document.getElementById('2106.03799v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.03799v1-abstract-full" style="display: none;">
        K-Nearest Neighbors (KNN) search is a fundamental algorithm in artificial intelligence <span class="search-hit mathjax">software</span> with <span class="search-hit mathjax">applications</span> in robotics, and autonomous vehicles. These wide-ranging <span class="search-hit mathjax">applications</span> utilize KNN either directly for simple classification or combine KNN results as input to other algorithms such as Locally Weighted Learning (LWL). Similar to binary trees, kd-trees become unbalanced as new data is added in online <span class="search-hit mathjax">applications</span> which can lead to rapid degradation in search <span class="search-hit mathjax">performance</span> unless the tree is rebuilt. Although approximate methods are suitable for graphics <span class="search-hit mathjax">applications</span>, which prioritize query speed over query <span class="search-hit mathjax">accuracy</span>, they are unsuitable for certain <span class="search-hit mathjax">applications</span> in autonomous systems, aeronautics, and robotic manipulation where exact solutions are desired. In this paper, we will attempt to assess the <span class="search-hit mathjax">performance</span> of non-recursive deterministic kd-tree functions and KNN functions. We will also present a &#34;forest of interval kd-trees&#34; which <span class="search-hit mathjax">reduces</span> the number of tree rebuilds, without compromising the exactness of query results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03799v1-abstract-full').style.display = 'none'; document.getElementById('2106.03799v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.03368">arXiv:2106.03368</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.03368">pdf</a>, <a href="https://arxiv.org/format/2106.03368">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/978-3-319-64119-5_14">10.1007/978-3-319-64119-5_14 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Verification of Component Fault Trees Using Error <span class="search-hit mathjax">Effect</span> Simulations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Reiter%2C+S">Sebastian Reiter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeller%2C+M">Marc Zeller</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoefig%2C+K">Kai Hoefig</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Viehl%2C+A">Alexander Viehl</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bringmann%2C+O">Oliver Bringmann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosenstiel%2C+W">Wolfgang Rosenstiel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.03368v1-abstract-short" style="display: inline;">
        &hellip;effort for safety assurance. The reduction of development costs and time-to-market, while guaranteeing safe operation, is therefore a major challenge. In order to enable <span class="search-hit mathjax">efficient</span> safety assessment of complex architectures, we present an approach, which combines deductive safety analyses, in form of Component Fault Trees (CFTs), with an Error&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03368v1-abstract-full').style.display = 'inline'; document.getElementById('2106.03368v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.03368v1-abstract-full" style="display: none;">
        The growing complexity of safety-relevant systems causes an increasing effort for safety assurance. The reduction of development costs and time-to-market, while guaranteeing safe operation, is therefore a major challenge. In order to enable <span class="search-hit mathjax">efficient</span> safety assessment of complex architectures, we present an approach, which combines deductive safety analyses, in form of Component Fault Trees (CFTs), with an Error <span class="search-hit mathjax">Effect</span> Simulation (EES) for sanity checks. The combination <span class="search-hit mathjax">reduces</span> the drawbacks of both analyses, such as the subjective failure propagation assumptions in the CFTs or the determination of relevant fault scenarios for the EES. Both CFTs and the EES provide a modular, reusable and compositional safety analysis and are <span class="search-hit mathjax">applicable</span> throughout the whole design process. They support continuous model refinement and the reuse of conducted safety analysis and simulation models. Hence, safety goal violations can be identified in early design stages and the reuse of conducted safety analyses <span class="search-hit mathjax">reduces</span> the overhead for safety assessment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03368v1-abstract-full').style.display = 'none'; document.getElementById('2106.03368v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.03353">arXiv:2106.03353</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.03353">pdf</a>, <a href="https://arxiv.org/format/2106.03353">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding Neural <span class="search-hit mathjax">Code</span> Intelligence Through <span class="search-hit mathjax">Program</span> Simplification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rabin%2C+M+R+I">Md Rafiqul Islam Rabin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hellendoorn%2C+V+J">Vincent J. Hellendoorn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alipour%2C+M+A">Mohammad Amin Alipour</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.03353v1-abstract-short" style="display: inline;">
        A wide range of <span class="search-hit mathjax">code</span> intelligence (CI) tools, powered by deep neural networks, have been developed recently to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03353v1-abstract-full').style.display = 'inline'; document.getElementById('2106.03353v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.03353v1-abstract-full" style="display: none;">
        A wide range of <span class="search-hit mathjax">code</span> intelligence (CI) tools, powered by deep neural networks, have been developed recently to <span class="search-hit mathjax">improve</span> <span class="search-hit mathjax">programming</span> productivity and <span class="search-hit mathjax">perform</span> <span class="search-hit mathjax">program</span> analysis. To reliably use such tools, developers often need to reason about the behavior of the underlying models and the factors that affect them. This is especially challenging for tools backed by deep neural networks. Various methods have tried to <span class="search-hit mathjax">reduce</span> this opacity in the vein of &#34;transparent/interpretable-AI&#34;. However, these approaches are often specific to a particular set of network architectures, even requiring access to the network&#39;s parameters. This makes them difficult to use for the average programmer, which hinders the reliable adoption of neural CI systems. In this paper, we propose a simple, model-agnostic approach to identify critical input features for models in CI systems, by drawing on <span class="search-hit mathjax">software</span> debugging research, specifically delta debugging. Our approach, SIVAND, uses simplification techniques that <span class="search-hit mathjax">reduce</span> the size of input <span class="search-hit mathjax">programs</span> of a CI model while preserving the predictions of the model. We show that this approach yields remarkably small outputs and is broadly <span class="search-hit mathjax">applicable</span> across many model architectures and problem domains. We find that the models in our experiments often rely heavily on just a few syntactic features in input <span class="search-hit mathjax">programs</span>. We believe that SIVAND&#39;s extracted features may help understand neural CI systems&#39; predictions and learned behavior.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03353v1-abstract-full').style.display = 'none'; document.getElementById('2106.03353v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The 29th ACM Joint European <span class="search-hit mathjax">Software</span> Engineering Conference and Symposium on the Foundations of <span class="search-hit mathjax">Software</span> Engineering (ESEC/FSE'21)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.03272">arXiv:2106.03272</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.03272">pdf</a>, <a href="https://arxiv.org/format/2106.03272">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Finance">q-fin.CP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Signatured Deep Fictitious Play for Mean Field Games with Common Noise
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Min%2C+M">Ming Min</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+R">Ruimeng Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.03272v1-abstract-short" style="display: inline;">
        &hellip;structure with millions of simulations of common noise paths in order to produce accurate solutions, which results in prohibitive computational cost and limits the <span class="search-hit mathjax">applications</span> to a large extent. In this paper, based on the rough path theory, we propose a novel single-loop algorithm, named signatured deep fictitious play, by which we can work with the unfixe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03272v1-abstract-full').style.display = 'inline'; document.getElementById('2106.03272v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.03272v1-abstract-full" style="display: none;">
        Existing deep learning methods for solving mean-field games (MFGs) with common noise fix the sampling common noise paths and then solve the corresponding MFGs. This leads to a nested-loop structure with millions of simulations of common noise paths in order to produce accurate solutions, which results in prohibitive computational cost and limits the <span class="search-hit mathjax">applications</span> to a large extent. In this paper, based on the rough path theory, we propose a novel single-loop algorithm, named signatured deep fictitious play, by which we can work with the unfixed common noise setup to avoid the nested-loop structure and <span class="search-hit mathjax">reduce</span> the computational complexity significantly. The proposed algorithm can accurately capture the <span class="search-hit mathjax">effect</span> of common uncertainty changes on mean-field equilibria without further training of neural networks, as previously needed in the existing machine learning algorithms. The <span class="search-hit mathjax">efficiency</span> is supported by three <span class="search-hit mathjax">applications</span>, including linear-quadratic MFGs, mean-field portfolio game, and mean-field game of <span class="search-hit mathjax">optimal</span> consumption and investment. Overall, we provide a new point of view from the rough path theory to solve MFGs with common noise with significantly <span class="search-hit mathjax">improved</span> <span class="search-hit mathjax">efficiency</span> and an extensive range of <span class="search-hit mathjax">applications</span>. In addition, we report the first deep learning work to deal with extended MFGs (a mean-field interaction via both the states and controls) with common noise.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03272v1-abstract-full').style.display = 'none'; document.getElementById('2106.03272v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published at ICML 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.02964">arXiv:2106.02964</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.02964">pdf</a>, <a href="https://arxiv.org/format/2106.02964">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Review of Machine Learning Classification Using Quantum Annealing for Real-world <span class="search-hit mathjax">Applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nath%2C+R+K">Rajdeep Kumar Nath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thapliyal%2C+H">Himanshu Thapliyal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Humble%2C+T+S">Travis S. Humble</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.02964v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Optimizing</span> the training of a machine learning pipeline helps in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.02964v1-abstract-full').style.display = 'inline'; document.getElementById('2106.02964v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.02964v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Optimizing</span> the training of a machine learning pipeline helps in <span class="search-hit mathjax">reducing</span> training costs and <span class="search-hit mathjax">improving</span> model <span class="search-hit mathjax">performance</span>. One such <span class="search-hit mathjax">optimizing</span> strategy is quantum annealing, which is an emerging computing paradigm that has shown potential in <span class="search-hit mathjax">optimizing</span> the training of a machine learning model. The implementation of a physical quantum annealer has been realized by D-Wave systems and is available to the research community for experiments. Recent experimental results on a variety of machine learning <span class="search-hit mathjax">applications</span> using quantum annealing have shown interesting results where the <span class="search-hit mathjax">performance</span> of classical machine learning techniques is limited by limited training data and high dimensional features. This article explores the <span class="search-hit mathjax">application</span> of D-Wave&#39;s quantum annealer for <span class="search-hit mathjax">optimizing</span> machine learning pipelines for real-world classification problems. We review the <span class="search-hit mathjax">application</span> domains on which a physical quantum annealer has been used to train machine learning classifiers. We discuss and analyze the experiments <span class="search-hit mathjax">performed</span> on the D-Wave quantum annealer for <span class="search-hit mathjax">applications</span> such as image recognition, remote sensing imagery, computational biology, and particle physics. We discuss the possible advantages and the problems for which quantum annealing is likely to be advantageous over classical computation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.02964v1-abstract-full').style.display = 'none'; document.getElementById('2106.02964v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 Pages</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Springer Nature Computer Science (SNCS), 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.02140">arXiv:2106.02140</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.02140">pdf</a>, <a href="https://arxiv.org/format/2106.02140">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/978-3-319-10557-4_43">10.1007/978-3-319-10557-4_43 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards a Cross-Domain <span class="search-hit mathjax">Software</span> Safety Assurance Process for Embedded Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zeller%2C+M">Marc Zeller</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoefig%2C+K">Kai Hoefig</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rothfelder%2C+M">Martin Rothfelder</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.02140v1-abstract-short" style="display: inline;">
        In this work, we outline a cross-domain assurance process for safety-relevant <span class="search-hit mathjax">software</span> in embedded systems. This process aims to be applied in various different <span class="search-hit mathjax">application</span> domains and in conjunction with any development methodology. With this approach we plan to <span class="search-hit mathjax">reduce</span> the growi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.02140v1-abstract-full').style.display = 'inline'; document.getElementById('2106.02140v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.02140v1-abstract-full" style="display: none;">
        In this work, we outline a cross-domain assurance process for safety-relevant <span class="search-hit mathjax">software</span> in embedded systems. This process aims to be applied in various different <span class="search-hit mathjax">application</span> domains and in conjunction with any development methodology. With this approach we plan to <span class="search-hit mathjax">reduce</span> the growing effort for safety assessment in embedded systems by reusing safety analysis techniques and tools for the product development in different domains.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.02140v1-abstract-full').style.display = 'none'; document.getElementById('2106.02140v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.01847">arXiv:2106.01847</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.01847">pdf</a>, <a href="https://arxiv.org/format/2106.01847">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Cost-<span class="search-hit mathjax">Optimal</span> Policies for DAGs to Utilize IaaS Clouds with Online Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xiaohu Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+H">Han Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Casale%2C+G">Giuliano Casale</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+G">Guanyu Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.01847v1-abstract-short" style="display: inline;">
        &hellip;namely on-demand and spot instances, with time-varying features in availability and price. Users like startups have to operate on a limited budget and similarly others hope to <span class="search-hit mathjax">reduce</span> their costs. While interacting with a CSP, central to their concerns is the process of cost-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.01847v1-abstract-full').style.display = 'inline'; document.getElementById('2106.01847v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.01847v1-abstract-full" style="display: none;">
        Premier cloud service providers (CSPs) offer two types of purchase options, namely on-demand and spot instances, with time-varying features in availability and price. Users like startups have to operate on a limited budget and similarly others hope to <span class="search-hit mathjax">reduce</span> their costs. While interacting with a CSP, central to their concerns is the process of cost-<span class="search-hit mathjax">effectively</span> utilizing different purchase options possibly in addition to self-owned instances. A job in data-intensive <span class="search-hit mathjax">applications</span> is typically represented by a directed acyclic graph which can further be transformed into a chain of tasks. The key to achieving cost <span class="search-hit mathjax">efficiency</span> is determining the allocation of a specific deadline to each task, as well as the allocation of different types of instances to the task. In this paper, we propose a framework that determines the <span class="search-hit mathjax">optimal</span> allocation of deadlines to tasks. The framework also features an <span class="search-hit mathjax">optimal</span> policy to determine the allocation of spot and on-demand instances in a predefined time window, and a near-<span class="search-hit mathjax">optimal</span> policy for allocating self-owned instances. The policies are designed to be parametric to support the usage of online learning to infer the <span class="search-hit mathjax">optimal</span> values against the dynamics of cloud markets. Finally, several intuitive heuristics are used as baselines to validate the cost <span class="search-hit mathjax">improvement</span> brought by the proposed solutions. We show that the cost <span class="search-hit mathjax">improvement</span> over the state-of-the-art is up to 24.87% when spot and on-demand instances are considered and up to 59.05% when self-owned instances are considered.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.01847v1-abstract-full').style.display = 'none'; document.getElementById('2106.01847v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.01766">arXiv:2106.01766</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.01766">pdf</a>, <a href="https://arxiv.org/ps/2106.01766">ps</a>, <a href="https://arxiv.org/format/2106.01766">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ISPRAS.2018.00009">10.1109/ISPRAS.2018.00009 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Analysis of ARINC 653 RTOS with LLVM
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cheptsov%2C+V">Vitaly Cheptsov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khoroshilov%2C+A">Alexey Khoroshilov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.01766v1-abstract-short" style="display: inline;">
        Existing standards for airborne-embedded <span class="search-hit mathjax">software</span> systems impose a number of requirements&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.01766v1-abstract-full').style.display = 'inline'; document.getElementById('2106.01766v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.01766v1-abstract-full" style="display: none;">
        Existing standards for airborne-embedded <span class="search-hit mathjax">software</span> systems impose a number of requirements <span class="search-hit mathjax">applicable</span> to the <span class="search-hit mathjax">software</span> development cycle of hard real-time operating systems found in modern aircraft. The measures taken are meant to <span class="search-hit mathjax">reduce</span> the risks of undesired consequences, but have strongly varying costs. Dynamic instrumentation and static analysis are common practices used to <span class="search-hit mathjax">automatically</span> find <span class="search-hit mathjax">software</span> defects, from strictly non-conforming <span class="search-hit mathjax">code</span> constructions to memory corruptions or invalid control flow. LLVM analyser and sanitizer infrastructure, while regularly applied to general-purpose <span class="search-hit mathjax">software</span>, originally was not thought to be introduced to heavily restricted environments. In this paper we discuss the specifics of airborne systems with regards to dynamic instrumentation and provide practical considerations to be taken into account for the <span class="search-hit mathjax">effective</span> use of general-purpose instrumentation tools. We bring a complete LLVM stack support to JetOS, a prospective onboard real-time operating system currently being developed at ISP RAS in collaboration with GosNIIAS. As an example, we port AddressSanitizer, MemorySanitizer, and UndefinedBehaviorSanitizer and provide the details against the caveats on all relevant sides: a sanitizer, a compiler, and an operating system. In addition we suggest uninvolved optimisations and enhancements to the runtimes to maximise the <span class="search-hit mathjax">effects</span> of the tools.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.01766v1-abstract-full').style.display = 'none'; document.getElementById('2106.01766v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2018 Ivannikov Ispras Open Conference (ISPRAS), 2018, pp. 9-15
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.00730">arXiv:2106.00730</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.00730">pdf</a>, <a href="https://arxiv.org/format/2106.00730">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enabling <span class="search-hit mathjax">Efficiency</span>-<span class="search-hit mathjax">Precision</span> Trade-offs for Label Trees in Extreme Classification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Baharav%2C+T+Z">Tavor Z. Baharav</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+D+L">Daniel L. Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kolluri%2C+K">Kedarnath Kolluri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sanghavi%2C+S">Sujay Sanghavi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhillon%2C+I+S">Inderjit S. Dhillon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.00730v1-abstract-short" style="display: inline;">
        &hellip;multi-label classification (XMC) aims to learn a model that can tag data points with a subset of relevant labels from an extremely large label set. Real world e-commerce <span class="search-hit mathjax">applications</span> like personalized recommendations and product advertising can be formulated as XMC problems, where the objective is to predict for a user a small subset of items from a catalog&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.00730v1-abstract-full').style.display = 'inline'; document.getElementById('2106.00730v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.00730v1-abstract-full" style="display: none;">
        Extreme multi-label classification (XMC) aims to learn a model that can tag data points with a subset of relevant labels from an extremely large label set. Real world e-commerce <span class="search-hit mathjax">applications</span> like personalized recommendations and product advertising can be formulated as XMC problems, where the objective is to predict for a user a small subset of items from a catalog of several million products. For such <span class="search-hit mathjax">applications</span>, a common approach is to organize these labels into a tree, enabling training and inference times that are logarithmic in the number of labels. While training a model once a label tree is available is well studied, designing the structure of the tree is a difficult task that is not yet well understood, and can dramatically impact both model latency and statistical <span class="search-hit mathjax">performance</span>. Existing approaches to tree construction fall at an extreme point, either <span class="search-hit mathjax">optimizing</span> exclusively for statistical <span class="search-hit mathjax">performance</span>, or for latency. We propose an <span class="search-hit mathjax">efficient</span> information theory inspired algorithm to construct intermediary operating points that trade off between the benefits of both. Our algorithm enables interpolation between these objectives, which was not previously possible. We corroborate our theoretical analysis with numerical results, showing that on the Wiki-500K benchmark dataset our method can <span class="search-hit mathjax">reduce</span> a proxy for expected latency by up to 28% while maintaining the same <span class="search-hit mathjax">accuracy</span> as Parabel. On several datasets derived from e-commerce customer logs, our modified label tree is able to <span class="search-hit mathjax">improve</span> this expected latency metric by up to 20% while maintaining the same <span class="search-hit mathjax">accuracy</span>. Finally, we discuss challenges in realizing these latency <span class="search-hit mathjax">improvements</span> in deployed models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.00730v1-abstract-full').style.display = 'none'; document.getElementById('2106.00730v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.00606">arXiv:2106.00606</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.00606">pdf</a>, <a href="https://arxiv.org/format/2106.00606">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic-Deep: ECG Task-Aware Compression
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Brosh%2C+E">Eli Brosh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wasserstein%2C+E">Elad Wasserstein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bremler-Barr%2C+A">Anat Bremler-Barr</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.00606v1-abstract-short" style="display: inline;">
        Monitoring medical data, e.g., Electrocardiogram (ECG) signals, is a common <span class="search-hit mathjax">application</span> of Internet of Things (IoT) devices. Compression methods are often applied on the massive amounts of sensor data generated before sending it to the Cloud to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.00606v1-abstract-full').style.display = 'inline'; document.getElementById('2106.00606v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.00606v1-abstract-full" style="display: none;">
        Monitoring medical data, e.g., Electrocardiogram (ECG) signals, is a common <span class="search-hit mathjax">application</span> of Internet of Things (IoT) devices. Compression methods are often applied on the massive amounts of sensor data generated before sending it to the Cloud to <span class="search-hit mathjax">reduce</span> storage and delivery costs. A lossy compression provides high compression gain (CG) but may <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">performance</span> of an ECG <span class="search-hit mathjax">application</span> (downstream task) due to information loss. Previous works on ECG monitoring focus either on <span class="search-hit mathjax">optimizing</span> the signal reconstruction or the task&#39;s <span class="search-hit mathjax">performance</span>. Instead, we advocate a lossy compression solution that allows configuring a desired <span class="search-hit mathjax">performance</span> level on the downstream tasks while maintaining an <span class="search-hit mathjax">optimized</span> CG.
  We propose Dynamic-Deep, a task-aware compression that uses convolutional autoencoders. The compression level is dynamically selected to yield an <span class="search-hit mathjax">optimized</span> compression without violating tasks&#39; <span class="search-hit mathjax">performance</span> requirements. We conduct an extensive evaluation of our approach on common ECG datasets using two popular ECG <span class="search-hit mathjax">applications</span>, which includes heart rate (HR) arrhythmia classification. We demonstrate that Dynamic-Deep <span class="search-hit mathjax">improves</span> HR classification F1-score by a factor of 3 and increases CG by up to 83% compared to the previous state-of-the-art (autoencoder-based) compressor. Additionally, Dynamic-Deep has a 67% lower memory footprint. Analyzing Dynamic-Deep on the Google Cloud Platform, we observe a 97% reduction in cloud costs compared to a no compression solution.
  To the best of our knowledge, Dynamic-Deep is the first proposal to focus on balancing the need for high <span class="search-hit mathjax">performance</span> of cloud-based downstream tasks and the desire to achieve <span class="search-hit mathjax">optimized</span> compression in IoT ECG monitoring settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.00606v1-abstract-full').style.display = 'none'; document.getElementById('2106.00606v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted to Globecom2021 SAC MLC</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.15015">arXiv:2105.15015</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.15015">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/RAM.2018.8463058">10.1109/RAM.2018.8463058 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Model-Based Reliability and Safety: <span class="search-hit mathjax">Reducing</span> the Complexity of Safety Analyses Using Component Fault Trees
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hoefig%2C+K">Kai Hoefig</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Joanni%2C+A">Andreas Joanni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeller%2C+M">Marc Zeller</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Montrone%2C+F">Francesco Montrone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rothfelder%2C+M">Martin Rothfelder</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amarnath%2C+R">Rakshith Amarnath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Munk%2C+P">Peter Munk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nordmann%2C+A">Arne Nordmann</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.15015v1-abstract-short" style="display: inline;">
        The importance of mission or safety critical <span class="search-hit mathjax">software</span> systems in many&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.15015v1-abstract-full').style.display = 'inline'; document.getElementById('2105.15015v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.15015v1-abstract-full" style="display: none;">
        The importance of mission or safety critical <span class="search-hit mathjax">software</span> systems in many <span class="search-hit mathjax">application</span> domains of embedded systems is continuously growing, and so is the effort and complexity for reliability and safety analysis. Model driven development is currently one of the key approaches to cope with increasing development complexity, in general. Applying similar concepts to reliability, availability, maintainability and safety (RAMS) analysis activities is a promising approach to extend the advantages of model driven development to safety engineering activities aiming at a reduction of development costs, a higher product quality and a shorter time-to-market. Nevertheless, many model-based safety or reliability engineering approaches aim at <span class="search-hit mathjax">reducing</span> the analysis complexity but <span class="search-hit mathjax">applications</span> or case studies are rare. Therefore we present here a large scale industrial case study which shows the benefits of the <span class="search-hit mathjax">application</span> of component fault trees when it comes to complex safety mechanisms. We compare the methodology of component fault trees against classic fault trees and summarize benefits and drawbacks of both modeling methodologies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.15015v1-abstract-full').style.display = 'none'; document.getElementById('2105.15015v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2018 Annual Reliability and Maintainability Symposium (RAMS)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.13894">arXiv:2105.13894</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.13894">pdf</a>, <a href="https://arxiv.org/format/2105.13894">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Performance</span> Evaluation of Snapshot Methods to Warm the Serverless Cold Start
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Silva%2C+P">Paulo Silva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pereira%2C+T+E">Thiago Emmanuel Pereira</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.13894v1-abstract-short" style="display: inline;">
        &hellip;computing model strengthens the cloud computing tendency to abstract resource management. Serverless platforms are responsible for deploying and scaling the developer&#39;s <span class="search-hit mathjax">applications</span>. Serverless also incorporated the pay-as-you-go billing model, which only considers the time spent processing client requests. Such a decision created a natural incentive for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.13894v1-abstract-full').style.display = 'inline'; document.getElementById('2105.13894v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.13894v1-abstract-full" style="display: none;">
        The serverless computing model strengthens the cloud computing tendency to abstract resource management. Serverless platforms are responsible for deploying and scaling the developer&#39;s <span class="search-hit mathjax">applications</span>. Serverless also incorporated the pay-as-you-go billing model, which only considers the time spent processing client requests. Such a decision created a natural incentive for <span class="search-hit mathjax">improving</span> the platform&#39;s <span class="search-hit mathjax">efficient</span> resource usage. This search for <span class="search-hit mathjax">efficiency</span> can lead to the cold start problem, which represents a delay to execute serverless <span class="search-hit mathjax">applications</span>. Among the solutions proposed to deal with the cold start, those based on the snapshot method stand out. Despite the rich exploration of the technique, there is a lack of research that evaluates the solution&#39;s trade-offs. In this direction, this work compares two solutions to mitigate the cold start: Prebaking and SEUSS. We analyzed the solution&#39;s <span class="search-hit mathjax">performance</span> with functions of different levels of complexity: NoOp, a function that renders Markdown to HTML, and a function that loads 41 MB of dependencies. Preliminary results indicated that Prebaking showed a 33% and 25% superior <span class="search-hit mathjax">performance</span> to startup the NoOp and Markdown functions, respectively. Further analysis also revealed that Prebaking&#39;s warmup mechanism <span class="search-hit mathjax">reduced</span> the Markdown first request processing time by 69%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.13894v1-abstract-full').style.display = 'none'; document.getElementById('2105.13894v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">in Portuguese</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.12912">arXiv:2105.12912</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.12912">pdf</a>, <a href="https://arxiv.org/format/2105.12912">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        cuSZ(x): <span class="search-hit mathjax">Optimizing</span> Error-Bounded Lossy Compression for Scientific Data on GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+J">Jiannan Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Di%2C+S">Sheng Di</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+X">Xiaodong Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rivera%2C+C">Cody Rivera</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+K">Kai Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+S">Sian Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+Y">Yunhe Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+X">Xin Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tao%2C+D">Dingwen Tao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cappello%2C+F">Franck Cappello</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.12912v1-abstract-short" style="display: inline;">
        Error-bounded lossy compression is a critical technique for significantly <span class="search-hit mathjax">reducing</span> scientific data volumes. With ever-emerging heterogeneous HPC architecture, GPU-accelerated error-bounded compressors (such as cuSZ and cuZFP) have been developed. However, they suffer from either low&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.12912v1-abstract-full').style.display = 'inline'; document.getElementById('2105.12912v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.12912v1-abstract-full" style="display: none;">
        Error-bounded lossy compression is a critical technique for significantly <span class="search-hit mathjax">reducing</span> scientific data volumes. With ever-emerging heterogeneous HPC architecture, GPU-accelerated error-bounded compressors (such as cuSZ and cuZFP) have been developed. However, they suffer from either low <span class="search-hit mathjax">performance</span> or low compression ratios. To this end, we propose cuSZ(x) to target both high compression ratio and throughput. We identify that data sparsity and data smoothness are key factors for high compression throughput. Our key contributions in this work are fourfold: (1) We propose an <span class="search-hit mathjax">efficient</span> compression workflow to adaptively <span class="search-hit mathjax">perform</span> run-length encoding and/or variable-length encoding. (2) We derive Lorenzo reconstruction in decompression as multidimensional partial-sum computation and propose a fine-grained Lorenzo reconstruction algorithm for GPU architectures. (3) We carefully <span class="search-hit mathjax">optimize</span> each of cuSZ&#39;s kernels by leveraging state-of-the-art CUDA parallel primitives. (4) We evaluate cuSZ(x) using seven real-world HPC <span class="search-hit mathjax">application</span> datasets on V100 and A100 GPUs. Experiments show cuSZ(x) <span class="search-hit mathjax">improves</span> the compression <span class="search-hit mathjax">performance</span> and ratios by up to 18.4$\times$ and 5.3$\times$, respectively, over cuSZ on the tested datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.12912v1-abstract-full').style.display = 'none'; document.getElementById('2105.12912v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 3 figures, 8 table, submitted to IEEE Cluster&#39;21</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.12841">arXiv:2105.12841</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.12841">pdf</a>, <a href="https://arxiv.org/format/2105.12841">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DNNV: A Framework for Deep Neural Network Verification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shriver%2C+D">David Shriver</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elbaum%2C+S">Sebastian Elbaum</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dwyer%2C+M+B">Matthew B. Dwyer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.12841v1-abstract-short" style="display: inline;">
        &hellip;Existing benchmarks are rarely in formats supported by verifiers other than the one for which the benchmark was introduced. In this work we present DNNV, a framework for <span class="search-hit mathjax">reducing</span> the burden on DNN verifier researchers, developers, and users. DNNV standardizes input and output formats, includes a simple yet expressive DSL for specifying DNN properties, and p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.12841v1-abstract-full').style.display = 'inline'; document.getElementById('2105.12841v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.12841v1-abstract-full" style="display: none;">
        Despite the large number of sophisticated deep neural network (DNN) verification algorithms, DNN verifier developers, users, and researchers still face several challenges. First, verifier developers must contend with the rapidly changing DNN field to support new DNN operations and property types. Second, verifier users have the burden of selecting a verifier input format to specify their problem. Due to the many input formats, this decision can greatly restrict the verifiers that a user may run. Finally, researchers face difficulties in re-using benchmarks to evaluate and compare verifiers, due to the large number of input formats required to run different verifiers. Existing benchmarks are rarely in formats supported by verifiers other than the one for which the benchmark was introduced. In this work we present DNNV, a framework for <span class="search-hit mathjax">reducing</span> the burden on DNN verifier researchers, developers, and users. DNNV standardizes input and output formats, includes a simple yet expressive DSL for specifying DNN properties, and provides powerful simplification and reduction operations to facilitate the <span class="search-hit mathjax">application</span>, development, and comparison of DNN verifiers. We show how DNNV increases the support of verifiers for existing benchmarks from 30% to 74%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.12841v1-abstract-full').style.display = 'none'; document.getElementById('2105.12841v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.12678">arXiv:2105.12678</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.12678">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Flexible FPGA-Based ISA Configurable SoC platform
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+S">Shih-Yi Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+B">Bo-Yu Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.12678v1-abstract-short" style="display: inline;">
        We proposes a platform which can generate hardware/<span class="search-hit mathjax">software</span> description based on flexible in-struction set architectures (ISAs). The platform takes advantage of the flexibility of field pro-grammable gate array (FPGA) to design many micro control units (MCUs) based on different ISAs. The platform can generate many ISAs, MCUs, and Assemblers according to a pr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.12678v1-abstract-full').style.display = 'inline'; document.getElementById('2105.12678v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.12678v1-abstract-full" style="display: none;">
        We proposes a platform which can generate hardware/<span class="search-hit mathjax">software</span> description based on flexible in-struction set architectures (ISAs). The platform takes advantage of the flexibility of field pro-grammable gate array (FPGA) to design many micro control units (MCUs) based on different ISAs. The platform can generate many ISAs, MCUs, and Assemblers according to a pre-defined ISA and user <span class="search-hit mathjax">applications</span>. Although the MCU <span class="search-hit mathjax">performance</span> is not <span class="search-hit mathjax">optimized</span>, the FPGA shows a great potential on resource reduction and enough <span class="search-hit mathjax">performance</span> at very low system clock rate. The flexible ISA has shown great importance for the design targeted to specific purpose. We also show a case study of the proposed flexible ISA-based FPGA-MCU. It can control many specifi-cally designed hardware IPs and a customized multi-task OS with tasks. Not only the case works correctly, but also the proposed FPGA-MCU of the case is flexible with <span class="search-hit mathjax">reduced</span> FPGA resources, low cost, and within time constraints.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.12678v1-abstract-full').style.display = 'none'; document.getElementById('2105.12678v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.12356">arXiv:2105.12356</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.12356">pdf</a>, <a href="https://arxiv.org/format/2105.12356">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Submodular Kernels for <span class="search-hit mathjax">Efficient</span> Rankings
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Conserva%2C+M">Michelangelo Conserva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deisenroth%2C+M+P">Marc Peter Deisenroth</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+K+S+S">K S Sesh Kumar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.12356v1-abstract-short" style="display: inline;">
        &hellip;by partial rankings, i.e. rankings in which the preference is only known for a subset of all objects. For these reasons, state-of-the-art methods cannot scale to real-world <span class="search-hit mathjax">applications</span>, such as recommender systems. We address this challenge by exploiting geometric structure of ranked data and additional available information about the objects to derive a su&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.12356v1-abstract-full').style.display = 'inline'; document.getElementById('2105.12356v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.12356v1-abstract-full" style="display: none;">
        Many algorithms for ranked data become computationally intractable as the number of objects grows due to complex geometric structure induced by rankings. An additional challenge is posed by partial rankings, i.e. rankings in which the preference is only known for a subset of all objects. For these reasons, state-of-the-art methods cannot scale to real-world <span class="search-hit mathjax">applications</span>, such as recommender systems. We address this challenge by exploiting geometric structure of ranked data and additional available information about the objects to derive a submodular kernel for ranking. The submodular kernel combines the <span class="search-hit mathjax">efficiency</span> of submodular <span class="search-hit mathjax">optimization</span> with the theoretical properties of kernel-based methods. We demonstrate that the submodular kernel drastically <span class="search-hit mathjax">reduces</span> the computational cost compared to state-of-the-art kernels and scales well to large datasets while attaining good empirical <span class="search-hit mathjax">performance</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.12356v1-abstract-full').style.display = 'none'; document.getElementById('2105.12356v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.11654">arXiv:2105.11654</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.11654">pdf</a>, <a href="https://arxiv.org/format/2105.11654">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Optimal</span> ANN-SNN Conversion for Fast and Accurate Inference in Deep Spiking Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+J">Jianhao Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Z">Zhaofei Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+Y">Yonghong Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+T">Tiejun Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.11654v1-abstract-short" style="display: inline;">
        Spiking Neural Networks (SNNs), as bio-inspired energy-<span class="search-hit mathjax">efficient</span> neural networks, have attracted great attentions from researchers and industry. The most&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.11654v1-abstract-full').style.display = 'inline'; document.getElementById('2105.11654v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.11654v1-abstract-full" style="display: none;">
        Spiking Neural Networks (SNNs), as bio-inspired energy-<span class="search-hit mathjax">efficient</span> neural networks, have attracted great attentions from researchers and industry. The most <span class="search-hit mathjax">efficient</span> way to train deep SNNs is through ANN-SNN conversion. However, the conversion usually suffers from <span class="search-hit mathjax">accuracy</span> loss and long inference time, which impede the practical <span class="search-hit mathjax">application</span> of SNN. In this paper, we theoretically analyze ANN-SNN conversion and derive sufficient conditions of the <span class="search-hit mathjax">optimal</span> conversion. To better correlate ANN-SNN and get greater <span class="search-hit mathjax">accuracy</span>, we propose Rate Norm Layer to replace the ReLU activation function in source ANN training, enabling direct conversion from a trained ANN to an SNN. Moreover, we propose an <span class="search-hit mathjax">optimal</span> fit curve to quantify the fit between the activation value of source ANN and the actual firing rate of target SNN. We show that the inference time can be <span class="search-hit mathjax">reduced</span> by <span class="search-hit mathjax">optimizing</span> the upper bound of the fit curve in the revised ANN to achieve fast inference. Our theory can explain the existing work on fast reasoning and get better results. The experimental results show that the proposed method achieves near loss less conversion with VGG-16, PreActResNet-18, and deeper structures. Moreover, it can reach 8.6x faster reasoning <span class="search-hit mathjax">performance</span> under 0.265x energy consumption of the typical method. The <span class="search-hit mathjax">code</span> is available at https://github.com/DingJianhao/OptSNNConvertion-RNL-RIL.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.11654v1-abstract-full').style.display = 'none'; document.getElementById('2105.11654v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 7 figures, 2 tables. To appear in the 30th International Joint Conference on Artificial Intelligence (IJCAI 2021)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.07806">arXiv:2105.07806</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.07806">pdf</a>, <a href="https://arxiv.org/format/2105.07806">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3448016.3459240">10.1145/3448016.3459240 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Demystifying Serverless Machine Learning Training
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+J">Jiawei Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gan%2C+S">Shaoduo Gan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yue Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+F">Fanlin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alonso%2C+G">Gustavo Alonso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Klimovic%2C+A">Ana Klimovic</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singla%2C+A">Ankit Singla</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+W">Wentao Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Ce Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.07806v1-abstract-short" style="display: inline;">
        The appeal of serverless (FaaS) has triggered a growing interest on how to use it in data-intensive <span class="search-hit mathjax">applications</span> such as ETL, query processing, or machine learning (ML). Several systems exist for training large-scale ML models on top of serverless infrastructures (e.g., AWS Lambda) but with inconclusive results in terms of their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07806v1-abstract-full').style.display = 'inline'; document.getElementById('2105.07806v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.07806v1-abstract-full" style="display: none;">
        The appeal of serverless (FaaS) has triggered a growing interest on how to use it in data-intensive <span class="search-hit mathjax">applications</span> such as ETL, query processing, or machine learning (ML). Several systems exist for training large-scale ML models on top of serverless infrastructures (e.g., AWS Lambda) but with inconclusive results in terms of their <span class="search-hit mathjax">performance</span> and relative advantage over &#34;serverful&#34; infrastructures (IaaS). In this paper we present a systematic, comparative study of distributed ML training over FaaS and IaaS. We present a design space covering design choices such as <span class="search-hit mathjax">optimization</span> algorithms and synchronization protocols, and implement a platform, LambdaML, that enables a fair comparison between FaaS and IaaS. We present experimental results using LambdaML, and further develop an analytic model to capture cost/<span class="search-hit mathjax">performance</span> tradeoffs that must be considered when opting for a serverless infrastructure. Our results indicate that ML training pays off in serverless only for models with <span class="search-hit mathjax">efficient</span> (i.e., <span class="search-hit mathjax">reduced</span>) communication and that quickly converge. In general, FaaS can be much faster but it is never significantly cheaper than IaaS.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07806v1-abstract-full').style.display = 'none'; document.getElementById('2105.07806v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.07544">arXiv:2105.07544</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.07544">pdf</a>, <a href="https://arxiv.org/format/2105.07544">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Experimental Evaluation of Multiprecision Strategies for GMRES on GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Loe%2C+J+A">Jennifer A. Loe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Glusa%2C+C+A">Christian A. Glusa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yamazaki%2C+I">Ichitaro Yamazaki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boman%2C+E+G">Erik G. Boman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rajamanickam%2C+S">Sivasankaran Rajamanickam</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.07544v1-abstract-short" style="display: inline;">
        Support for lower <span class="search-hit mathjax">precision</span> computation is becoming more common in accelerator hardware due to lower power usage,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07544v1-abstract-full').style.display = 'inline'; document.getElementById('2105.07544v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.07544v1-abstract-full" style="display: none;">
        Support for lower <span class="search-hit mathjax">precision</span> computation is becoming more common in accelerator hardware due to lower power usage, <span class="search-hit mathjax">reduced</span> data movement and increased computational <span class="search-hit mathjax">performance</span>. However, computational science and engineering (CSE) problems require double <span class="search-hit mathjax">precision</span> <span class="search-hit mathjax">accuracy</span> in several domains. This conflict between hardware trends and <span class="search-hit mathjax">application</span> needs has resulted in a need for multiprecision strategies at the linear algebra algorithms level if we want to exploit the hardware to its full potential while meeting the <span class="search-hit mathjax">accuracy</span> requirements. In this paper, we focus on preconditioned sparse iterative linear solvers, a key kernel in several CSE <span class="search-hit mathjax">applications</span>. We present a study of multiprecision strategies for accelerating this kernel on GPUs. We seek the best methods for incorporating multiple <span class="search-hit mathjax">precisions</span> into the GMRES linear solver; these include iterative refinement and parallelizable preconditioners. Our work presents strategies to determine when multiprecision GMRES will be <span class="search-hit mathjax">effective</span> and to choose parameters for a multiprecision iterative refinement solver to achieve better <span class="search-hit mathjax">performance</span>. We use an implementation that is based on the Trilinos library and employs Kokkos Kernels for <span class="search-hit mathjax">performance</span> portability of linear algebra kernels. <span class="search-hit mathjax">Performance</span> results demonstrate the promise of multiprecision approaches and demonstrate even further <span class="search-hit mathjax">improvements</span> are possible by <span class="search-hit mathjax">optimizing</span> low-level kernels.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07544v1-abstract-full').style.display = 'none'; document.getElementById('2105.07544v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication in the IEEE IPDPS Accelerators and Hybrid Emerging Systems (AsHES) 11th Workshop, 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.07420">arXiv:2105.07420</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.07420">pdf</a>, <a href="https://arxiv.org/format/2105.07420">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Resource Planning for Hospitals Under Special Consideration of the COVID-19 Pandemic: <span class="search-hit mathjax">Optimization</span> and Sensitivity Analysis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bartz-Beielstein%2C+T">Thomas Bartz-Beielstein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dr%C3%B6scher%2C+M">Marcel Dröscher</a>, 
      
      <a href="/search/?searchtype=author&amp;query=G%C3%BCr%2C+A">Alpar Gür</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hinterleitner%2C+A">Alexander Hinterleitner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mersmann%2C+O">Olaf Mersmann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peeva%2C+D">Dessislava Peeva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Reese%2C+L">Lennard Reese</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rehbach%2C+N">Nicolas Rehbach</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rehbach%2C+F">Frederik Rehbach</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sen%2C+A">Amrita Sen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Subbotin%2C+A">Aleksandr Subbotin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaefferer%2C+M">Martin Zaefferer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.07420v1-abstract-short" style="display: inline;">
        &hellip;is determined by 29 parameters. Reasonable default values of these parameters were obtained in detailed discussions with medical professionals. We aim to investigate and <span class="search-hit mathjax">optimize</span> these parameters to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07420v1-abstract-full').style.display = 'inline'; document.getElementById('2105.07420v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.07420v1-abstract-full" style="display: none;">
        Crises like the COVID-19 pandemic pose a serious challenge to health-care institutions. They need to plan the resources required for handling the increased load, for instance, hospital beds and ventilators. To support the resource planning of local health authorities from the Cologne region, BaBSim.Hospital, a tool for capacity planning based on discrete event simulation, was created. The predictive quality of the simulation is determined by 29 parameters. Reasonable default values of these parameters were obtained in detailed discussions with medical professionals. We aim to investigate and <span class="search-hit mathjax">optimize</span> these parameters to <span class="search-hit mathjax">improve</span> BaBSim.Hospital. First approaches with &#34;out-of-the-box&#34; <span class="search-hit mathjax">optimization</span> algorithms failed. Implementing a surrogate-based <span class="search-hit mathjax">optimization</span> approach generated useful results in a reasonable time. To understand the behavior of the algorithm and to get valuable insights into the fitness landscape, an in-depth sensitivity analysis was <span class="search-hit mathjax">performed</span>. The sensitivity analysis is crucial for the <span class="search-hit mathjax">optimization</span> process because it allows focusing the <span class="search-hit mathjax">optimization</span> on the most important parameters. We illustrate how this <span class="search-hit mathjax">reduces</span> the problem dimension without compromising the resulting <span class="search-hit mathjax">accuracy</span>. The presented approach is <span class="search-hit mathjax">applicable</span> to many other real-world problems, e.g., the development of new elevator systems to cover the last mile or simulation of student flow in academic study periods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07420v1-abstract-full').style.display = 'none'; document.getElementById('2105.07420v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.07026">arXiv:2105.07026</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.07026">pdf</a>, <a href="https://arxiv.org/format/2105.07026">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Probability">math.PR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Monotone Approximate Dynamic <span class="search-hit mathjax">Programming</span> Approach for the Stochastic Scheduling, Allocation, and Inventory Replenishment Problem: <span class="search-hit mathjax">Applications</span> to Drone and Electric Vehicle Battery Swap Stations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Asadi%2C+A">Amin Asadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pinkley%2C+S+N">Sarah Nurre Pinkley</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.07026v1-abstract-short" style="display: inline;">
        There is a growing interest in using electric vehicles (EVs) and drones for many <span class="search-hit mathjax">applications</span>. However, battery-oriented issues, including range anxiety and battery degradation, impede adoption. Battery swap stations are one alternative to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07026v1-abstract-full').style.display = 'inline'; document.getElementById('2105.07026v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.07026v1-abstract-full" style="display: none;">
        There is a growing interest in using electric vehicles (EVs) and drones for many <span class="search-hit mathjax">applications</span>. However, battery-oriented issues, including range anxiety and battery degradation, impede adoption. Battery swap stations are one alternative to <span class="search-hit mathjax">reduce</span> these concerns that allow the swap of depleted for full batteries in minutes. We consider the problem of deriving actions at a battery swap station when explicitly considering the uncertain arrival of swap demand, battery degradation, and replacement. We model the operations at a battery swap station using a finite horizon Markov Decision Process model for the stochastic scheduling, allocation, and inventory replenishment problem (SAIRP), which determines when and how many batteries are charged, discharged, and replaced over time. We present theoretical proofs for the monotonicity of the value function and monotone structure of an <span class="search-hit mathjax">optimal</span> policy for special SAIRP cases. Due to the curses of dimensionality, we develop a new monotone approximate dynamic <span class="search-hit mathjax">programming</span> (ADP) method, which intelligently initializes a value function approximation using regression. In computational tests, we demonstrate the superior <span class="search-hit mathjax">performance</span> of the new regression-based monotone ADP method as compared to exact methods and other monotone ADP methods. Further, with the tests, we deduce policy insights for drone swap stations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07026v1-abstract-full').style.display = 'none'; document.getElementById('2105.07026v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.05004">arXiv:2105.05004</a>
        <span>&nbsp;&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Smart Name Lookup for NDN Forwarding Plane via Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhuo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jindian Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+L">Liu Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Beichuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+P">Peng Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+K">Kaihua Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.05004v2-abstract-short" style="display: inline;">
        Name lookup is a key technology for the forwarding plane of content router in Named Data Networking (NDN). To realize the <span class="search-hit mathjax">efficient</span> name lookup, what counts is deploying a highperformance index in content routers. So far, the proposed indexes have shown good&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.05004v2-abstract-full').style.display = 'inline'; document.getElementById('2105.05004v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.05004v2-abstract-full" style="display: none;">
        Name lookup is a key technology for the forwarding plane of content router in Named Data Networking (NDN). To realize the <span class="search-hit mathjax">efficient</span> name lookup, what counts is deploying a highperformance index in content routers. So far, the proposed indexes have shown good <span class="search-hit mathjax">performance</span>, most of which are <span class="search-hit mathjax">optimized</span> for or evaluated with URLs collected from the current Internet, as the large-scale NDN names are not available yet. Unfortunately, the <span class="search-hit mathjax">performance</span> of these indexes is always impacted in terms of lookup speed, memory consumption and false positive probability, as the distributions of URLs retrieved in memory may differ from those of real NDN names independently generated by content-centric <span class="search-hit mathjax">applications</span> online. Focusing on this gap, a smart mapping model named Pyramid-NN via neural networks is proposed to build an index called LNI for NDN forwarding plane. Through learning the distributions of the names retrieved in the static memory, LNI can not only <span class="search-hit mathjax">reduce</span> the memory consumption and the probability of false positive, but also ensure the <span class="search-hit mathjax">performance</span> of real NDN name lookup. Experimental results show that LNI-based FIB can <span class="search-hit mathjax">reduce</span> the memory consumption to 58.258 MB for 2 million names. Moreover, as it can be deployed on SRAMs, the throughput is about 177 MSPS, which well meets the current network requirement for fast packet processing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.05004v2-abstract-full').style.display = 'none'; document.getElementById('2105.05004v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">We need to refine the paper further including the title and the structure of the paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.04086">arXiv:2105.04086</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.04086">pdf</a>, <a href="https://arxiv.org/format/2105.04086">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Reinforcement Learning-based Methods for Resource Scheduling in Cloud Computing: A Review and Future Directions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+G">Guangyao Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+W">Wenhong Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buyya%2C+R">Rajkumar Buyya</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.04086v1-abstract-short" style="display: inline;">
        As the quantity and complexity of information processed by <span class="search-hit mathjax">software</span> systems increase, large-scale&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.04086v1-abstract-full').style.display = 'inline'; document.getElementById('2105.04086v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.04086v1-abstract-full" style="display: none;">
        As the quantity and complexity of information processed by <span class="search-hit mathjax">software</span> systems increase, large-scale <span class="search-hit mathjax">software</span> systems have an increasing requirement for high-<span class="search-hit mathjax">performance</span> distributed computing systems. With the acceleration of the Internet in Web 2.0, Cloud computing as a paradigm to provide dynamic, uncertain and elastic services has shown superiorities to meet the computing needs dynamically. Without an appropriate scheduling approach, extensive Cloud computing may cause high energy consumptions and high cost, in addition that high energy consumption will cause massive carbon dioxide emissions. Moreover, inappropriate scheduling will <span class="search-hit mathjax">reduce</span> the service life of physical devices as well as increase response time to users&#39; request. Hence, <span class="search-hit mathjax">efficient</span> scheduling of resource or <span class="search-hit mathjax">optimal</span> allocation of request, that usually a NP-hard problem, is one of the prominent issues in emerging trends of Cloud computing. Focusing on <span class="search-hit mathjax">improving</span> quality of service (QoS), <span class="search-hit mathjax">reducing</span> cost and abating contamination, researchers have conducted extensive work on resource scheduling problems of Cloud computing over years. Nevertheless, growing complexity of Cloud computing, that the super-massive distributed system, is limiting the <span class="search-hit mathjax">application</span> of scheduling approaches. Machine learning, a utility method to tackle problems in complex scenes, is used to resolve the resource scheduling of Cloud computing as an innovative idea in recent years. Deep reinforcement learning (DRL), a combination of deep learning (DL) and reinforcement learning (RL), is one branch of the machine learning and has a considerable prospect in resource scheduling of Cloud computing. This paper surveys the methods of resource scheduling with focus on DRL-based scheduling approaches in Cloud computing, also reviews the <span class="search-hit mathjax">application</span> of DRL as well as discusses challenges and future directions of DRL in scheduling of Cloud computing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.04086v1-abstract-full').style.display = 'none'; document.getElementById('2105.04086v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages,9 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.03858">arXiv:2105.03858</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.03858">pdf</a>, <a href="https://arxiv.org/format/2105.03858">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Location-Based Timing Advance Estimation for 5G Integrated LEO Satellite Communications
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wenjin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+T">Tingting Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+R">Rui Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Seco-Granados%2C+G">Gonzalo Seco-Granados</a>, 
      
      <a href="/search/?searchtype=author&amp;query=You%2C+L">Li You</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+X">Xiqi Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.03858v1-abstract-short" style="display: inline;">
        &hellip;the inherent characteristics of LEO satellite communication systems, e.g., wide beam coverage and long propagation delays, the existing 5G terrestrial uplink TA scheme is not <span class="search-hit mathjax">applicable</span> in the satellite networks. In this paper, we investigate location-based TA estimation for 5G integrated LEO satellite communication systems. We obtain the time difference of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.03858v1-abstract-full').style.display = 'inline'; document.getElementById('2105.03858v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.03858v1-abstract-full" style="display: none;">
        Integrated satellite-terrestrial communications networks aim to exploit both the satellite and the ground mobile communications, thus providing genuine ubiquitous coverage. For 5G integrated low earth orbit (LEO) satellite communication systems, the timing advance (TA) is required to be estimated in the initial random access procedure in order to facilitate the uplink frame alignment among different users. However, due to the inherent characteristics of LEO satellite communication systems, e.g., wide beam coverage and long propagation delays, the existing 5G terrestrial uplink TA scheme is not <span class="search-hit mathjax">applicable</span> in the satellite networks. In this paper, we investigate location-based TA estimation for 5G integrated LEO satellite communication systems. We obtain the time difference of arrival (TDOA) and frequency difference of arrival (FDOA) measurements in the downlink timing and frequency synchronization phase, which are made from the satellite at different time instants. We propose to take these measurements for either UE geolocation or ephemeris estimation, thus calculating the TA value. The estimation is then formulated as a quadratic <span class="search-hit mathjax">optimization</span> problem whose globally <span class="search-hit mathjax">optimal</span> solution can be obtained by a quadratic penalty algorithm. To <span class="search-hit mathjax">reduce</span> the computational complexity, we further propose an alternative approximation method based on iteratively <span class="search-hit mathjax">performing</span> a linearization procedure on the quadratic equality constraints. Numerical results show that the proposed methods can approach the constrained Cramer-Rao lower bound (CRLB) of the TA estimation and thus assure uplink frame alignment for different users.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.03858v1-abstract-full').style.display = 'none'; document.getElementById('2105.03858v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.03725">arXiv:2105.03725</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.03725">pdf</a>, <a href="https://arxiv.org/format/2105.03725">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DAMOV: A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Oliveira%2C+G+F">Geraldo F. Oliveira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=G%C3%B3mez-Luna%2C+J">Juan Gómez-Luna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Orosa%2C+L">Lois Orosa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghose%2C+S">Saugata Ghose</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vijaykumar%2C+N">Nandita Vijaykumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fernandez%2C+I">Ivan Fernandez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sadrosadati%2C+M">Mohammad Sadrosadati</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mutlu%2C+O">Onur Mutlu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.03725v4-abstract-short" style="display: inline;">
        Data movement between the CPU and main memory is a first-order obstacle against <span class="search-hit mathjax">improving</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.03725v4-abstract-full').style.display = 'inline'; document.getElementById('2105.03725v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.03725v4-abstract-full" style="display: none;">
        Data movement between the CPU and main memory is a first-order obstacle against <span class="search-hit mathjax">improving</span> <span class="search-hit mathjax">performance</span>, scalability, and energy <span class="search-hit mathjax">efficiency</span> in modern systems. Computer systems employ a range of techniques to <span class="search-hit mathjax">reduce</span> overheads tied to data movement, spanning from traditional mechanisms (e.g., deep multi-level cache hierarchies, aggressive hardware prefetchers) to emerging techniques such as Near-Data Processing (NDP), where some computation is moved close to memory. Our goal is to methodically identify potential sources of data movement over a broad set of <span class="search-hit mathjax">applications</span> and to comprehensively compare traditional compute-centric data movement mitigation techniques to more memory-centric techniques, thereby developing a rigorous understanding of the best techniques to mitigate each source of data movement.
  With this goal in mind, we <span class="search-hit mathjax">perform</span> the first large-scale characterization of a wide variety of <span class="search-hit mathjax">applications</span>, across a wide range of <span class="search-hit mathjax">application</span> domains, to identify fundamental <span class="search-hit mathjax">program</span> properties that lead to data movement to/from main memory. We develop the first systematic methodology to classify <span class="search-hit mathjax">applications</span> based on the sources contributing to data movement bottlenecks. From our large-scale characterization of 77K functions across 345 <span class="search-hit mathjax">applications</span>, we select 144 functions to form the first open-source benchmark suite (DAMOV) for main memory data movement studies. We select a diverse range of functions that (1) represent different types of data movement bottlenecks, and (2) come from a wide range of <span class="search-hit mathjax">application</span> domains. Using NDP as a case study, we identify new insights about the different data movement bottlenecks and use these insights to determine the most suitable data movement mitigation mechanism for a particular <span class="search-hit mathjax">application</span>. We open-source DAMOV and the complete source <span class="search-hit mathjax">code</span> for our new characterization methodology at https://github.com/CMU-SAFARI/DAMOV.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.03725v4-abstract-full').style.display = 'none'; document.getElementById('2105.03725v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Our open source <span class="search-hit mathjax">software</span> is available at https://github.com/CMU-SAFARI/DAMOV</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.02600">arXiv:2105.02600</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.02600">pdf</a>, <a href="https://arxiv.org/format/2105.02600">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Discrete Mathematics">cs.DM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Optimal</span> Subgraph on Disturbed Network
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guillot%2C+M">Matthieu Guillot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aghezzaf%2C+E">El-Houssaine Aghezzaf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faouzi%2C+N+E">Nour-Eddin El Faouzi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Furno%2C+A">Angelo Furno</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.02600v1-abstract-short" style="display: inline;">
        &hellip;transportation systems are drastically changed both qualitatively and quantitatively and the network has become obsolete. In this article, we study the problem of finding an <span class="search-hit mathjax">optimal</span> subnetwork that guarantee that (i) the minimal access time from any node of the urban network to the new network is not {\em too large} compared to the original transportation ne&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.02600v1-abstract-full').style.display = 'inline'; document.getElementById('2105.02600v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.02600v1-abstract-full" style="display: none;">
        During the pandemic of COVID-19, the demand of the transportation systems are drastically changed both qualitatively and quantitatively and the network has become obsolete. In this article, we study the problem of finding an <span class="search-hit mathjax">optimal</span> subnetwork that guarantee that (i) the minimal access time from any node of the urban network to the new network is not {\em too large} compared to the original transportation network; (ii) for any itinerary, the delay caused by the deletion of nodes of the transportation network is not {\em too big}; and (iii) the number of nodes of the transportation network has been <span class="search-hit mathjax">reduced</span> at least by a known factor. A solution is <span class="search-hit mathjax">optimal</span> if it induces a minimal global delay. We model this problem as a Mixed Integer Linear <span class="search-hit mathjax">Program</span> before applying the model on a real-case <span class="search-hit mathjax">application</span> on the Lyon&#39;s buses transportation network.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.02600v1-abstract-full').style.display = 'none'; document.getElementById('2105.02600v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.02540">arXiv:2105.02540</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.02540">pdf</a>, <a href="https://arxiv.org/format/2105.02540">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Distribution Awareness for AI System Testing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Berend%2C+D">David Berend</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.02540v1-abstract-short" style="display: inline;">
        As Deep Learning (DL) is continuously adopted in many safety critical <span class="search-hit mathjax">applications</span>, its quality and reliability start to raise concerns. Similar to the traditional&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.02540v1-abstract-full').style.display = 'inline'; document.getElementById('2105.02540v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.02540v1-abstract-full" style="display: none;">
        As Deep Learning (DL) is continuously adopted in many safety critical <span class="search-hit mathjax">applications</span>, its quality and reliability start to raise concerns. Similar to the traditional <span class="search-hit mathjax">software</span> development process, testing the DL <span class="search-hit mathjax">software</span> to uncover its defects at an early stage is an <span class="search-hit mathjax">effective</span> way to <span class="search-hit mathjax">reduce</span> risks after deployment. Although recent progress has been made in designing novel testing techniques for DL <span class="search-hit mathjax">software</span>, the distribution of generated test data is not taken into consideration. It is therefore hard to judge whether the identified errors are indeed meaningful errors to the DL <span class="search-hit mathjax">application</span>. Therefore, we propose a new OOD-guided testing technique which aims to generate new unseen test cases relevant to the underlying DL system task. Our results show that this technique is able to filter up to 55.44% of error test case on CIFAR-10 and is 10.05% more <span class="search-hit mathjax">effective</span> in enhancing robustness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.02540v1-abstract-full').style.display = 'none'; document.getElementById('2105.02540v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">2 pages, 1 figure, pre-print</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.02184">arXiv:2105.02184</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.02184">pdf</a>, <a href="https://arxiv.org/format/2105.02184">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+E">Enze Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wenhai Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+M">Mingyu Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+R">Ruimao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+P">Ping Luo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.02184v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Reducing</span> the complexity of the pipeline of instance segmentation is crucial for real-world&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.02184v1-abstract-full').style.display = 'inline'; document.getElementById('2105.02184v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.02184v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Reducing</span> the complexity of the pipeline of instance segmentation is crucial for real-world <span class="search-hit mathjax">applications</span>. This work addresses this issue by introducing an anchor-box free and single-shot instance segmentation framework, termed PolarMask, which reformulates the instance segmentation problem as predicting the contours of objects in the polar coordinate, with several appealing benefits. (1) The polar representation unifies instance segmentation (masks) and object detection (bounding boxes) into a single framework, <span class="search-hit mathjax">reducing</span> the design and computational complexity. (2) Two modules are carefully designed (i.e. soft polar centerness and polar IoU loss) to sample high-quality center examples and <span class="search-hit mathjax">optimize</span> polar contour regression, making the <span class="search-hit mathjax">performance</span> of PolarMask does not depend on the bounding box prediction results and thus becomes more <span class="search-hit mathjax">efficient</span> in training. (3) PolarMask is fully convolutional and can be easily embedded into most off-the-shelf detection methods. To further <span class="search-hit mathjax">improve</span> the <span class="search-hit mathjax">accuracy</span> of the framework, a Refined Feature Pyramid is introduced to further <span class="search-hit mathjax">improve</span> the feature representation at different scales, termed PolarMask++. Extensive experiments demonstrate the <span class="search-hit mathjax">effectiveness</span> of both PolarMask and PolarMask++, which achieve competitive results on instance segmentation in the challenging COCO dataset with single-model and single-scale training and testing, as well as new state-of-the-art results on rotate text detection and cell segmentation. We hope the proposed polar representation can provide a new perspective for designing algorithms to solve single-shot instance segmentation. The <span class="search-hit mathjax">codes</span> and models are available at: github.com/xieenze/PolarMask.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.02184v1-abstract-full').style.display = 'none'; document.getElementById('2105.02184v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">TPAMI 2021 Accepted</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.00657">arXiv:2105.00657</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.00657">pdf</a>, <a href="https://arxiv.org/ps/2105.00657">ps</a>, <a href="https://arxiv.org/format/2105.00657">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Distributionally robust risk map for learning-based motion planning and control: A semidefinite <span class="search-hit mathjax">programming</span> approach
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hakobyan%2C+A">Astghik Hakobyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+I">Insoon Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.00657v1-abstract-short" style="display: inline;">
        &hellip;characterizes allowable distribution errors. To resolve the infinite-dimensionality issue inherent in the construction of the DR-risk map, we derive a tractable semidefinite <span class="search-hit mathjax">programming</span> formulation that provides an upper bound of the risk, exploiting techniques from modern distributionally robust&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00657v1-abstract-full').style.display = 'inline'; document.getElementById('2105.00657v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.00657v1-abstract-full" style="display: none;">
        This paper proposes a novel safety specification tool, called the distributionally robust risk map (DR-risk map), for a mobile robot operating in a learning-enabled environment. Given the robot&#39;s position, the map aims to reliably assess the conditional value-at-risk (CVaR) of collision with obstacles whose movements are inferred by Gaussian process regression (GPR). Unfortunately, the inferred distribution is subject to errors, making it difficult to accurately evaluate the CVaR of collision. To overcome this challenge, this tool measures the risk under the worst-case distribution in a so-called ambiguity set that characterizes allowable distribution errors. To resolve the infinite-dimensionality issue inherent in the construction of the DR-risk map, we derive a tractable semidefinite <span class="search-hit mathjax">programming</span> formulation that provides an upper bound of the risk, exploiting techniques from modern distributionally robust <span class="search-hit mathjax">optimization</span>. As a concrete <span class="search-hit mathjax">application</span> for motion planning, a distributionally robust RRT* algorithm is considered using the risk map that addresses distribution errors caused by GPR. Furthermore, a motion control method is devised using the DR-risk map in a learning-based model predictive control (MPC) formulation. In particular, a neural network approximation of the risk map is proposed to <span class="search-hit mathjax">reduce</span> the computational cost in solving the MPC problem. The <span class="search-hit mathjax">performance</span> and utility of the proposed risk map are demonstrated through simulation studies that show its ability to ensure the safety of mobile robots despite learning errors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00657v1-abstract-full').style.display = 'none'; document.getElementById('2105.00657v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.00146">arXiv:2105.00146</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.00146">pdf</a>, <a href="https://arxiv.org/format/2105.00146">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EntrapNet: a Blockchain-Based Verification Protocol for Trustless Computing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Lei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+S">Serbiao Fang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.00146v1-abstract-short" style="display: inline;">
        &hellip;computing verification protocol, called EntrapNet, for distributed shared computing networks, an emerging underlying network for many internet of things (IoT) <span class="search-hit mathjax">applications</span>. EntrapNet borrows the idea from the practice of entrapment in criminal law to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00146v1-abstract-full').style.display = 'inline'; document.getElementById('2105.00146v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.00146v1-abstract-full" style="display: none;">
        In this paper, we propose a blockchain-based computing verification protocol, called EntrapNet, for distributed shared computing networks, an emerging underlying network for many internet of things (IoT) <span class="search-hit mathjax">applications</span>. EntrapNet borrows the idea from the practice of entrapment in criminal law to <span class="search-hit mathjax">reduce</span> the possibility of receiving incorrect computing results from trustless service providers who have offered the computing resources. Furthermore, we mathematically <span class="search-hit mathjax">optimize</span> EntrapNet to deal with the fundamental tradeoff of a network: security and <span class="search-hit mathjax">efficiency</span>. We present an asymptotic <span class="search-hit mathjax">optimal</span> solution to this <span class="search-hit mathjax">optimization</span>. It will be seen that EntrapNet can be <span class="search-hit mathjax">performed</span> as an independent and low-cost layer atop any trustless network that requires outsourced computing, thus making secure computing affordable and practical.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00146v1-abstract-full').style.display = 'none'; document.getElementById('2105.00146v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages. submitted to Journal Internet of Things</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.00115">arXiv:2105.00115</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.00115">pdf</a>, <a href="https://arxiv.org/format/2105.00115">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        QDOT: Quantized Dot Product Kernel for Approximate High-<span class="search-hit mathjax">Performance</span> Computing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Diffenderfer%2C+J">James Diffenderfer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Osei-Kuffuor%2C+D">Daniel Osei-Kuffuor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Menon%2C+H">Harshitha Menon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.00115v1-abstract-short" style="display: inline;">
        Approximate computing techniques have been successful in <span class="search-hit mathjax">reducing</span> computation and power costs in several domains. However, error sensitive&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00115v1-abstract-full').style.display = 'inline'; document.getElementById('2105.00115v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.00115v1-abstract-full" style="display: none;">
        Approximate computing techniques have been successful in <span class="search-hit mathjax">reducing</span> computation and power costs in several domains. However, error sensitive <span class="search-hit mathjax">applications</span> in high-<span class="search-hit mathjax">performance</span> computing are unable to benefit from existing approximate computing strategies that are not developed with guaranteed error bounds. While approximate computing techniques can be developed for individual high-<span class="search-hit mathjax">performance</span> computing <span class="search-hit mathjax">applications</span> by domain specialists, this often requires additional theoretical analysis and potentially extensive <span class="search-hit mathjax">software</span> modification. Hence, the development of low-level error-bounded approximate computing strategies that can be introduced into any high-<span class="search-hit mathjax">performance</span> computing <span class="search-hit mathjax">application</span> without requiring additional analysis or significant <span class="search-hit mathjax">software</span> alterations is desirable. In this paper, we provide a contribution in this direction by proposing a general framework for designing error-bounded approximate computing strategies and apply it to the dot product kernel to develop qdot -- an error-bounded approximate dot product kernel. Following the introduction of qdot, we <span class="search-hit mathjax">perform</span> a theoretical analysis that yields a deterministic bound on the relative approximation error introduced by qdot. Empirical tests are <span class="search-hit mathjax">performed</span> to illustrate the tightness of the derived error bound and to demonstrate the <span class="search-hit mathjax">effectiveness</span> of qdot on a synthetic dataset, as well as two scientific benchmarks -- Conjugate Gradient (CG) and the Power method. In particular, using qdot for the dot products in CG can result in a majority of components being perforated or quantized to half <span class="search-hit mathjax">precision</span> without increasing the iteration count required for convergence to the same solution as CG using a double <span class="search-hit mathjax">precision</span> dot product.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00115v1-abstract-full').style.display = 'none'; document.getElementById('2105.00115v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.00027">arXiv:2105.00027</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.00027">pdf</a>, <a href="https://arxiv.org/format/2105.00027">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Strongly Correlated Electrons">cond-mat.str-el</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Superconductivity">cond-mat.supr-con</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Memory Reduction using a Ring Abstraction over GPU RDMA for Distributed Quantum Monte Carlo Solver
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+W">Weile Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=D%27Azevedo%2C+E">Eduardo D&#39;Azevedo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huck%2C+K">Kevin Huck</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatterjee%2C+A">Arghya Chatterjee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hernandez%2C+O">Oscar Hernandez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaiser%2C+H">Hartmut Kaiser</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.00027v2-abstract-short" style="display: inline;">
        Scientific <span class="search-hit mathjax">applications</span> that run on leadership computing facilities often face the challenge of being unable to fit leading science cases onto accelerator devices due to memory constraints (memory-bound&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00027v2-abstract-full').style.display = 'inline'; document.getElementById('2105.00027v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.00027v2-abstract-full" style="display: none;">
        Scientific <span class="search-hit mathjax">applications</span> that run on leadership computing facilities often face the challenge of being unable to fit leading science cases onto accelerator devices due to memory constraints (memory-bound <span class="search-hit mathjax">applications</span>). In this work, the authors studied one such US Department of Energy mission-critical condensed matter physics <span class="search-hit mathjax">application</span>, Dynamical Cluster Approximation (DCA++), and this paper discusses how device memory-bound challenges were successfully <span class="search-hit mathjax">reduced</span> by proposing an <span class="search-hit mathjax">effective</span> &#34;all-to-all&#34; communication method -- a ring communication algorithm. This implementation takes advantage of acceleration on GPUs and remote direct memory access (RDMA) for fast data exchange between GPUs.
  Additionally, the ring algorithm was <span class="search-hit mathjax">optimized</span> with sub-ring communicators and multi-threaded support to further <span class="search-hit mathjax">reduce</span> communication overhead and expose more concurrency, respectively. The computation and communication were also analyzed by using the Autonomic <span class="search-hit mathjax">Performance</span> Environment for Exascale (APEX) profiling tool, and this paper further discusses the <span class="search-hit mathjax">performance</span> trade-off for the ring algorithm implementation. The memory analysis on the ring algorithm shows that the allocation size for the authors&#39; most memory-intensive data structure per GPU is now <span class="search-hit mathjax">reduced</span> to 1/p of the original size, where p is the number of GPUs in the ring communicator. The communication analysis suggests that the distributed Quantum Monte Carlo execution time grows linearly as sub-ring size increases, and the cost of messages passing through the network interface connector could be a limiting factor.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00027v2-abstract-full').style.display = 'none'; document.getElementById('2105.00027v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.14246">arXiv:2104.14246</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.14246">pdf</a>, <a href="https://arxiv.org/format/2104.14246">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/s11227-021-03951-w">10.1007/s11227-021-03951-w <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Legio: Fault Resiliency for Embarrassingly Parallel MPI <span class="search-hit mathjax">Applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rocco%2C+R">Roberto Rocco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gadioli%2C+D">Davide Gadioli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Palermo%2C+G">Gianluca Palermo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.14246v1-abstract-short" style="display: inline;">
        Due to the increasing size of HPC machines, the fault presence is becoming an eventuality that <span class="search-hit mathjax">applications</span> must face. Natively, MPI provides no support for the execution past the detection of a fault, and this is becoming more and more constraining. With the introduction of ULFM (User Level Fault Mitigation library), it has been provided with a possible way&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.14246v1-abstract-full').style.display = 'inline'; document.getElementById('2104.14246v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.14246v1-abstract-full" style="display: none;">
        Due to the increasing size of HPC machines, the fault presence is becoming an eventuality that <span class="search-hit mathjax">applications</span> must face. Natively, MPI provides no support for the execution past the detection of a fault, and this is becoming more and more constraining. With the introduction of ULFM (User Level Fault Mitigation library), it has been provided with a possible way to overtake a fault during the <span class="search-hit mathjax">application</span> execution at the cost of <span class="search-hit mathjax">code</span> modifications. ULFM is intrusive in the <span class="search-hit mathjax">application</span> and requires also a deep understanding of its recovery procedures.
  In this paper we propose Legio, a framework that lowers the complexity of introducing resiliency in an embarrassingly parallel MPI <span class="search-hit mathjax">application</span>. By hiding ULFM behind the MPI calls, the library is capable to expose resiliency features to the <span class="search-hit mathjax">application</span> in a transparent manner thus removing any integration effort. Upon fault, the failed nodes are discarded and the execution continues only with the non-failed ones. A hierarchical implementation of the solution has been also proposed to <span class="search-hit mathjax">reduce</span> the overhead of the repair process when scaling towards a large number of nodes.
  We evaluated our solutions on the Marconi100 cluster at CINECA, showing that the overhead introduced by the library is negligible and it does not limit the scalability properties of MPI. Moreover, we also integrated the solution in real-world <span class="search-hit mathjax">applications</span> to further prove its robustness by injecting faults.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.14246v1-abstract-full').style.display = 'none'; document.getElementById('2104.14246v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.14056">arXiv:2104.14056</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.14056">pdf</a>, <a href="https://arxiv.org/format/2104.14056">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Machine Learning Techniques for <span class="search-hit mathjax">Software</span> Quality Assurance: A Survey
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Omri%2C+S">Safa Omri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sinz%2C+C">Carsten Sinz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.14056v1-abstract-short" style="display: inline;">
        Over the last years, machine learning techniques have been applied to more and more <span class="search-hit mathjax">application</span> domains, including&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.14056v1-abstract-full').style.display = 'inline'; document.getElementById('2104.14056v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.14056v1-abstract-full" style="display: none;">
        Over the last years, machine learning techniques have been applied to more and more <span class="search-hit mathjax">application</span> domains, including <span class="search-hit mathjax">software</span> engineering and, especially, <span class="search-hit mathjax">software</span> quality assurance. Important <span class="search-hit mathjax">application</span> domains have been, e.g., <span class="search-hit mathjax">software</span> defect prediction or test case selection and prioritization. The ability to predict which components in a large <span class="search-hit mathjax">software</span> system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to <span class="search-hit mathjax">improve</span> the quality of the <span class="search-hit mathjax">software</span>. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Closely related to estimating defect-prone parts of a <span class="search-hit mathjax">software</span> system is the question of how to select and prioritize test cases, and indeed test case prioritization has been extensively researched as a means for <span class="search-hit mathjax">reducing</span> the time taken to discover regressions in <span class="search-hit mathjax">software</span>. In this survey, we discuss various approaches in both fault prediction and test case prioritization, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between <span class="search-hit mathjax">programs</span>&#39; semantics and fault prediction features. We also review recently proposed machine learning methods for test case prioritization (TCP), and their ability to <span class="search-hit mathjax">reduce</span> the cost of regression testing without negatively affecting fault detection capabilities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.14056v1-abstract-full').style.display = 'none'; document.getElementById('2104.14056v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.13209">arXiv:2104.13209</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.13209">pdf</a>, <a href="https://arxiv.org/format/2104.13209">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        K-Clique Counting on GPUs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Almasri%2C+M">Mohammad Almasri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hajj%2C+I+E">Izzat El Hajj</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nagi%2C+R">Rakesh Nagi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+J">Jinjun Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hwu%2C+W">Wen-mei Hwu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.13209v1-abstract-short" style="display: inline;">
        Counting k-cliques in a graph is an important problem in graph analysis with many <span class="search-hit mathjax">applications</span>. Counting k-cliques is typically done by traversing search trees starting at each vertex in the graph. An important&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.13209v1-abstract-full').style.display = 'inline'; document.getElementById('2104.13209v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.13209v1-abstract-full" style="display: none;">
        Counting k-cliques in a graph is an important problem in graph analysis with many <span class="search-hit mathjax">applications</span>. Counting k-cliques is typically done by traversing search trees starting at each vertex in the graph. An important <span class="search-hit mathjax">optimization</span> is to eliminate search tree branches that discover the same clique redundantly. Eliminating redundant clique discovery is typically done via graph orientation or pivoting. Parallel implementations for both of these approaches have demonstrated promising <span class="search-hit mathjax">performance</span> on CPUs. In this paper, we present our GPU implementations of k-clique counting for both the graph orientation and pivoting approaches. Our implementations explore both vertex-centric and edge-centric parallelization schemes, and replace recursive search tree traversal with iterative traversal based on an explicitly-managed shared stack. We also apply various <span class="search-hit mathjax">optimizations</span> to <span class="search-hit mathjax">reduce</span> memory consumption and <span class="search-hit mathjax">improve</span> the utilization of parallel execution resources. Our evaluation shows that our best GPU implementation outperforms the best state-of-the-art parallel CPU implementation by a geometric mean speedup of 12.39x, 6.21x, and 18.99x for k = 4, 7, and 10, respectively. We also evaluate the impact of the choice of parallelization scheme and the incremental speedup of each <span class="search-hit mathjax">optimization</span>. Our <span class="search-hit mathjax">code</span> will be open-sourced to enable further research on parallelizing k-clique counting on GPUs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.13209v1-abstract-full').style.display = 'none'; document.getElementById('2104.13209v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.12865">arXiv:2104.12865</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.12865">pdf</a>, <a href="https://arxiv.org/format/2104.12865">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Density Attention Network for Loop Filtering in Video Compression
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+C">Changyue Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+Y">Yan Ye</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.12865v1-abstract-short" style="display: inline;">
        Video compression is a basic requirement for consumer and professional video <span class="search-hit mathjax">applications</span> alike. Video&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.12865v1-abstract-full').style.display = 'inline'; document.getElementById('2104.12865v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.12865v1-abstract-full" style="display: none;">
        Video compression is a basic requirement for consumer and professional video <span class="search-hit mathjax">applications</span> alike. Video <span class="search-hit mathjax">coding</span> standards such as H.264/AVC and H.265/HEVC are widely deployed in the market to enable <span class="search-hit mathjax">efficient</span> use of bandwidth and storage for many video <span class="search-hit mathjax">applications</span>. To <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">coding</span> artifacts and <span class="search-hit mathjax">improve</span> the compression <span class="search-hit mathjax">efficiency</span>, neural network based loop filtering of the reconstructed video has been developed in the literature. However, loop filtering is a challenging task due to the variation in video content and sampling densities. In this paper, we propose a on-line scaling based multi-density attention network for loop filtering in video compression. The core of our approach lies in several aspects: (a) parallel multi-resolution convolution streams for extracting multi-density features, (b) single attention branch to learn the sample correlations and generate mask maps, (c) a channel-mutual attention procedure to fuse the data from multiple branches, (d) on-line scaling technique to further <span class="search-hit mathjax">optimize</span> the output results of network according to the actual signal. The proposed multi-density attention network learns rich features from multiple sampling densities and <span class="search-hit mathjax">performs</span> robustly on video content of different resolutions. Moreover, the online scaling process enhances the signal adaptability of the off-line pre-trained model. Experimental results show that 10.18% bit-rate reduction at the same video quality can be achieved over the latest Versatile Video <span class="search-hit mathjax">Coding</span> (VVC) standard. The objective <span class="search-hit mathjax">performance</span> of the proposed algorithm outperforms the state-of-the-art methods and the subjective quality <span class="search-hit mathjax">improvement</span> is obvious in terms of detail preservation and artifact alleviation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.12865v1-abstract-full').style.display = 'none'; document.getElementById('2104.12865v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.12586">arXiv:2104.12586</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.12586">pdf</a>, <a href="https://arxiv.org/ps/2104.12586">ps</a>, <a href="https://arxiv.org/format/2104.12586">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Consistency issues in Gaussian Mixture Models reduction algorithms
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=D%27Ortenzio%2C+A">A. D&#39;Ortenzio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Manes%2C+C">C. Manes</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.12586v1-abstract-short" style="display: inline;">
        In many contexts Gaussian Mixtures (GM) are used to approximate probability distributions, possibly time-varying. In some <span class="search-hit mathjax">applications</span> the number of GM components exponentially increases over time, and reduction procedures are required to keep them reasonably limited. The GM reduction (GMR) problem can be formulated by choosing different measures of the diss&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.12586v1-abstract-full').style.display = 'inline'; document.getElementById('2104.12586v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.12586v1-abstract-full" style="display: none;">
        In many contexts Gaussian Mixtures (GM) are used to approximate probability distributions, possibly time-varying. In some <span class="search-hit mathjax">applications</span> the number of GM components exponentially increases over time, and reduction procedures are required to keep them reasonably limited. The GM reduction (GMR) problem can be formulated by choosing different measures of the dissimilarity of GMs before and after reduction, like the Kullback-Leibler Divergence (KLD) and the Integral Squared Error (ISE). Since in no case the solution is obtained in closed form, many approximate GMR algorithms have been proposed in the past three decades, although none of them provides <span class="search-hit mathjax">optimality</span> guarantees. In this work we discuss the importance of the choice of the dissimilarity measure and the issue of consistency of all steps of a reduction algorithm with the chosen measure. Indeed, most of the existing GMR algorithms are composed by several steps which are not consistent with a unique measure, and for this reason may produce <span class="search-hit mathjax">reduced</span> GMs far from <span class="search-hit mathjax">optimality</span>. In particular, the use of the KLD, of the ISE and normalized ISE is discussed and compared in this perspective.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.12586v1-abstract-full').style.display = 'none'; document.getElementById('2104.12586v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.11904">arXiv:2104.11904</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.11904">pdf</a>, <a href="https://arxiv.org/format/2104.11904">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Spatial-Spectral Clustering with Anchor Graph for Hyperspectral Image
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Qi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miao%2C+Y">Yanling Miao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+M">Mulin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xuelong Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.11904v1-abstract-short" style="display: inline;">
        Hyperspectral image (HSI) clustering, which aims at dividing hyperspectral pixels into clusters, has drawn significant attention in practical <span class="search-hit mathjax">applications</span>. Recently, many graph-based clustering methods, which construct an adjacent graph to model the data relationship, have shown dominant&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.11904v1-abstract-full').style.display = 'inline'; document.getElementById('2104.11904v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.11904v1-abstract-full" style="display: none;">
        Hyperspectral image (HSI) clustering, which aims at dividing hyperspectral pixels into clusters, has drawn significant attention in practical <span class="search-hit mathjax">applications</span>. Recently, many graph-based clustering methods, which construct an adjacent graph to model the data relationship, have shown dominant <span class="search-hit mathjax">performance</span>. However, the high dimensionality of HSI data makes it hard to construct the pairwise adjacent graph. Besides, abundant spatial structures are often overlooked during the clustering procedure. In order to better handle the high dimensionality problem and preserve the spatial structures, this paper proposes a novel unsupervised approach called spatial-spectral clustering with anchor graph (SSCAG) for HSI data clustering. The SSCAG has the following contributions: 1) the anchor graph-based strategy is used to construct a tractable large graph for HSI data, which <span class="search-hit mathjax">effectively</span> exploits all data points and <span class="search-hit mathjax">reduces</span> the computational complexity; 2) a new similarity metric is presented to embed the spatial-spectral information into the combined adjacent graph, which can mine the intrinsic property structure of HSI data; 3) an <span class="search-hit mathjax">effective</span> neighbors assignment strategy is adopted in the <span class="search-hit mathjax">optimization</span>, which <span class="search-hit mathjax">performs</span> the singular value decomposition (SVD) on the adjacent graph to get solutions <span class="search-hit mathjax">efficiently</span>. Extensive experiments on three public HSI datasets show that the proposed SSCAG is competitive against the state-of-the-art approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.11904v1-abstract-full').style.display = 'none'; document.getElementById('2104.11904v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.10826">arXiv:2104.10826</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.10826">pdf</a>, <a href="https://arxiv.org/format/2104.10826">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-<span class="search-hit mathjax">optimizing</span> loop sifting and majorization for 3D reconstruction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+G">Guoxiang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">YangQuan Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.10826v1-abstract-short" style="display: inline;">
        &hellip;and mapping (vSLAM) and 3D reconstruction methods have gone through impressive progress. These methods are very promising for autonomous vehicle and consumer robot <span class="search-hit mathjax">applications</span> because they can map large-scale environments such as cities and indoor environments without the need for much human effort. However, when it comes to loop detection and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10826v1-abstract-full').style.display = 'inline'; document.getElementById('2104.10826v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.10826v1-abstract-full" style="display: none;">
        Visual simultaneous localization and mapping (vSLAM) and 3D reconstruction methods have gone through impressive progress. These methods are very promising for autonomous vehicle and consumer robot <span class="search-hit mathjax">applications</span> because they can map large-scale environments such as cities and indoor environments without the need for much human effort. However, when it comes to loop detection and <span class="search-hit mathjax">optimization</span>, there is still room for <span class="search-hit mathjax">improvement</span>. vSLAM systems tend to add the loops very conservatively to <span class="search-hit mathjax">reduce</span> the severe influence of the false loops. These conservative checks usually lead to correct loops rejected, thus decrease <span class="search-hit mathjax">performance</span>. In this paper, an algorithm that can sift and majorize loop detections is proposed. Our proposed algorithm can compare the usefulness and <span class="search-hit mathjax">effectiveness</span> of different loops with the dense map posterior (DMP) metric. The algorithm tests and decides the acceptance of each loop without a single user-defined threshold. Thus it is adaptive to different data conditions. The proposed method is general and agnostic to sensor type (as long as depth or LiDAR reading presents), loop detection, and <span class="search-hit mathjax">optimization</span> methods. Neither does it require a specific type of SLAM system. Thus it has great potential to be applied to various <span class="search-hit mathjax">application</span> scenarios. Experiments are conducted on public datasets. Results show that the proposed method outperforms state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10826v1-abstract-full').style.display = 'none'; document.getElementById('2104.10826v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.10716">arXiv:2104.10716</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.10716">pdf</a>, <a href="https://arxiv.org/format/2104.10716">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Accelerating SpMM Kernel with Cache-First Edge Sampling for Graph Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+C">Chien-Yu Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+L">Liang Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ceze%2C+L">Luis Ceze</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.10716v2-abstract-short" style="display: inline;">
        &hellip;deep learning model class, can extract meaningful representations from highly expressive graph-structured data and are therefore gaining popularity for wider ranges of <span class="search-hit mathjax">applications</span>. However, current GNNs suffer from the poor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10716v2-abstract-full').style.display = 'inline'; document.getElementById('2104.10716v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.10716v2-abstract-full" style="display: none;">
        Graph neural networks (GNNs), an emerging deep learning model class, can extract meaningful representations from highly expressive graph-structured data and are therefore gaining popularity for wider ranges of <span class="search-hit mathjax">applications</span>. However, current GNNs suffer from the poor <span class="search-hit mathjax">performance</span> of their sparse-dense matrix multiplication (SpMM) operator, even when using powerful GPUs. Our analysis shows that 95% of the inference time could be spent on SpMM when running popular GNN models on NVIDIA&#39;s advanced V100 GPU. Such SpMM <span class="search-hit mathjax">performance</span> bottleneck hinders GNNs&#39; <span class="search-hit mathjax">applicability</span> to large-scale problems or the development of more sophisticated GNN models. To address this inference time bottleneck, we introduce ES-SpMM, a cache-first edge sampling mechanism and codesigned SpMM kernel. ES-SpMM uses edge sampling to downsize the graph to fit into GPU&#39;s shared memory. It thus <span class="search-hit mathjax">reduces</span> the computation cost and <span class="search-hit mathjax">improves</span> SpMM&#39;s cache locality. To evaluate ES-SpMM&#39;s <span class="search-hit mathjax">performance</span>, we integrated it with a popular GNN framework, DGL, and tested it using representative GNN models and datasets. Our results show that ES-SpMM outperforms the highly <span class="search-hit mathjax">optimized</span> cuSPARSE SpMM kernel by up to 4.35x with no <span class="search-hit mathjax">accuracy</span> loss and by 45.3x with less than a 1% <span class="search-hit mathjax">accuracy</span> loss.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10716v2-abstract-full').style.display = 'none'; document.getElementById('2104.10716v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.10403">arXiv:2104.10403</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.10403">pdf</a>, <a href="https://arxiv.org/ps/2104.10403">ps</a>, <a href="https://arxiv.org/format/2104.10403">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Model-aided Deep Reinforcement Learning for Sample-<span class="search-hit mathjax">efficient</span> UAV Trajectory Design in IoT Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Esrafilian%2C+O">Omid Esrafilian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bayerlein%2C+H">Harald Bayerlein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gesbert%2C+D">David Gesbert</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.10403v2-abstract-short" style="display: inline;">
        &hellip;learning hence relying on very little prior contextual information. A corresponding drawback however lies in the need for many learning episodes which severely restricts the <span class="search-hit mathjax">applicability</span> of such approach in real-world time- and energy-constrained missions. Here, we propose a model-aided deep Q-learning approach that, in contrast to previous work, considerab&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10403v2-abstract-full').style.display = 'inline'; document.getElementById('2104.10403v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.10403v2-abstract-full" style="display: none;">
        Deep Reinforcement Learning (DRL) is gaining attention as a potential approach to design trajectories for autonomous unmanned aerial vehicles (UAV) used as flying access points in the context of cellular or Internet of Things (IoT) connectivity. DRL solutions offer the advantage of on-the-go learning hence relying on very little prior contextual information. A corresponding drawback however lies in the need for many learning episodes which severely restricts the <span class="search-hit mathjax">applicability</span> of such approach in real-world time- and energy-constrained missions. Here, we propose a model-aided deep Q-learning approach that, in contrast to previous work, considerably <span class="search-hit mathjax">reduces</span> the need for extensive training data samples, while still achieving the overarching goal of DRL, i.e to guide a battery-limited UAV towards an <span class="search-hit mathjax">efficient</span> data harvesting trajectory, without prior knowledge of wireless channel characteristics and limited knowledge of wireless node locations. The key idea consists in using a small subset of nodes as anchors (i.e. with known location) and learning a model of the propagation environment while implicitly estimating the positions of regular nodes. Interaction with the model allows us to train a deep Q-network (DQN) to approximate the <span class="search-hit mathjax">optimal</span> UAV control policy. We show that in comparison with standard DRL approaches, the proposed model-aided approach requires at least one order of magnitude less training data samples to reach identical data collection <span class="search-hit mathjax">performance</span>, hence offering a first step towards making DRL a viable solution to the problem.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10403v2-abstract-full').style.display = 'none'; document.getElementById('2104.10403v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, 2 figures, submitted to GLOBECOM 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.10314">arXiv:2104.10314</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.10314">pdf</a>, <a href="https://arxiv.org/format/2104.10314">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Efficient</span> Sparse <span class="search-hit mathjax">Coding</span> using Hierarchical Riemannian Pursuit
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+Y">Ye Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lau%2C+V">Vincent Lau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+S">Songfu Cai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.10314v3-abstract-short" style="display: inline;">
        Sparse <span class="search-hit mathjax">coding</span> is a class of unsupervised methods for learning a sparse representation of the input data in the form of a linear combination of a dictionary and a sparse&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10314v3-abstract-full').style.display = 'inline'; document.getElementById('2104.10314v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.10314v3-abstract-full" style="display: none;">
        Sparse <span class="search-hit mathjax">coding</span> is a class of unsupervised methods for learning a sparse representation of the input data in the form of a linear combination of a dictionary and a sparse <span class="search-hit mathjax">code</span>. This learning framework has led to state-of-the-art results in various image and video processing tasks. However, classical methods learn the dictionary and the sparse <span class="search-hit mathjax">code</span> based on alternative <span class="search-hit mathjax">optimizations</span>, usually without theoretical guarantees for either <span class="search-hit mathjax">optimality</span> or convergence due to non-convexity of the problem. Recent works on sparse <span class="search-hit mathjax">coding</span> with a complete dictionary provide strong theoretical guarantees thanks to the development of the non-convex <span class="search-hit mathjax">optimization</span>. However, initial non-convex approaches learn the dictionary in the sparse <span class="search-hit mathjax">coding</span> problem sequentially in an atom-by-atom manner, which leads to a long execution time. More recent works seek to directly learn the entire dictionary at once, which substantially <span class="search-hit mathjax">reduces</span> the execution time. However, the associated recovery <span class="search-hit mathjax">performance</span> is degraded with a finite number of data samples. In this paper, we propose an <span class="search-hit mathjax">efficient</span> sparse <span class="search-hit mathjax">coding</span> scheme with a two-stage <span class="search-hit mathjax">optimization</span>. The proposed scheme leverages the global and local Riemannian geometry of the two-stage <span class="search-hit mathjax">optimization</span> problem and facilitates fast implementation for superb dictionary recovery <span class="search-hit mathjax">performance</span> by a finite number of samples without atom-by-atom calculation. We further prove that, with high probability, the proposed scheme can exactly recover any atom in the target dictionary with a finite number of samples if it is adopted to recover one atom of the dictionary. An <span class="search-hit mathjax">application</span> on wireless sensor data compression is also proposed. Experiments on both synthetic and real-world data verify the <span class="search-hit mathjax">efficiency</span> and <span class="search-hit mathjax">effectiveness</span> of the proposed scheme.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10314v3-abstract-full').style.display = 'none'; document.getElementById('2104.10314v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been accepted by IEEE Transactions on Signal Processing</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.09252">arXiv:2104.09252</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.09252">pdf</a>, <a href="https://arxiv.org/format/2104.09252">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning on Hardware: A Tutorial on Neural Network Accelerators and Co-Processors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Baischer%2C+L">Lukas Baischer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wess%2C+M">Matthias Wess</a>, 
      
      <a href="/search/?searchtype=author&amp;query=TaheriNejad%2C+N">Nima TaheriNejad</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.09252v1-abstract-short" style="display: inline;">
        &hellip;that they can take into account a large number of parameters, which enables them to solve complex tasks. In computer vision and speech recognition, they have a better <span class="search-hit mathjax">accuracy</span> than common algorithms, and in some tasks, they boast an even higher&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.09252v1-abstract-full').style.display = 'inline'; document.getElementById('2104.09252v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.09252v1-abstract-full" style="display: none;">
        Deep neural networks (DNNs) have the advantage that they can take into account a large number of parameters, which enables them to solve complex tasks. In computer vision and speech recognition, they have a better <span class="search-hit mathjax">accuracy</span> than common algorithms, and in some tasks, they boast an even higher <span class="search-hit mathjax">accuracy</span> than human experts. With the progress of DNNs in recent years, many other fields of <span class="search-hit mathjax">application</span> such as diagnosis of diseases and autonomous driving are taking advantage of them. The trend at DNNs is clear: The network size is growing exponentially, which leads to an exponential increase in computational effort and required memory size. For this reason, <span class="search-hit mathjax">optimized</span> hardware accelerators are used to increase the <span class="search-hit mathjax">performance</span> of the inference of neuronal networks. However, there are various neural network hardware accelerator platforms, such as graphics processing units (GPUs), <span class="search-hit mathjax">application</span> specific integrated circuits (ASICs) and field programmable gate arrays (FPGAs). Each of these platforms offer certain advantages and disadvantages. Also, there are various methods for <span class="search-hit mathjax">reducing</span> the computational effort of DNNs, which are differently suitable for each hardware accelerator. In this article an overview of existing neural network hardware accelerators and acceleration methods is given. Their strengths and weaknesses are shown and a recommendation of suitable <span class="search-hit mathjax">applications</span> is given. In particular, we focus on acceleration of the inference of convolutional neural networks (CNNs) used for image recognition tasks. Given that there exist many different hardware architectures. FPGA-based implementations are well-suited to show the <span class="search-hit mathjax">effect</span> of DNN <span class="search-hit mathjax">optimization</span> methods on <span class="search-hit mathjax">accuracy</span> and throughput. For this reason, the focus of this work is more on FPGA-based implementations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.09252v1-abstract-full').style.display = 'none'; document.getElementById('2104.09252v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.08490">arXiv:2104.08490</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.08490">pdf</a>, <a href="https://arxiv.org/format/2104.08490">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dual Metric Learning for <span class="search-hit mathjax">Effective</span> and <span class="search-hit mathjax">Efficient</span> Cross-Domain Recommendations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+P">Pan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tuzhilin%2C+A">Alexander Tuzhilin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.08490v2-abstract-short" style="display: inline;">
        Cross domain recommender systems have been increasingly valuable for helping consumers identify useful items in different <span class="search-hit mathjax">applications</span>. However, existing cross-domain models typically require large number of overlap users, which can be difficult to obtain in some&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.08490v2-abstract-full').style.display = 'inline'; document.getElementById('2104.08490v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.08490v2-abstract-full" style="display: none;">
        Cross domain recommender systems have been increasingly valuable for helping consumers identify useful items in different <span class="search-hit mathjax">applications</span>. However, existing cross-domain models typically require large number of overlap users, which can be difficult to obtain in some <span class="search-hit mathjax">applications</span>. In addition, they did not consider the duality structure of cross-domain recommendation tasks, thus failing to take into account bidirectional latent relations between users and items and achieve <span class="search-hit mathjax">optimal</span> recommendation <span class="search-hit mathjax">performance</span>. To address these issues, in this paper we propose a novel cross-domain recommendation model based on dual learning that transfers information between two related domains in an iterative manner until the learning process stabilizes. We develop a novel latent orthogonal mapping to extract user preferences over multiple domains while preserving relations between users across different latent spaces. Furthermore, we combine the dual learning method with the metric learning approach, which allows us to significantly <span class="search-hit mathjax">reduce</span> the required common user overlap across the two domains and leads to even better cross-domain recommendation <span class="search-hit mathjax">performance</span>. We test the proposed model on two large-scale industrial datasets and six domain pairs, demonstrating that it consistently and significantly outperforms all the state-of-the-art baselines. We also show that the proposed model works well with very few overlap users to obtain satisfying recommendation <span class="search-hit mathjax">performance</span> comparable to the state-of-the-art baselines that use many overlap users.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.08490v2-abstract-full').style.display = 'none'; document.getElementById('2104.08490v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to IEEE TKDE. arXiv admin note: text overlap with arXiv:1910.05189</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.08002">arXiv:2104.08002</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.08002">pdf</a>, <a href="https://arxiv.org/format/2104.08002">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Efficient</span> and Generic 1D Dilated Convolution Layer for Deep Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chaudhary%2C+N">Narendra Chaudhary</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Misra%2C+S">Sanchit Misra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kalamkar%2C+D">Dhiraj Kalamkar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Heinecke%2C+A">Alexander Heinecke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Georganas%2C+E">Evangelos Georganas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ziv%2C+B">Barukh Ziv</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adelman%2C+M">Menachem Adelman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaul%2C+B">Bharat Kaul</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.08002v1-abstract-short" style="display: inline;">
        Convolutional neural networks (CNNs) have found many <span class="search-hit mathjax">applications</span> in tasks involving two-dimensional (2D) data, such as image classification and image processing. Therefore, 2D convolution layers have been heavily&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.08002v1-abstract-full').style.display = 'inline'; document.getElementById('2104.08002v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.08002v1-abstract-full" style="display: none;">
        Convolutional neural networks (CNNs) have found many <span class="search-hit mathjax">applications</span> in tasks involving two-dimensional (2D) data, such as image classification and image processing. Therefore, 2D convolution layers have been heavily <span class="search-hit mathjax">optimized</span> on CPUs and GPUs. However, in many <span class="search-hit mathjax">applications</span> - for example genomics and speech recognition, the data can be one-dimensional (1D). Such <span class="search-hit mathjax">applications</span> can benefit from <span class="search-hit mathjax">optimized</span> 1D convolution layers. In this work, we introduce our <span class="search-hit mathjax">efficient</span> implementation of a generic 1D convolution layer covering a wide range of parameters. It is <span class="search-hit mathjax">optimized</span> for x86 CPU architectures, in particular, for architectures containing Intel AVX-512 and AVX-512 BFloat16 instructions. We use the LIBXSMM library&#39;s batch-<span class="search-hit mathjax">reduce</span> General Matrix Multiplication (BRGEMM) kernel for FP32 and BFloat16 <span class="search-hit mathjax">precision</span>. We demonstrate that our implementation can achieve up to 80% <span class="search-hit mathjax">efficiency</span> on Intel Xeon Cascade Lake and Cooper Lake CPUs. Additionally, we show the generalization capability of our BRGEMM based approach by achieving high <span class="search-hit mathjax">efficiency</span> across a range of parameters. We consistently achieve higher <span class="search-hit mathjax">efficiency</span> than the 1D convolution layer with Intel oneDNN library backend for varying input tensor widths, filter widths, number of channels, filters, and dilation parameters. Finally, we demonstrate the <span class="search-hit mathjax">performance</span> of our <span class="search-hit mathjax">optimized</span> 1D convolution layer by utilizing it in the end-to-end neural network training with real genomics datasets and achieve up to 6.86x speedup over the oneDNN library-based implementation on Cascade Lake CPUs. We also demonstrate the scaling with 16 sockets of Cascade/Cooper Lake CPUs and achieve significant speedup over eight V100 GPUs using a similar power envelop. In the end-to-end training, we get a speedup of 1.41x on Cascade Lake with FP32, 1.57x on Cooper Lake with FP32, and 2.27x on Cooper Lake with BFloat16 over eight V100 GPUs with FP32.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.08002v1-abstract-full').style.display = 'none'; document.getElementById('2104.08002v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.07526">arXiv:2104.07526</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.07526">pdf</a>, <a href="https://arxiv.org/format/2104.07526">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rendering Point Clouds with Compute Shaders and Vertex Order <span class="search-hit mathjax">Optimization</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sch%C3%BCtz%2C+M">Markus Schütz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kerbl%2C+B">Bernhard Kerbl</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wimmer%2C+M">Michael Wimmer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.07526v1-abstract-short" style="display: inline;">
        &hellip;and achieve significantly better frame times than previous compute-based methods. Beyond basic closest-point rendering, we also introduce a fast, high-quality variant to <span class="search-hit mathjax">reduce</span> aliasing. We present and evaluate several variants of our proposed methods with different flavors of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.07526v1-abstract-full').style.display = 'inline'; document.getElementById('2104.07526v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.07526v1-abstract-full" style="display: none;">
        While commodity GPUs provide a continuously growing range of features and sophisticated methods for accelerating compute jobs, many state-of-the-art solutions for point cloud rendering still rely on the provided point primitives (GL_POINTS, POINTLIST, ...) of graphics APIs for image synthesis. In this paper, we present several compute-based point cloud rendering approaches that outperform the hardware pipeline by up to an order of magnitude and achieve significantly better frame times than previous compute-based methods. Beyond basic closest-point rendering, we also introduce a fast, high-quality variant to <span class="search-hit mathjax">reduce</span> aliasing. We present and evaluate several variants of our proposed methods with different flavors of <span class="search-hit mathjax">optimization</span>, in order to ensure their <span class="search-hit mathjax">applicability</span> and achieve <span class="search-hit mathjax">optimal</span> <span class="search-hit mathjax">performance</span> on a range of platforms and architectures with varying support for novel GPU hardware features. During our experiments, the observed peak <span class="search-hit mathjax">performance</span> was reached rendering 796 million points (12.7GB) at rates of 62 to 64 frames per second (50 billion points per second, 802GB/s) on an RTX 3090 without the use of level-of-detail structures.
  We further introduce an <span class="search-hit mathjax">optimized</span> vertex order for point clouds to boost the <span class="search-hit mathjax">efficiency</span> of GL_POINTS by a factor of 5x in cases where hardware rendering is compulsory. We compare different orderings and show that Morton sorted buffers are faster for some viewpoints, while shuffled vertex buffers are faster in others. In contrast, combining both approaches by first sorting according to Morton-<span class="search-hit mathjax">code</span> and shuffling the resulting sequence in batches of 128 points leads to a vertex buffer layout with high rendering <span class="search-hit mathjax">performance</span> and low sensitivity to viewpoint changes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.07526v1-abstract-full').style.display = 'none'; document.getElementById('2104.07526v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages content, 5 pages appendix</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.05158">arXiv:2104.05158</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.05158">pdf</a>, <a href="https://arxiv.org/format/2104.05158">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        High-<span class="search-hit mathjax">performance</span>, Distributed Training of Large-scale Deep Learning Recommendation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mudigere%2C+D">Dheevatsa Mudigere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+Y">Yuchen Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+J">Jianyu Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tulloch%2C+A">Andrew Tulloch</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sridharan%2C+S">Srinivas Sridharan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xing Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ozdal%2C+M">Mustafa Ozdal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nie%2C+J">Jade Nie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+J">Jongsoo Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+L">Liang Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J+A">Jie Amy Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+L">Leon Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ivchenko%2C+D">Dmytro Ivchenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Basant%2C+A">Aarti Basant</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Y">Yuxi Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jiyan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ardestani%2C+E+K">Ehsan K. Ardestani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaodong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Komuravelli%2C+R">Rakesh Komuravelli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+C">Ching-Hsiang Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yilmaz%2C+S">Serhat Yilmaz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Huayu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+J">Jiyuan Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+Z">Zhuobo Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Y">Yinbin Ma</a>
      , et al. (26 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.05158v3-abstract-short" style="display: inline;">
        Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI <span class="search-hit mathjax">application</span> in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.05158v3-abstract-full').style.display = 'inline'; document.getElementById('2104.05158v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.05158v3-abstract-full" style="display: none;">
        Deep learning recommendation models (DLRMs) are used across many business-critical services at Facebook and are the single largest AI <span class="search-hit mathjax">application</span> in terms of infrastructure demand in its data-centers. In this paper we discuss the SW/HW co-designed solution for high-<span class="search-hit mathjax">performance</span> distributed training of large-scale DLRMs. We introduce a high-<span class="search-hit mathjax">performance</span> scalable <span class="search-hit mathjax">software</span> stack based on PyTorch and pair it with the new evolution of Zion platform, namely ZionEX. We demonstrate the capability to train very large DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup in terms of time to solution over previous systems. We achieve this by (i) designing the ZionEX platform with dedicated scale-out network, provisioned with high bandwidth, <span class="search-hit mathjax">optimal</span> topology and <span class="search-hit mathjax">efficient</span> transport (ii) implementing an <span class="search-hit mathjax">optimized</span> PyTorch-based training stack supporting both model and data parallelism (iii) developing sharding algorithms capable of hierarchical partitioning of the embedding tables along row, column dimensions and load balancing them across multiple workers; (iv) adding high-<span class="search-hit mathjax">performance</span> core operators while retaining flexibility to support <span class="search-hit mathjax">optimizers</span> with fully deterministic updates (v) leveraging <span class="search-hit mathjax">reduced</span> <span class="search-hit mathjax">precision</span> communications, multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we develop and briefly comment on distributed data ingestion and other supporting services that are required for the robust and <span class="search-hit mathjax">efficient</span> end-to-end training in production environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.05158v3-abstract-full').style.display = 'none'; document.getElementById('2104.05158v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.04611">arXiv:2104.04611</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.04611">pdf</a>, <a href="https://arxiv.org/format/2104.04611">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Boosted <span class="search-hit mathjax">Automated</span> <span class="search-hit mathjax">Program</span> Repair
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Benton%2C+S">Samuel Benton</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Mengshi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xia Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Lingming Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.04611v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Program</span> repair is an integral part of every&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.04611v1-abstract-full').style.display = 'inline'; document.getElementById('2104.04611v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.04611v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Program</span> repair is an integral part of every <span class="search-hit mathjax">software</span> system&#39;s life-cycle but can be extremely challenging. To date, researchers have proposed various <span class="search-hit mathjax">automated</span> <span class="search-hit mathjax">program</span> repair (APR) techniques to <span class="search-hit mathjax">reduce</span> efforts of manual debugging. However, given a real-world buggy <span class="search-hit mathjax">program</span>, a typical APR technique usually generates a large number of patches, each of which needs to be validated against the original test suite which incurs extremely high computation costs. Although existing APR techniques have already leveraged various static and/or dynamic information to find the desired patches faster, they are still rather costly. In a recent work, researchers proposed unified debugging to leverage the patch execution information during APR to help boost fault localization; in this way,the <span class="search-hit mathjax">application</span> scope of APR techniques can be extended to all possible bugs, e.g., the patch execution information during APR can help with manual repair of the bugs that cannot be <span class="search-hit mathjax">automatically</span> fixed. Inspired by unified debugging, this work proposes SeAPR (Self-Boosted <span class="search-hit mathjax">Automated</span> <span class="search-hit mathjax">Program</span> Repair), the first technique to leverage the earlier patch execution information during APR to help boost <span class="search-hit mathjax">automated</span> repair itself on-the-fly. Our basic intuition is that patches similar to earlier high-quality/low-quality patches should be promoted/degraded to speed up the detection of the desired patches. This experimental study on 12 state-of-the-art APR systems demonstrates that, overall, SeAPR can substantially <span class="search-hit mathjax">reduce</span> the number of patch executions with negligible overhead. Our study also investigates the impact of various configurations on SeAPR. Lastly, our study demonstrates that SeAPR can even leverage the patch execution information from other APR tools from the same buggy <span class="search-hit mathjax">program</span> to further boost APR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.04611v1-abstract-full').style.display = 'none'; document.getElementById('2104.04611v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.04049">arXiv:2104.04049</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.04049">pdf</a>, <a href="https://arxiv.org/format/2104.04049">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantum-Assisted Feature Selection for Vehicle Price Prediction Modeling
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Von+Dollen%2C+D">David Von Dollen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Neukart%2C+F">Florian Neukart</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Weimer%2C+D">Daniel Weimer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=B%C3%A4ck%2C+T">Thomas Bäck</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.04049v2-abstract-short" style="display: inline;">
        Within machine learning model evaluation regimes, feature selection is a technique to <span class="search-hit mathjax">reduce</span> model complexity and <span class="search-hit mathjax">improve</span> model <span class="search-hit mathjax">performance</span> in regards to generalization, model fit, and <span class="search-hit mathjax">accuracy</span> of prediction. However, the search over the sp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.04049v2-abstract-full').style.display = 'inline'; document.getElementById('2104.04049v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.04049v2-abstract-full" style="display: none;">
        Within machine learning model evaluation regimes, feature selection is a technique to <span class="search-hit mathjax">reduce</span> model complexity and <span class="search-hit mathjax">improve</span> model <span class="search-hit mathjax">performance</span> in regards to generalization, model fit, and <span class="search-hit mathjax">accuracy</span> of prediction. However, the search over the space of features to find the subset of $k$ <span class="search-hit mathjax">optimal</span> features is a known NP-Hard problem. In this work, we study metrics for encoding the combinatorial search as a binary quadratic model, such as Generalized Mean Information Coefficient and Pearson Correlation Coefficient in <span class="search-hit mathjax">application</span> to the underlying regression problem of price prediction. We investigate trade-offs in the form of run-times and model <span class="search-hit mathjax">performance</span>, of leveraging quantum-assisted vs. classical subroutines for the combinatorial search, using minimum redundancy maximal relevancy as the heuristic for our approach. We achieve <span class="search-hit mathjax">accuracy</span> scores of 0.9 (in the range of [0,1]) for finding <span class="search-hit mathjax">optimal</span> subsets on synthetic data using a new metric that we define. We test and cross-validate predictive models on a real-world problem of price prediction, and show a <span class="search-hit mathjax">performance</span> <span class="search-hit mathjax">improvement</span> of mean absolute error scores for our quantum-assisted method $(1471.02 \pm{135.6})$, vs. similar methodologies such as recursive feature elimination $(1678.3 \pm{143.7})$. Our findings show that by leveraging quantum-assisted routines we find solutions that increase the quality of predictive model output while <span class="search-hit mathjax">reducing</span> the input dimensionality to the learning algorithm on synthetic and real-world data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.04049v2-abstract-full').style.display = 'none'; document.getElementById('2104.04049v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.04000">arXiv:2104.04000</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.04000">pdf</a>, <a href="https://arxiv.org/format/2104.04000">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Software</span>/Hardware Co-design for Multi-modal Multi-task Learning in Autonomous Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+C">Cong Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+D">Deming Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.04000v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Optimizing</span> the quality of result (QoR) and the quality of service (QoS) of AI-empowered autonomous systems simultaneously is very challenging. First, there are multiple input sources, e.g., multi-modal data from different sensors, requiring diverse data preprocessing, sensor fusion, and feature aggregation. Second, there are multiple tasks that require vario&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.04000v1-abstract-full').style.display = 'inline'; document.getElementById('2104.04000v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.04000v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Optimizing</span> the quality of result (QoR) and the quality of service (QoS) of AI-empowered autonomous systems simultaneously is very challenging. First, there are multiple input sources, e.g., multi-modal data from different sensors, requiring diverse data preprocessing, sensor fusion, and feature aggregation. Second, there are multiple tasks that require various AI models to run simultaneously, e.g., perception, localization, and control. Third, the computing and control system is heterogeneous, composed of hardware components with varied features, such as embedded CPUs, GPUs, FPGAs, and dedicated accelerators. Therefore, autonomous systems essentially require multi-modal multi-task (MMMT) learning which must be aware of hardware <span class="search-hit mathjax">performance</span> and implementation strategies. While MMMT learning has been attracting intensive research interests, its <span class="search-hit mathjax">applications</span> in autonomous systems are still underexplored. In this paper, we first discuss the opportunities of applying MMMT techniques in autonomous systems and then discuss the unique challenges that must be solved. In addition, we discuss the necessity and opportunities of MMMT model and hardware co-design, which is critical for autonomous systems especially with power/resource-limited or heterogeneous platforms. We formulate the MMMT model and heterogeneous hardware implementation co-design as a differentiable <span class="search-hit mathjax">optimization</span> problem, with the objective of <span class="search-hit mathjax">improving</span> the solution quality and <span class="search-hit mathjax">reducing</span> the overall power consumption and critical path latency. We advocate for further explorations of MMMT in autonomous systems and <span class="search-hit mathjax">software</span>/hardware co-design solutions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.04000v1-abstract-full').style.display = 'none'; document.getElementById('2104.04000v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Invited paper at IEEE AICAS 2021, 5 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.03888">arXiv:2104.03888</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.03888">pdf</a>, <a href="https://arxiv.org/format/2104.03888">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.neucom.2021.04.001">10.1016/j.neucom.2021.04.001 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Object Detection for Autonomous Driving by <span class="search-hit mathjax">Optimizing</span> Anchor Generation and Addressing Class Imbalance
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Carranza-Garc%C3%ADa%2C+M">Manuel Carranza-García</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lara-Ben%C3%ADtez%2C+P">Pedro Lara-Benítez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garc%C3%ADa-Guti%C3%A9rrez%2C+J">Jorge García-Gutiérrez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Riquelme%2C+J+C">José C. Riquelme</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.03888v1-abstract-short" style="display: inline;">
        &hellip;past years. Recent works have mainly focused on pushing the state-of-the-art in the general-purpose COCO benchmark. However, the use of such detection frameworks in specific <span class="search-hit mathjax">applications</span> such as autonomous driving is yet an area to be addressed. This study presents an enhanced 2D object detector based on Faster R-CNN that is better suited for the context of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.03888v1-abstract-full').style.display = 'inline'; document.getElementById('2104.03888v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.03888v1-abstract-full" style="display: none;">
        Object detection has been one of the most active topics in computer vision for the past years. Recent works have mainly focused on pushing the state-of-the-art in the general-purpose COCO benchmark. However, the use of such detection frameworks in specific <span class="search-hit mathjax">applications</span> such as autonomous driving is yet an area to be addressed. This study presents an enhanced 2D object detector based on Faster R-CNN that is better suited for the context of autonomous vehicles. Two main aspects are <span class="search-hit mathjax">improved</span>: the anchor generation procedure and the <span class="search-hit mathjax">performance</span> drop in minority classes. The default uniform anchor configuration is not suitable in this scenario due to the perspective projection of the vehicle cameras. Therefore, we propose a perspective-aware methodology that divides the image into key regions via clustering and uses evolutionary algorithms to <span class="search-hit mathjax">optimize</span> the base anchors for each of them. Furthermore, we add a module that enhances the <span class="search-hit mathjax">precision</span> of the second-stage header network by including the spatial information of the candidate regions proposed in the first stage. We also explore different re-weighting strategies to address the foreground-foreground class imbalance, showing that the use of a <span class="search-hit mathjax">reduced</span> version of focal loss can significantly <span class="search-hit mathjax">improve</span> the detection of difficult and underrepresented objects in two-stage detectors. Finally, we design an ensemble model to combine the strengths of the different learning strategies. Our proposal is evaluated with the Waymo Open Dataset, which is the most extensive and diverse up to date. The results demonstrate an average <span class="search-hit mathjax">accuracy</span> <span class="search-hit mathjax">improvement</span> of 6.13% mAP when using the best single model, and of 9.69% mAP with the ensemble. The proposed modifications over the Faster R-CNN do not increase computational cost and can easily be extended to <span class="search-hit mathjax">optimize</span> other anchor-based detection frameworks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.03888v1-abstract-full').style.display = 'none'; document.getElementById('2104.03888v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Neurocomputing, 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.03058">arXiv:2104.03058</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.03058">pdf</a>, <a href="https://arxiv.org/format/2104.03058">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Optimizing</span> Memory <span class="search-hit mathjax">Efficiency</span> of Graph Neural Networks on Edge Computing Platforms
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+A">Ao Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jianlei Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Y">Yeqi Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiao%2C+T">Tong Qiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qi%2C+Y">Yingjie Qi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaoyi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yunli Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+P">Pengcheng Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+W">Weisheng Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+C">Chunming Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.03058v2-abstract-short" style="display: inline;">
        Graph neural networks (GNN) have achieved state-of-the-art <span class="search-hit mathjax">performance</span> on various industrial tasks. However, the poor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.03058v2-abstract-full').style.display = 'inline'; document.getElementById('2104.03058v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.03058v2-abstract-full" style="display: none;">
        Graph neural networks (GNN) have achieved state-of-the-art <span class="search-hit mathjax">performance</span> on various industrial tasks. However, the poor <span class="search-hit mathjax">efficiency</span> of GNN inference and frequent Out-Of-Memory (OOM) problem limit the successful <span class="search-hit mathjax">application</span> of GNN on edge computing platforms. To tackle these problems, a feature decomposition approach is proposed for memory <span class="search-hit mathjax">efficiency</span> <span class="search-hit mathjax">optimization</span> of GNN inference. The proposed approach could achieve outstanding <span class="search-hit mathjax">optimization</span> on various GNN models, covering a wide range of datasets, which speeds up the inference by up to 3x. Furthermore, the proposed feature decomposition could significantly <span class="search-hit mathjax">reduce</span> the peak memory usage (up to 5x in memory <span class="search-hit mathjax">efficiency</span> <span class="search-hit mathjax">improvement</span>) and mitigate OOM problems during GNN inference.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.03058v2-abstract-full').style.display = 'none'; document.getElementById('2104.03058v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been accepted by RTAS 2021(brief industry track), with link to publicly available <span class="search-hit mathjax">code</span></span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.02214">arXiv:2104.02214</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.02214">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intelligent Building Control Systems for Thermal Comfort and Energy-<span class="search-hit mathjax">Efficiency</span>: A Systematic Review of Artificial Intelligence-Assisted Techniques
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Merabet%2C+G+H">Ghezlane Halhoul Merabet</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Essaaidi%2C+M">Mohamed Essaaidi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haddou%2C+M+B">Mohamed Ben Haddou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qolomany%2C+B">Basheer Qolomany</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qadir%2C+J">Junaid Qadir</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Anan%2C+M">Muhammad Anan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Al-Fuqaha%2C+A">Ala Al-Fuqaha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abid%2C+M+R">Mohamed Riduan Abid</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Benhaddou%2C+D">Driss Benhaddou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.02214v1-abstract-short" style="display: inline;">
        &hellip;primary energy consumed in most countries due to the proliferation of Heating, Ventilation and Air-Conditioning (HVAC) installations in response to the growing demand for <span class="search-hit mathjax">improved</span> thermal comfort.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02214v1-abstract-full').style.display = 'inline'; document.getElementById('2104.02214v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.02214v1-abstract-full" style="display: none;">
        Building operations represent a significant percentage of the total primary energy consumed in most countries due to the proliferation of Heating, Ventilation and Air-Conditioning (HVAC) installations in response to the growing demand for <span class="search-hit mathjax">improved</span> thermal comfort. <span class="search-hit mathjax">Reducing</span> the associated energy consumption while maintaining comfortable conditions in buildings are conflicting objectives and represent a typical <span class="search-hit mathjax">optimization</span> problem that requires intelligent system design. Over the last decade, different methodologies based on the Artificial Intelligence (AI) techniques have been deployed to find the sweet spot between energy use in HVAC systems and suitable indoor comfort levels to the occupants. This paper <span class="search-hit mathjax">performs</span> a comprehensive and an in-depth systematic review of AI-based techniques used for building control systems by assessing the outputs of these techniques, and their implementations in the reviewed works, as well as investigating their abilities to <span class="search-hit mathjax">improve</span> the energy-<span class="search-hit mathjax">efficiency</span>, while maintaining thermal comfort conditions. This enables a holistic view of (1) the complexities of delivering thermal comfort to users inside buildings in an energy-<span class="search-hit mathjax">efficient</span> way, and (2) the associated bibliographic material to assist researchers and experts in the field in tackling such a challenge. Among the 20 AI tools developed for both energy consumption and comfort control, functions such as identification and recognition patterns, <span class="search-hit mathjax">optimization</span>, predictive control. Based on the findings of this work, the <span class="search-hit mathjax">application</span> of AI technology in building control is a promising area of research and still an ongoing, i.e., the <span class="search-hit mathjax">performance</span> of AI-based control is not yet completely satisfactory. This is mainly due in part to the fact that these algorithms usually need a large amount of high-quality real-world data, which is lacking in the building or, more <span class="search-hit mathjax">precisely</span>, the energy sector.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02214v1-abstract-full').style.display = 'none'; document.getElementById('2104.02214v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: text overlap with arXiv:2006.12559</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.02188">arXiv:2104.02188</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.02188">pdf</a>, <a href="https://arxiv.org/format/2104.02188">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GPU Domain Specialization via Composable On-Package Architecture
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+Y">Yaosheng Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bolotin%2C+E">Evgeny Bolotin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatterjee%2C+N">Niladrish Chatterjee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nellans%2C+D">David Nellans</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keckler%2C+S+W">Stephen W. Keckler</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.02188v1-abstract-short" style="display: inline;">
        As GPUs scale their low <span class="search-hit mathjax">precision</span> matrix math throughput to boost deep learning (DL)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02188v1-abstract-full').style.display = 'inline'; document.getElementById('2104.02188v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.02188v1-abstract-full" style="display: none;">
        As GPUs scale their low <span class="search-hit mathjax">precision</span> matrix math throughput to boost deep learning (DL) <span class="search-hit mathjax">performance</span>, they upset the balance between math throughput and memory system capabilities. We demonstrate that converged GPU design trying to address diverging architectural requirements between FP32 (or larger) based HPC and FP16 (or smaller) based DL workloads results in sub-<span class="search-hit mathjax">optimal</span> configuration for either of the <span class="search-hit mathjax">application</span> domains. We argue that a Composable On-PAckage GPU (COPAGPU) architecture to provide domain-specialized GPU products is the most practical solution to these diverging requirements. A COPA-GPU leverages multi-chip-module disaggregation to support maximal design reuse, along with memory system specialization per <span class="search-hit mathjax">application</span> domain. We show how a COPA-GPU enables DL-specialized products by modular augmentation of the baseline GPU architecture with up to 4x higher off-die bandwidth, 32x larger on-package cache, 2.3x higher DRAM bandwidth and capacity, while conveniently supporting scaled-down HPC-oriented designs. This work explores the microarchitectural design necessary to enable composable GPUs and evaluates the benefits composability can provide to HPC, DL training, and DL inference. We show that when compared to a converged GPU design, a DL-<span class="search-hit mathjax">optimized</span> COPA-GPU featuring a combination of 16x larger cache capacity and 1.6x higher DRAM bandwidth scales per-GPU training and inference <span class="search-hit mathjax">performance</span> by 31% and 35% respectively and <span class="search-hit mathjax">reduces</span> the number of GPU instances by 50% in scale-out training scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02188v1-abstract-full').style.display = 'none'; document.getElementById('2104.02188v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.02162">arXiv:2104.02162</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.02162">pdf</a>, <a href="https://arxiv.org/format/2104.02162">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Near-<span class="search-hit mathjax">Precise</span> Parameter Approximation for Multiple Multiplications on A Single DSP Block
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kalali%2C+E">Ercan Kalali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=van+Leuken%2C+R">Rene van Leuken</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.02162v1-abstract-short" style="display: inline;">
        A multiply-accumulate (MAC) operation is the main computation unit for DSP <span class="search-hit mathjax">applications</span>. DSP blocks are one of the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02162v1-abstract-full').style.display = 'inline'; document.getElementById('2104.02162v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.02162v1-abstract-full" style="display: none;">
        A multiply-accumulate (MAC) operation is the main computation unit for DSP <span class="search-hit mathjax">applications</span>. DSP blocks are one of the <span class="search-hit mathjax">efficient</span> solutions to implement MACs in FPGA&#39;s. However, since the DSP blocks have wide multiplier and adder blocks, MAC operations using low bit-length parameters lead to an underutilization problem. Hence, an <span class="search-hit mathjax">efficient</span> approximation technique is introduced. The technique includes manipulation and approximation of the low bit-length fixed-point parameters based upon a Single DSP - Multiple Multiplication (SDMM) execution. The SDMM changes the traditional MAC implementation in the DSP block by separating multiplication and accumulation operations. While the accumulator hardware available in the DSP block is used for multiple parameter multiplication, parallel LUTs are employed for the accumulation part of the MAC operation. The <span class="search-hit mathjax">accuracy</span> of the developed <span class="search-hit mathjax">optimization</span> technique was evaluated for different CNN weight bit <span class="search-hit mathjax">precisions</span> using the Alexnet and VGG-16 networks and the Tiny ImageNet dataset. The <span class="search-hit mathjax">optimization</span> can be implemented without loss of <span class="search-hit mathjax">accuracy</span> in almost all cases, while it causes slight <span class="search-hit mathjax">accuracy</span> losses in a few cases. Through these <span class="search-hit mathjax">optimizations</span>, the SDMM is <span class="search-hit mathjax">performed</span> at the cost of a small hardware overhead. For example, a single DSP block executes 3 8-bit fixed-point parameter multiplications. As a result of our <span class="search-hit mathjax">optimizations</span>, the parameters are represented in a different format on off-chip memory, providing up to 33% compression without any hardware cost. The compression rate can be further increased by up to 97% when used in conjunction with other compression methods for the VGG-16. Reaching this compression rate requires extra hardware cost. A prototype systolic array architecture was implemented employing our <span class="search-hit mathjax">optimizations</span> on a Xilinx Zynq FPGA. It <span class="search-hit mathjax">reduced</span> the number of DSP blocks by 66.6%, 75%, and 83.3% for 8, 6, and 4-bit input variables, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02162v1-abstract-full').style.display = 'none'; document.getElementById('2104.02162v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 10 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.02151">arXiv:2104.02151</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.02151">pdf</a>, <a href="https://arxiv.org/format/2104.02151">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Distributed Learning in Wireless Networks: Recent Progress and Future Challenges
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+M">Mingzhe Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=G%C3%BCnd%C3%BCz%2C+D">Deniz Gündüz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+K">Kaibin Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saad%2C+W">Walid Saad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bennis%2C+M">Mehdi Bennis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feljan%2C+A+V">Aneta Vulgarakis Feljan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Poor%2C+H+V">H. Vincent Poor</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.02151v1-abstract-short" style="display: inline;">
        The next-generation of wireless networks will enable many machine learning (ML) tools and <span class="search-hit mathjax">applications</span> to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02151v1-abstract-full').style.display = 'inline'; document.getElementById('2104.02151v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.02151v1-abstract-full" style="display: none;">
        The next-generation of wireless networks will enable many machine learning (ML) tools and <span class="search-hit mathjax">applications</span> to <span class="search-hit mathjax">efficiently</span> analyze various types of data collected by edge devices for inference, autonomy, and decision making purposes. However, due to resource constraints, delay limitations, and privacy challenges, edge devices cannot offload their entire collected datasets to a cloud server for centrally training their ML models or inference purposes. To overcome these challenges, distributed learning and inference techniques have been proposed as a means to enable edge devices to collaboratively train ML models without raw data exchanges, thus <span class="search-hit mathjax">reducing</span> the communication overhead and latency as well as <span class="search-hit mathjax">improving</span> data privacy. However, deploying distributed learning over wireless networks faces several challenges including the uncertain wireless environment, limited wireless resources (e.g., transmit power and radio spectrum), and hardware resources. This paper provides a comprehensive study of how distributed learning can be <span class="search-hit mathjax">efficiently</span> and <span class="search-hit mathjax">effectively</span> deployed over wireless edge networks. We present a detailed overview of several emerging distributed learning paradigms, including federated learning, federated distillation, distributed inference, and multi-agent reinforcement learning. For each learning framework, we first introduce the motivation for deploying it over wireless networks. Then, we present a detailed literature review on the use of communication techniques for its <span class="search-hit mathjax">efficient</span> deployment. We then introduce an illustrative example to show how to <span class="search-hit mathjax">optimize</span> wireless networks to <span class="search-hit mathjax">improve</span> its <span class="search-hit mathjax">performance</span>. Finally, we introduce future research opportunities. In a nutshell, this paper provides a holistic set of guidelines on how to deploy a broad range of distributed learning frameworks over real-world wireless communication networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02151v1-abstract-full').style.display = 'none'; document.getElementById('2104.02151v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.01750">arXiv:2104.01750</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.01750">pdf</a>, <a href="https://arxiv.org/format/2104.01750">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Optimal</span> Sampling Gaps for Adaptive Submodular Maximization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+S">Shaojie Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+J">Jing Yuan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.01750v2-abstract-short" style="display: inline;">
        Running machine learning algorithms on large and rapidly growing volumes of data are often computationally expensive, one common trick to <span class="search-hit mathjax">reduce</span> the size of a data set, and thus&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.01750v2-abstract-full').style.display = 'inline'; document.getElementById('2104.01750v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.01750v2-abstract-full" style="display: none;">
        Running machine learning algorithms on large and rapidly growing volumes of data are often computationally expensive, one common trick to <span class="search-hit mathjax">reduce</span> the size of a data set, and thus <span class="search-hit mathjax">reduce</span> the computational cost of machine learning algorithms, is \emph{probability sampling}. It creates a sampled data set by including each data point from the original data set with a known probability. Although the benefit of running machine learning algorithms on the <span class="search-hit mathjax">reduced</span> data set is obvious, one major concern is that the <span class="search-hit mathjax">performance</span> of the solution obtained from samples might be much worse than that of the <span class="search-hit mathjax">optimal</span> solution when using the full data set. In this paper, we examine the <span class="search-hit mathjax">performance</span> loss caused by probability sampling in the context of adaptive submodular maximization. We consider a easiest probability sampling method which selects each data point independently with probability $r\in[0,1]$. We define sampling gap as the largest ratio of the <span class="search-hit mathjax">optimal</span> solution obtained from the full data set and the <span class="search-hit mathjax">optimal</span> solution obtained from the samples, over independence systems. Our main contribution is to show that if the utility function is policywise submodular, then for a given sampling rate $r$, the sampling gap is both upper bounded and lower bounded by $1/r$. One immediate implication of our result is that if we can find an $α$-approximation solution based on a sampled data set (which is sampled at sampling rate $r$), then this solution achieves an $αr$ approximation ratio for the original problem when using the full data set. We also show that the property of policywise submodular can be found in a wide range of real-world <span class="search-hit mathjax">applications</span>, including pool-based active learning and adaptive viral marketing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.01750v2-abstract-full').style.display = 'none'; document.getElementById('2104.01750v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.01192">arXiv:2104.01192</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.01192">pdf</a>, <a href="https://arxiv.org/format/2104.01192">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Energy-saving Cross-layer <span class="search-hit mathjax">Optimization</span> of Big Data Transfer Based on Historical Log Analysis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rodolph%2C+L">Lavone Rodolph</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nine%2C+M+S+Q+Z">MD S Q Zulkar Nine</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Di+Tacchio%2C+L">Luigi Di Tacchio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kosar%2C+T">Tevfik Kosar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.01192v1-abstract-short" style="display: inline;">
        &hellip;at the network end-systems. Although extensive research has been done on managing power consumption within the core networking infrastructure, there is little research on <span class="search-hit mathjax">reducing</span> the power consumption at the end-systems during active data transfers. This paper presents a novel cross-layer&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.01192v1-abstract-full').style.display = 'inline'; document.getElementById('2104.01192v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.01192v1-abstract-full" style="display: none;">
        With the proliferation of data movement across the Internet, global data traffic per year has already exceeded the Zettabyte scale. The network infrastructure and end-systems facilitating the vast data movement consume an extensive amount of electricity, measured in terawatt-hours per year. This massive energy footprint costs the world economy billions of dollars partially due to energy consumed at the network end-systems. Although extensive research has been done on managing power consumption within the core networking infrastructure, there is little research on <span class="search-hit mathjax">reducing</span> the power consumption at the end-systems during active data transfers. This paper presents a novel cross-layer <span class="search-hit mathjax">optimization</span> framework, called Cross-LayerHLA, to minimize energy consumption at the end-systems by applying machine learning techniques to historical transfer logs and extracting the hidden relationships between different parameters affecting both the <span class="search-hit mathjax">performance</span> and resource utilization. It utilizes offline analysis to <span class="search-hit mathjax">improve</span> online learning and dynamic tuning of <span class="search-hit mathjax">application</span>-level and kernel-level parameters with minimal overhead. This approach minimizes end-system energy consumption and maximizes data transfer throughput. Our experimental results show that Cross-LayerHLA outperforms other state-of-the-art solutions in this area.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.01192v1-abstract-full').style.display = 'none'; document.getElementById('2104.01192v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.01129">arXiv:2104.01129</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.01129">pdf</a>, <a href="https://arxiv.org/format/2104.01129">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Using Simulation to Aid the Design and <span class="search-hit mathjax">Optimization</span> of Intelligent User Interfaces for Quality Assurance Processes in Machine Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tennekes%2C+M">Martijn Tennekes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Jong%2C+T">Tim de Jong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Curier%2C+L">Lyana Curier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Coecke%2C+B">Bob Coecke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+M">Min Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.01129v1-abstract-short" style="display: inline;">
        Many mission-critical <span class="search-hit mathjax">applications</span> of machine learning (ML) in the real-world require a quality assurance (QA) process before the decisions or predictions of an ML model can be deployed. Because QA4ML users have to view a non-trivial amount of data and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.01129v1-abstract-full').style.display = 'inline'; document.getElementById('2104.01129v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.01129v1-abstract-full" style="display: none;">
        Many mission-critical <span class="search-hit mathjax">applications</span> of machine learning (ML) in the real-world require a quality assurance (QA) process before the decisions or predictions of an ML model can be deployed. Because QA4ML users have to view a non-trivial amount of data and <span class="search-hit mathjax">perform</span> many input actions to correct errors made by the ML model, an <span class="search-hit mathjax">optimally</span>-designed user interface (UI) can <span class="search-hit mathjax">reduce</span> the cost of interactions significantly. A UI&#39;s <span class="search-hit mathjax">effectiveness</span> can be affected by many factors, such as the number of data objects processed concurrently, the types of commands for correcting errors, and the availability of algorithms for assisting users. We propose using simulation to aid the design and <span class="search-hit mathjax">optimization</span> of intelligent user interfaces for QA4ML processes. In particular, we focus on simulating the combined <span class="search-hit mathjax">effects</span> of human intelligence in selecting appropriate commands and algorithms, and machine intelligence in providing a collection of general-purpose algorithms for reordering data objects to be quality-assured.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.01129v1-abstract-full').style.display = 'none'; document.getElementById('2104.01129v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.00142">arXiv:2104.00142</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.00142">pdf</a>, <a href="https://arxiv.org/format/2104.00142">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NodeSRT: A Selective Regression Testing Tool for Node.js <span class="search-hit mathjax">Application</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yufeng Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.00142v1-abstract-short" style="display: inline;">
        Node.js is one of the most popular frameworks for building web <span class="search-hit mathjax">applications</span>. As&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.00142v1-abstract-full').style.display = 'inline'; document.getElementById('2104.00142v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.00142v1-abstract-full" style="display: none;">
        Node.js is one of the most popular frameworks for building web <span class="search-hit mathjax">applications</span>. As <span class="search-hit mathjax">software</span> systems mature, the cost of running their entire regression test suite can become significant. Selective Regression Testing (SRT) is a technique that executes only a subset of tests the regression test suite can detect <span class="search-hit mathjax">software</span> failures more <span class="search-hit mathjax">efficiently</span>. Previous SRT studies mainly focused on standard desktop <span class="search-hit mathjax">applications</span>. Node.js <span class="search-hit mathjax">applications</span> are considered hard to <span class="search-hit mathjax">perform</span> test reduction because of Node&#39;s asynchronous, event-driven <span class="search-hit mathjax">programming</span> model and because JavaScript is a dynamic <span class="search-hit mathjax">programming</span> language. In this paper, we present NodeSRT, a Selective Regression Testing framework for Node.js <span class="search-hit mathjax">applications</span>. By <span class="search-hit mathjax">performing</span> static and dynamic analysis, NodeSRT identifies the relationship between changed methods and tests, then <span class="search-hit mathjax">reduces</span> the regression test suite to only tests that are affected by the change to <span class="search-hit mathjax">improve</span> the execution time of the regression test suite. To evaluate our selection technique, we applied NodeSRT to two open-source projects: Uppy and Simorgh, then compared our approach with the retest-all strategy and current industry-standard SRT technique: Jest OnlyChange. The results demonstrate that NodeSRT correctly selects affected tests based on changes and is 250% faster, 450% more <span class="search-hit mathjax">precise</span> than the Jest OnlyChange.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.00142v1-abstract-full').style.display = 'none'; document.getElementById('2104.00142v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.17231">arXiv:2103.17231</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.17231">pdf</a>, <a href="https://arxiv.org/format/2103.17231">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CDiNN -Convex Difference Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sankaranarayanan%2C+P">Parameswaran Sankaranarayanan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rengaswamy%2C+R">Raghunathan Rengaswamy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.17231v2-abstract-short" style="display: inline;">
        &hellip;shown to be universal function approximators and learn function mapping as non-smooth functions. Recently, there is considerable interest in the use of neural networks in <span class="search-hit mathjax">applications</span> such as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.17231v2-abstract-full').style.display = 'inline'; document.getElementById('2103.17231v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.17231v2-abstract-full" style="display: none;">
        Neural networks with ReLU activation function have been shown to be universal function approximators and learn function mapping as non-smooth functions. Recently, there is considerable interest in the use of neural networks in <span class="search-hit mathjax">applications</span> such as <span class="search-hit mathjax">optimal</span> control. It is well-known that <span class="search-hit mathjax">optimization</span> involving non-convex, non-smooth functions are computationally intensive and have limited convergence guarantees. Moreover, the choice of <span class="search-hit mathjax">optimization</span> hyper-parameters used in gradient descent/ascent significantly affect the quality of the obtained solutions. A new neural network architecture called the Input Convex Neural Networks (ICNNs) learn the output as a convex function of inputs thereby allowing the use of <span class="search-hit mathjax">efficient</span> convex <span class="search-hit mathjax">optimization</span> methods. Use of ICNNs for determining the input for minimizing output has two major problems: learning of a non-convex function as a convex mapping could result in significant function approximation error, and we also note that the existing representations cannot capture simple dynamic structures like linear time delay systems. We attempt to address the above problems by introduction of a new neural network architecture, which we call the CDiNN, which learns the function as a difference of polyhedral convex functions from data. We also discuss that, in some cases, the <span class="search-hit mathjax">optimal</span> input can be obtained from CDiNN through difference of convex <span class="search-hit mathjax">optimization</span> with convergence guarantees and that at each iteration, the problem is <span class="search-hit mathjax">reduced</span> to a linear <span class="search-hit mathjax">programming</span> problem.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.17231v2-abstract-full').style.display = 'none'; document.getElementById('2103.17231v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.16365">arXiv:2103.16365</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.16365">pdf</a>, <a href="https://arxiv.org/format/2103.16365">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Foveated Neural Radiance Fields for Real-Time and Egocentric Virtual Reality
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+N">Nianchen Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Z">Zhenyi He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+J">Jiannan Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chakravarthula%2C+P">Praneeth Chakravarthula</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xubo Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Q">Qi Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.16365v1-abstract-short" style="display: inline;">
        Traditional high-quality 3D graphics requires large volumes of fine-detailed scene data for rendering. This demand compromises computational <span class="search-hit mathjax">efficiency</span> and local storage resources. Specifically, it becomes more concerning for future wearable and portable virtual and augmented reality (VR/AR) displays. Recent approaches to combat this problem include remote r&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.16365v1-abstract-full').style.display = 'inline'; document.getElementById('2103.16365v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.16365v1-abstract-full" style="display: none;">
        Traditional high-quality 3D graphics requires large volumes of fine-detailed scene data for rendering. This demand compromises computational <span class="search-hit mathjax">efficiency</span> and local storage resources. Specifically, it becomes more concerning for future wearable and portable virtual and augmented reality (VR/AR) displays. Recent approaches to combat this problem include remote rendering/streaming and neural representations of 3D assets. These approaches have redefined the traditional local storage-rendering pipeline by distributed computing or compression of large data. However, these methods typically suffer from high latency or low quality for practical visualization of large immersive virtual scenes, notably with extra high resolution and refresh rate requirements for VR <span class="search-hit mathjax">applications</span> such as gaming and design.
  Tailored for the future portable, low-storage, and energy-<span class="search-hit mathjax">efficient</span> VR platforms, we present the first gaze-contingent 3D neural representation and view synthesis method. We incorporate the human psychophysics of visual- and stereo-acuity into an egocentric neural representation of 3D scenery. Furthermore, we jointly <span class="search-hit mathjax">optimize</span> the latency/<span class="search-hit mathjax">performance</span> and visual quality, while mutually bridging human perception and neural scene synthesis, to achieve perceptually high-quality immersive interaction. Both objective analysis and subjective study demonstrate the <span class="search-hit mathjax">effectiveness</span> of our approach in significantly <span class="search-hit mathjax">reducing</span> local storage volume and synthesis latency (up to 99% reduction in both data size and computational time), while simultaneously presenting high-fidelity rendering, with perceptual quality identical to that of fully locally stored and rendered high-quality imagery.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.16365v1-abstract-full').style.display = 'none'; document.getElementById('2103.16365v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.15924">arXiv:2103.15924</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.15924">pdf</a>, <a href="https://arxiv.org/format/2103.15924">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How Far Can We Go in Compute-less Networking: Computation Correctness and <span class="search-hit mathjax">Accuracy</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nour%2C+B">Boubakr Nour</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cherkaoui%2C+S">Soumaya Cherkaoui</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.15924v1-abstract-short" style="display: inline;">
        Emerging <span class="search-hit mathjax">applications</span> such as augmented reality and the tactile Internet are compute-intensive and latency-sensitive, which hampers their running in constrained end devices alone or in the distant cloud. The stringent requirements of such&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.15924v1-abstract-full').style.display = 'inline'; document.getElementById('2103.15924v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.15924v1-abstract-full" style="display: none;">
        Emerging <span class="search-hit mathjax">applications</span> such as augmented reality and the tactile Internet are compute-intensive and latency-sensitive, which hampers their running in constrained end devices alone or in the distant cloud. The stringent requirements of such <span class="search-hit mathjax">application</span> drove to the realization of Edge computing in which computation is offloaded near to users. Moreover, these <span class="search-hit mathjax">applications</span> are often executed with similar input data that yield the same output. Compute-less networking is an extension of edge computing that aims at <span class="search-hit mathjax">reducing</span> computation and abridging communication by adopting in-network computing and computation reuse. Calculation-reuse aims to cache the result of calculations, and use them to <span class="search-hit mathjax">perform</span> similar tasks in the future and, therefore, avoid redundant calculations and <span class="search-hit mathjax">optimize</span> the use of resources. Since the input might not be identical but similar, the reuse of previous computation raises questions on the correctness and <span class="search-hit mathjax">accuracy</span> of the final results. In this paper, we study the correctness of output data in the computation reuse concept and gauge the <span class="search-hit mathjax">effectiveness</span> and <span class="search-hit mathjax">efficiency</span> of compute-less networking.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.15924v1-abstract-full').style.display = 'none'; document.getElementById('2103.15924v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.15820">arXiv:2103.15820</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.15820">pdf</a>, <a href="https://arxiv.org/format/2103.15820">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Discrete Mathematics">cs.DM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ZX-Calculus and Extended Wolfram Model Systems II: Fast Diagrammatic Reasoning with an <span class="search-hit mathjax">Application</span> to Quantum Circuit Simplification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gorard%2C+J">Jonathan Gorard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Namuduri%2C+M">Manojna Namuduri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arsiwalla%2C+X+D">Xerxes D. Arsiwalla</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.15820v1-abstract-short" style="display: inline;">
        This article presents a novel algorithmic methodology for <span class="search-hit mathjax">performing</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.15820v1-abstract-full').style.display = 'inline'; document.getElementById('2103.15820v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.15820v1-abstract-full" style="display: none;">
        This article presents a novel algorithmic methodology for <span class="search-hit mathjax">performing</span> <span class="search-hit mathjax">automated</span> diagrammatic deductions over combinatorial structures, using a combination of modified equational theorem-proving techniques and the extended Wolfram model hypergraph rewriting formalism developed by the authors in previous work. We focus especially upon the <span class="search-hit mathjax">application</span> of this new algorithm to the problem of <span class="search-hit mathjax">automated</span> circuit simplification in quantum information theory, using Wolfram model multiway operator systems combined with the ZX-calculus formalism for enacting fast diagrammatic reasoning over linear transformations between qubits. We show how to construct a generalization of the deductive inference rules for Knuth-Bendix completion in which equation matches are selected on the basis of causal edge density in the associated multiway system, before proceeding to demonstrate how to embed the higher-order logic of the ZX-calculus rules within this first-order equational framework. After showing explicitly how the (hyper)graph rewritings of both Wolfram model systems and the ZX-calculus can be <span class="search-hit mathjax">effectively</span> realized within this formalism, we proceed to exhibit comparisons of time complexity vs. proof complexity for this new algorithmic approach when simplifying randomly-generated Clifford circuits down to pseudo-normal form, as well as when <span class="search-hit mathjax">reducing</span> the number of T-gates in randomly-generated non-Clifford circuits, with circuit sizes ranging up to 3000 gates, illustrating that the method <span class="search-hit mathjax">performs</span> favorably in comparison with existing circuit simplification frameworks, and also exhibiting the approximately quadratic speedup obtained by employing the causal edge density <span class="search-hit mathjax">optimization</span>. Finally, we present a worked example of an <span class="search-hit mathjax">automated</span> proof of correctness for a simple quantum teleportation protocol, in order to demonstrate more clearly the internal operations of the theorem-proving procedure.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.15820v1-abstract-full').style.display = 'none'; document.getElementById('2103.15820v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">104 pages, 57 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68R10
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.14695">arXiv:2103.14695</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.14695">pdf</a>, <a href="https://arxiv.org/format/2103.14695">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MultiScope: <span class="search-hit mathjax">Efficient</span> Video Pre-processing for Exploratory Video Analytics
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bastani%2C+F">Favyen Bastani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Madden%2C+S">Sam Madden</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.14695v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Performing</span> analytics tasks over large-scale video datasets is increasingly common in a wide range of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.14695v1-abstract-full').style.display = 'inline'; document.getElementById('2103.14695v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.14695v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Performing</span> analytics tasks over large-scale video datasets is increasingly common in a wide range of <span class="search-hit mathjax">applications</span>. These tasks generally involve object detection and tracking operations that require applying expensive machine learning models, and several systems have recently been proposed to <span class="search-hit mathjax">optimize</span> the execution of video queries to <span class="search-hit mathjax">reduce</span> their cost. However, prior work generally <span class="search-hit mathjax">optimizes</span> execution speed in only one dimension, focusing on one <span class="search-hit mathjax">optimization</span> technique while ignoring other potential avenues for accelerating execution, thereby delivering an unsatisfactory tradeoff between speed and <span class="search-hit mathjax">accuracy</span>. We propose MultiScope, a general-purpose video pre-processor for object detection and tracking that explores multiple avenues for <span class="search-hit mathjax">optimizing</span> video queries to extract tracks from video with a superior tradeoff between speed and <span class="search-hit mathjax">accuracy</span> over prior work. We compare MultiScope against three recent systems on seven diverse datasets, and find that it provides a 2.9x average speedup over the next best baseline at the same <span class="search-hit mathjax">accuracy</span> level.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.14695v1-abstract-full').style.display = 'none'; document.getElementById('2103.14695v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.14024">arXiv:2103.14024</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.14024">pdf</a>, <a href="https://arxiv.org/format/2103.14024">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PlenOctrees for Real-time Rendering of Neural Radiance Fields
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+A">Alex Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+R">Ruilong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tancik%2C+M">Matthew Tancik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Hao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ng%2C+R">Ren Ng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kanazawa%2C+A">Angjoo Kanazawa</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.14024v1-abstract-short" style="display: inline;">
        We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent <span class="search-hit mathjax">effects</span>. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.14024v1-abstract-full').style.display = 'inline'; document.getElementById('2103.14024v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.14024v1-abstract-full" style="display: none;">
        We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent <span class="search-hit mathjax">effects</span>. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to <span class="search-hit mathjax">perform</span> free-viewpoint rendering of scenes with arbitrary geometry and view-dependent <span class="search-hit mathjax">effects</span>. Real-time <span class="search-hit mathjax">performance</span> is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent <span class="search-hit mathjax">effects</span> such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly <span class="search-hit mathjax">optimized</span> to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree <span class="search-hit mathjax">optimization</span> step can be used to <span class="search-hit mathjax">reduce</span> the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new <span class="search-hit mathjax">applications</span> such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and <span class="search-hit mathjax">code</span>: https://alexyu.net/plenoctrees
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.14024v1-abstract-full').style.display = 'none'; document.getElementById('2103.14024v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.13909">arXiv:2103.13909</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.13909">pdf</a>, <a href="https://arxiv.org/ps/2103.13909">ps</a>, <a href="https://arxiv.org/format/2103.13909">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1098/rsta.2020.0191">10.1098/rsta.2020.0191 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Regularization by Denoising Sub-sampled Newton Method for Spectral CT Multi-Material Decomposition
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Perelli%2C+A">Alessandro Perelli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Andersen%2C+M+S">Martin S. Andersen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.13909v1-abstract-short" style="display: inline;">
        &hellip;technology that enables to estimate the concentration of basis materials within a scanned object by exploiting different photon energy spectra. In this work, we aim at <span class="search-hit mathjax">efficiently</span> solving a model-based maximum-a-posterior problem to reconstruct multi-materials images with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.13909v1-abstract-full').style.display = 'inline'; document.getElementById('2103.13909v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.13909v1-abstract-full" style="display: none;">
        Spectral Computed Tomography (CT) is an emerging technology that enables to estimate the concentration of basis materials within a scanned object by exploiting different photon energy spectra. In this work, we aim at <span class="search-hit mathjax">efficiently</span> solving a model-based maximum-a-posterior problem to reconstruct multi-materials images with <span class="search-hit mathjax">application</span> to spectral CT. In particular, we propose to solve a regularized <span class="search-hit mathjax">optimization</span> problem based on a plug-in image-denoising function using a randomized second order method. By approximating the Newton step using a sketching of the Hessian of the likelihood function, it is possible to <span class="search-hit mathjax">reduce</span> the complexity while retaining the complex prior structure given by the data-driven regularizer. We exploit a non-uniform block sub-sampling of the Hessian with inexact but <span class="search-hit mathjax">efficient</span> Conjugate gradient updates that require only Jacobian-vector products for denoising term. Finally, we show numerical and experimental results for spectral CT materials decomposition.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.13909v1-abstract-full').style.display = 'none'; document.getElementById('2103.13909v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in Philosophical Transactions A, issue &#34;Synergistic tomographic image reconstruction (Part 1)&#34;</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.13842">arXiv:2103.13842</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.13842">pdf</a>, <a href="https://arxiv.org/format/2103.13842">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Morgan%2C+A+S">Andrew S. Morgan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nandha%2C+D">Daljeet Nandha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chalvatzaki%2C+G">Georgia Chalvatzaki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=D%27Eramo%2C+C">Carlo D&#39;Eramo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dollar%2C+A+M">Aaron M. Dollar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peters%2C+J">Jan Peters</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.13842v1-abstract-short" style="display: inline;">
        Substantial advancements to model-based reinforcement learning algorithms have been impeded by the model-bias induced by the collected data, which generally hurts <span class="search-hit mathjax">performance</span>. Meanwhile, their inherent sample&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.13842v1-abstract-full').style.display = 'inline'; document.getElementById('2103.13842v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.13842v1-abstract-full" style="display: none;">
        Substantial advancements to model-based reinforcement learning algorithms have been impeded by the model-bias induced by the collected data, which generally hurts <span class="search-hit mathjax">performance</span>. Meanwhile, their inherent sample <span class="search-hit mathjax">efficiency</span> warrants utility for most robot <span class="search-hit mathjax">applications</span>, limiting potential damage to the robot and its environment during training. Inspired by information theoretic model predictive control and advances in deep reinforcement learning, we introduce Model Predictive Actor-Critic (MoPAC), a hybrid model-based/model-free method that combines model predictive rollouts with policy <span class="search-hit mathjax">optimization</span> as to mitigate model bias. MoPAC leverages <span class="search-hit mathjax">optimal</span> trajectories to guide policy learning, but explores via its model-free method, allowing the algorithm to learn more expressive dynamics models. This combination guarantees <span class="search-hit mathjax">optimal</span> skill learning up to an approximation error and <span class="search-hit mathjax">reduces</span> necessary physical interaction with the environment, making it suitable for real-robot training. We provide extensive results showcasing how our proposed method generally outperforms current state-of-the-art and conclude by evaluating MoPAC for learning on a physical robotic hand <span class="search-hit mathjax">performing</span> valve rotation and finger gaiting--a task that requires grasping, manipulation, and then regrasping of an object.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.13842v1-abstract-full').style.display = 'none'; document.getElementById('2103.13842v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE International Conference on Robotics and Automation (ICRA), Xi&#39;an, China, 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.13147">arXiv:2103.13147</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.13147">pdf</a>, <a href="https://arxiv.org/format/2103.13147">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Agent Off-Policy TD Learning: Finite-Time Analysis with Near-<span class="search-hit mathjax">Optimal</span> Sample Complexity and Communication Complexity
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Ziyi Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yi Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Rongrong Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.13147v1-abstract-short" style="display: inline;">
        &hellip;studied recently. However, such a type of convergence has not been well established for off-policy TD learning in the multi-agent setting, which covers broader <span class="search-hit mathjax">applications</span> and is fundamentally more challenging. This work develops two decentralized TD with correction (TDC) algorithms for multi-agent off-policy TD learning under Markovian sampling. In particu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.13147v1-abstract-full').style.display = 'inline'; document.getElementById('2103.13147v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.13147v1-abstract-full" style="display: none;">
        The finite-time convergence of off-policy TD learning has been comprehensively studied recently. However, such a type of convergence has not been well established for off-policy TD learning in the multi-agent setting, which covers broader <span class="search-hit mathjax">applications</span> and is fundamentally more challenging. This work develops two decentralized TD with correction (TDC) algorithms for multi-agent off-policy TD learning under Markovian sampling. In particular, our algorithms preserve full privacy of the actions, policies and rewards of the agents, and adopt mini-batch sampling to <span class="search-hit mathjax">reduce</span> the sampling variance and communication frequency. Under Markovian sampling and linear function approximation, we proved that the finite-time sample complexity of both algorithms for achieving an $ε$-accurate solution is in the order of $\mathcal{O}(ε^{-1}\ln ε^{-1})$, matching the near-<span class="search-hit mathjax">optimal</span> sample complexity of centralized TD(0) and TDC. Importantly, the communication complexity of our algorithms is in the order of $\mathcal{O}(\ln ε^{-1})$, which is significantly lower than the communication complexity $\mathcal{O}(ε^{-1}\ln ε^{-1})$ of the existing decentralized TD(0). Experiments corroborate our theoretical findings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.13147v1-abstract-full').style.display = 'none'; document.getElementById('2103.13147v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">34 pages, 3 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.12874">arXiv:2103.12874</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.12874">pdf</a>, <a href="https://arxiv.org/format/2103.12874">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Using Meta-learning to Recommend Process Discovery Methods
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Barbon%2C+S">Sylvio Barbon Jr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ceravolo%2C+P">Paolo Ceravolo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Damiani%2C+E">Ernesto Damiani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tavares%2C+G+M">Gabriel Marques Tavares</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.12874v1-abstract-short" style="display: inline;">
        &hellip;process models to enhance management capabilities. However, selecting the suitable method for a specific event log highly relies on human expertise, hindering its broad <span class="search-hit mathjax">application</span>. Solutions based on Meta-learning (MtL) have been promising for creating systems with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.12874v1-abstract-full').style.display = 'inline'; document.getElementById('2103.12874v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.12874v1-abstract-full" style="display: none;">
        Process discovery methods have obtained remarkable achievements in Process Mining, delivering comprehensible process models to enhance management capabilities. However, selecting the suitable method for a specific event log highly relies on human expertise, hindering its broad <span class="search-hit mathjax">application</span>. Solutions based on Meta-learning (MtL) have been promising for creating systems with <span class="search-hit mathjax">reduced</span> human assistance. This paper presents a MtL solution for recommending process discovery methods that maximize model quality according to complementary dimensions. Thanks to our MtL pipeline, it was possible to recommend a discovery method with 92% of <span class="search-hit mathjax">accuracy</span> using light-weight features that describe the event log. Our experimental analysis also provided significant insights on the importance of log features in generating recommendations, paving the way to a deeper understanding of the discovery algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.12874v1-abstract-full').style.display = 'none'; document.getElementById('2103.12874v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.12513">arXiv:2103.12513</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.12513">pdf</a>, <a href="https://arxiv.org/format/2103.12513">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On gray-box modeling for virtual flow metering
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hotvedt%2C+M">Mathilde Hotvedt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Grimstad%2C+B">Bjarne Grimstad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ljungquist%2C+D">Dag Ljungquist</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Imsland%2C+L">Lars Imsland</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.12513v2-abstract-short" style="display: inline;">
        A virtual flow meter (VFM) enables continuous prediction of flow rates in petroleum production systems. The predicted flow rates may aid the daily control and <span class="search-hit mathjax">optimization</span> of a petroleum asset. Gray-box modeling is an approach that combines mechanistic and data-driven modeling. The objective is to create a computationally feasible VFM for use in real-time&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.12513v2-abstract-full').style.display = 'inline'; document.getElementById('2103.12513v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.12513v2-abstract-full" style="display: none;">
        A virtual flow meter (VFM) enables continuous prediction of flow rates in petroleum production systems. The predicted flow rates may aid the daily control and <span class="search-hit mathjax">optimization</span> of a petroleum asset. Gray-box modeling is an approach that combines mechanistic and data-driven modeling. The objective is to create a computationally feasible VFM for use in real-time <span class="search-hit mathjax">applications</span>, with high prediction <span class="search-hit mathjax">accuracy</span> and scientifically consistent behavior. This article investigates five different gray-box model types in an industrial case study using real, historical production data from 10 petroleum wells, spanning at most four years of production. The results are diverse with an oil flow rate prediction error in the range of 1.8%-40.6%. Further, the study casts light upon the nontrivial task of balancing learning from both physics and data. Consequently, providing general recommendations towards the suitability of different hybrid models is challenging. Nevertheless, the results are promising and indicate that gray-box VFMs may <span class="search-hit mathjax">reduce</span> the prediction error of a mechanistic VFM while remaining scientifically consistent. The findings motivate further experimentation with gray-box VFM models and suggest several future research directions to <span class="search-hit mathjax">improve</span> upon the <span class="search-hit mathjax">performance</span> and scientific consistency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.12513v2-abstract-full').style.display = 'none'; document.getElementById('2103.12513v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">38 pages, 28 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.12293">arXiv:2103.12293</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.12293">pdf</a>, <a href="https://arxiv.org/format/2103.12293">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stochastic Reweighted Gradient Descent
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hanchi%2C+A+E">Ayoub El Hanchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stephens%2C+D+A">David A. Stephens</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.12293v1-abstract-short" style="display: inline;">
        Despite the strong theoretical guarantees that variance-<span class="search-hit mathjax">reduced</span> finite-sum&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.12293v1-abstract-full').style.display = 'inline'; document.getElementById('2103.12293v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.12293v1-abstract-full" style="display: none;">
        Despite the strong theoretical guarantees that variance-<span class="search-hit mathjax">reduced</span> finite-sum <span class="search-hit mathjax">optimization</span> algorithms enjoy, their <span class="search-hit mathjax">applicability</span> remains limited to cases where the memory overhead they introduce (SAG/SAGA), or the periodic full gradient computation they require (SVRG/SARAH) are manageable. A promising approach to achieving variance reduction while avoiding these drawbacks is the use of importance sampling instead of control variates. While many such methods have been proposed in the literature, directly proving that they <span class="search-hit mathjax">improve</span> the convergence of the resulting <span class="search-hit mathjax">optimization</span> algorithm has remained elusive. In this work, we propose an importance-sampling-based algorithm we call SRG (stochastic reweighted gradient). We analyze the convergence of SRG in the strongly-convex case and show that, while it does not recover the linear rate of control variates methods, it provably outperforms SGD. We pay particular attention to the time and memory overhead of our proposed method, and design a specialized red-black tree allowing its <span class="search-hit mathjax">efficient</span> implementation. Finally, we present empirical results to support our findings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.12293v1-abstract-full').style.display = 'none'; document.getElementById('2103.12293v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.11517">arXiv:2103.11517</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.11517">pdf</a>, <a href="https://arxiv.org/format/2103.11517">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dual Monte Carlo Tree Search
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kadam%2C+P">Prashank Kadam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+R">Ruiyang Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lieberherr%2C+K">Karl Lieberherr</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.11517v1-abstract-short" style="display: inline;">
        &hellip;and Monte Carlo Tree Search (MCTS), has successfully trained reinforcement learning agents in a tabula-rasa way. The neural MCTS algorithm has been successful in finding near-<span class="search-hit mathjax">optimal</span> strategies for games through self-play. However, the AlphaZero algorithm has a significant drawback; it takes a long time to converge and requires high computational power due t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.11517v1-abstract-full').style.display = 'inline'; document.getElementById('2103.11517v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.11517v1-abstract-full" style="display: none;">
        AlphaZero, using a combination of Deep Neural Networks and Monte Carlo Tree Search (MCTS), has successfully trained reinforcement learning agents in a tabula-rasa way. The neural MCTS algorithm has been successful in finding near-<span class="search-hit mathjax">optimal</span> strategies for games through self-play. However, the AlphaZero algorithm has a significant drawback; it takes a long time to converge and requires high computational power due to complex neural networks for solving games like Chess, Go, Shogi, etc. Owing to this, it is very difficult to pursue neural MCTS research without cutting-edge hardware, which is a roadblock for many aspiring neural MCTS researchers. In this paper, we propose a new neural MCTS algorithm, called Dual MCTS, which helps overcome these drawbacks. Dual MCTS uses two different search trees, a single deep neural network, and a new update technique for the search trees using a combination of the PUCB, a sliding-window, and the epsilon-greedy algorithm. This technique is <span class="search-hit mathjax">applicable</span> to any MCTS based algorithm to <span class="search-hit mathjax">reduce</span> the number of updates to the tree. We show that Dual MCTS <span class="search-hit mathjax">performs</span> better than one of the most widely used neural MCTS algorithms, AlphaZero, for various symmetric and asymmetric games.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.11517v1-abstract-full').style.display = 'none'; document.getElementById('2103.11517v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.11154">arXiv:2103.11154</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.11154">pdf</a>, <a href="https://arxiv.org/format/2103.11154">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Train Deep Neural Networks in 40-D Subspaces
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+T">Tao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+L">Lei Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tao%2C+Q">Qinghua Tao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yipeng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+X">Xiaolin Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.11154v1-abstract-short" style="display: inline;">
        &hellip;space. By investigating such low-dimensional properties of the training trajectory, we propose a Dynamic Linear Dimensionality Reduction (DLDR), which dramatically <span class="search-hit mathjax">reduces</span> the parameter space to a variable subspace of significantly lower dimension. Since there are only a few variables to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.11154v1-abstract-full').style.display = 'inline'; document.getElementById('2103.11154v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.11154v1-abstract-full" style="display: none;">
        Although there are massive parameters in deep neural networks, the training can actually proceed in a rather low-dimensional space. By investigating such low-dimensional properties of the training trajectory, we propose a Dynamic Linear Dimensionality Reduction (DLDR), which dramatically <span class="search-hit mathjax">reduces</span> the parameter space to a variable subspace of significantly lower dimension. Since there are only a few variables to <span class="search-hit mathjax">optimize</span>, second-order methods become <span class="search-hit mathjax">applicable</span>. Following this idea, we develop a quasi-Newton-based algorithm to train these variables obtained by DLDR, rather than the original parameters of neural networks. The experimental results strongly support the dimensionality reduction <span class="search-hit mathjax">performance</span>: for many standard neural networks, <span class="search-hit mathjax">optimizing</span> over only 40 variables, one can achieve comparable <span class="search-hit mathjax">performance</span> against the regular training over thousands or even millions of parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.11154v1-abstract-full').style.display = 'none'; document.getElementById('2103.11154v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.11137">arXiv:2103.11137</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.11137">pdf</a>, <a href="https://arxiv.org/format/2103.11137">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PathEnum: Towards Real-Time Hop-Constrained s-t Path Enumeration
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+S">Shixuan Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yuhang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+B">Bingsheng He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hooi%2C+B">Bryan Hooi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.11137v2-abstract-short" style="display: inline;">
        &hellip;. The state-of-the-art algorithms suffer from severe <span class="search-hit mathjax">performance</span> issues caused by the costly pruning operations during enumeration for the workloads with the large search space. Consequently, these algorithms hardly meet the real-time constraints of many online&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.11137v2-abstract-full').style.display = 'inline'; document.getElementById('2103.11137v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.11137v2-abstract-full" style="display: none;">
        We study the hop-constrained s-t path enumeration (HcPE) problem, which takes a graph $G$, two distinct vertices $s,t$ and a hop constraint $k$ as input, and outputs all paths from $s$ to $t$ whose length is at most $k$. The state-of-the-art algorithms suffer from severe <span class="search-hit mathjax">performance</span> issues caused by the costly pruning operations during enumeration for the workloads with the large search space. Consequently, these algorithms hardly meet the real-time constraints of many online <span class="search-hit mathjax">applications</span>. In this paper, we propose PathEnum, an <span class="search-hit mathjax">efficient</span> index-based algorithm towards real-time HcPE. For an input query, PathEnum first builds a light-weight index aiming to <span class="search-hit mathjax">reduce</span> the number of edges involved in the enumeration, and develops <span class="search-hit mathjax">efficient</span> index-based approaches for enumeration, one based on depth-first search and the other based on joins. We further develop a query <span class="search-hit mathjax">optimizer</span> based on a join-based cost model to <span class="search-hit mathjax">optimize</span> the search order. We conduct experiments with 15 real-world graphs. Our experiment results show that PathEnum outperforms the state-of-the-art approaches by orders of magnitude in terms of the query time, throughput and response time.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.11137v2-abstract-full').style.display = 'none'; document.getElementById('2103.11137v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.10872">arXiv:2103.10872</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.10872">pdf</a>, <a href="https://arxiv.org/format/2103.10872">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mathematical Finance">q-fin.MF</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Risk Management">q-fin.RM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Optimal</span> Clearing Payments in a Financial Contagion Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Calafiore%2C+G">Giuseppe Calafiore</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fracastoro%2C+G">Giulia Fracastoro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Proskurnikov%2C+A+V">Anton V. Proskurnikov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.10872v1-abstract-short" style="display: inline;">
        &hellip;of clearing vector (the vector of the entities&#39; total payments). In this paper, we propose a necessary and sufficient condition for the uniqueness of clearing vector <span class="search-hit mathjax">applicable</span> to an arbitrary topology of the financial network. Further, we show that the overall system loss can be&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.10872v1-abstract-full').style.display = 'inline'; document.getElementById('2103.10872v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.10872v1-abstract-full" style="display: none;">
        Modern financial networks are characterized by complex structures of mutual obligations. Such interconnections may propagate and amplificate individual defaults, leading in some cases to financial disaster. For this reason, mathematical models for the study and control of systemic risk (the risk of severe instabilities on the system as a whole, due to default of single entities) have attracted considerable research attention in recent years. One important line of research is concerned with mechanisms of clearing, that is, the mechanism by which mutual debts are repaid, in the regular regime, or in a default regime. One of the first models of a clearing mechanism was proposed by Eisenberg and Noe and is based on the three rules: limited liability, the priority of debt claims over the shareholders&#39; interests, and the equal priority of debts (pro-rata rule). These three principles naturally lead to the concept of clearing vector (the vector of the entities&#39; total payments). In this paper, we propose a necessary and sufficient condition for the uniqueness of clearing vector <span class="search-hit mathjax">applicable</span> to an arbitrary topology of the financial network. Further, we show that the overall system loss can be <span class="search-hit mathjax">reduced</span> if one relaxes the pro-rata rule and replaces the clearing vector by a matrix of clearing payments. This approach shifts the focus from the individual interest to the system, or social, interest, in order to control and contain the adverse <span class="search-hit mathjax">effects</span> of cascaded failures.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.10872v1-abstract-full').style.display = 'none'; document.getElementById('2103.10872v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.10779">arXiv:2103.10779</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.10779">pdf</a>, <a href="https://arxiv.org/format/2103.10779">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Page Table Management for Heterogeneous Memory Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+S">Sandeep Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prasad%2C+A">Aravinda Prasad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sarangi%2C+S+R">Smruti R. Sarangi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Subramoney%2C+S">Sreenivas Subramoney</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.10779v1-abstract-short" style="display: inline;">
        &hellip;with a combination of low latency DRAMs and large capacity but high latency non-volatile main memories (NVMMs) such as Intel&#39;s Optane DC PMM. Prior works have focused on <span class="search-hit mathjax">efficient</span> placement and migration of data on a tiered memory system, but have not studied the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.10779v1-abstract-full').style.display = 'inline'; document.getElementById('2103.10779v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.10779v1-abstract-full" style="display: none;">
        Modern enterprise servers are increasingly embracing tiered memory systems with a combination of low latency DRAMs and large capacity but high latency non-volatile main memories (NVMMs) such as Intel&#39;s Optane DC PMM. Prior works have focused on <span class="search-hit mathjax">efficient</span> placement and migration of data on a tiered memory system, but have not studied the <span class="search-hit mathjax">optimal</span> placement of page tables.
  Explicit and <span class="search-hit mathjax">efficient</span> placement of page tables is crucial for large memory footprint <span class="search-hit mathjax">applications</span> with high TLB miss rates because they incur dramatically higher page walk latency when page table pages are placed in NVMM. We show that (i) page table pages can end up on NVMM even when enough DRAM memory is available and (ii) page table pages that spill over to NVMM due to DRAM memory pressure are not migrated back later when memory is available in DRAM.
  We study the <span class="search-hit mathjax">performance</span> impact of page table placement in a tiered memory system and propose an <span class="search-hit mathjax">efficient</span> and transparent page table management technique that (i) applies different placement policies for data and page table pages, (ii) introduces a differentiating policy for page table pages by placing a small but critical part of the page table in DRAM, and (iii) dynamically and judiciously manages the rest of the page table by transparently migrating the page table pages between DRAM and NVMM. Our implementation on a real system equipped with Intel&#39;s Optane NVMM running Linux <span class="search-hit mathjax">reduces</span> the page table walk cycles by 12% and total cycles by 20% on an average. This <span class="search-hit mathjax">improves</span> the runtime by 20% on an average for a set of synthetic and real-world large memory footprint <span class="search-hit mathjax">applications</span> when compared with various default Linux kernel techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.10779v1-abstract-full').style.display = 'none'; document.getElementById('2103.10779v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.10294">arXiv:2103.10294</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.10294">pdf</a>, <a href="https://arxiv.org/format/2103.10294">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Discrete Mathematics">cs.DM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning to Schedule Heuristics in Branch-and-Bound
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chmiela%2C+A">Antonia Chmiela</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khalil%2C+E+B">Elias B. Khalil</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gleixner%2C+A">Ambros Gleixner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lodi%2C+A">Andrea Lodi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pokutta%2C+S">Sebastian Pokutta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.10294v1-abstract-short" style="display: inline;">
        Primal heuristics play a crucial role in exact solvers for Mixed Integer <span class="search-hit mathjax">Programming</span> (MIP). While solvers are guaranteed to find&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.10294v1-abstract-full').style.display = 'inline'; document.getElementById('2103.10294v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.10294v1-abstract-full" style="display: none;">
        Primal heuristics play a crucial role in exact solvers for Mixed Integer <span class="search-hit mathjax">Programming</span> (MIP). While solvers are guaranteed to find <span class="search-hit mathjax">optimal</span> solutions given sufficient time, real-world <span class="search-hit mathjax">applications</span> typically require finding good solutions early on in the search to enable fast decision-making. While much of MIP research focuses on designing <span class="search-hit mathjax">effective</span> heuristics, the question of how to manage multiple MIP heuristics in a solver has not received equal attention. Generally, solvers follow hard-<span class="search-hit mathjax">coded</span> rules derived from empirical testing on broad sets of instances. Since the <span class="search-hit mathjax">performance</span> of heuristics is instance-dependent, using these general rules for a particular problem might not yield the best <span class="search-hit mathjax">performance</span>. In this work, we propose the first data-driven framework for scheduling heuristics in an exact MIP solver. By learning from data describing the <span class="search-hit mathjax">performance</span> of primal heuristics, we obtain a problem-specific schedule of heuristics that collectively find many solutions at minimal cost. We provide a formal description of the problem and propose an <span class="search-hit mathjax">efficient</span> algorithm for computing such a schedule. Compared to the default settings of a state-of-the-art academic MIP solver, we are able to <span class="search-hit mathjax">reduce</span> the average primal integral by up to 49% on a class of challenging instances.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.10294v1-abstract-full').style.display = 'none'; document.getElementById('2103.10294v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.09782">arXiv:2103.09782</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.09782">pdf</a>, <a href="https://arxiv.org/format/2103.09782">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards <span class="search-hit mathjax">Automated</span> Metamorphic Test Identification for Ocean System Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hiremath%2C+D+J">Dilip Jagadeeshwarswamy Hiremath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Claus%2C+M">Martin Claus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hasselbring%2C+W">Wilhelm Hasselbring</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rath%2C+W">Willi Rath</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.09782v1-abstract-short" style="display: inline;">
        Metamorphic testing seeks to verify <span class="search-hit mathjax">software</span> in the absence of test oracles. Our&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09782v1-abstract-full').style.display = 'inline'; document.getElementById('2103.09782v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.09782v1-abstract-full" style="display: none;">
        Metamorphic testing seeks to verify <span class="search-hit mathjax">software</span> in the absence of test oracles. Our <span class="search-hit mathjax">application</span> domain is ocean system modeling, where test oracles rarely exist, but where symmetries of the simulated physical systems are known. The input data set is large owing to the requirements of the <span class="search-hit mathjax">application</span> domain. This paper presents work in progress for the <span class="search-hit mathjax">automated</span> generation of metamorphic test scenarios using machine learning. We extended our previously proposed method [1] to identify metamorphic relations with <span class="search-hit mathjax">reduced</span> computational complexity. Initially, we represent metamorphic relations as identity maps. We construct a cost function that minimizes for identifying a metamorphic relation orthogonal to previously found metamorphic relations and penalize for the identity map. A machine learning algorithm is used to identify all possible metamorphic relations minimizing the defined cost function. We propose applying dimensionality reduction techniques to identify attributes in the input which have high variance among the identified metamorphic relations. We apply mutation on these selected attributes to identify distinct metamorphic relations with <span class="search-hit mathjax">reduced</span> computational complexity. For experimental evaluation, we subject the two implementations of an ocean-modeling <span class="search-hit mathjax">application</span> to the proposed method to present the use of metamorphic relations to test the two implementations of this <span class="search-hit mathjax">application</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09782v1-abstract-full').style.display = 'none'; document.getElementById('2103.09782v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 Pages, 1 Figure</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          J.2
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.09693">arXiv:2103.09693</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.09693">pdf</a>, <a href="https://arxiv.org/format/2103.09693">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Robust Tube-Based Smooth-MPC for Robot Manipulator Planning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+Y">Yu Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jing%2C+M">Mingxuan Jing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+T">Tianying Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+F">Fuchun Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Huaping Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.09693v1-abstract-short" style="display: inline;">
        Model Predictive Control (MPC) has shown the great <span class="search-hit mathjax">performance</span> of target&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09693v1-abstract-full').style.display = 'inline'; document.getElementById('2103.09693v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.09693v1-abstract-full" style="display: none;">
        Model Predictive Control (MPC) has shown the great <span class="search-hit mathjax">performance</span> of target <span class="search-hit mathjax">optimization</span> and constraint satisfaction. However, the heavy computation of the <span class="search-hit mathjax">Optimal</span> Control Problem (OCP) at each triggering instant brings the serious delay from state sampling to the control signals, which limits the <span class="search-hit mathjax">applications</span> of MPC in resource-limited robot manipulator systems over complicated tasks. In this paper, we propose a novel robust tube-based smooth-MPC strategy for nonlinear robot manipulator planning systems with disturbances and constraints. Based on piecewise linearization and state prediction, our control strategy <span class="search-hit mathjax">improves</span> the smoothness and <span class="search-hit mathjax">optimizes</span> the delay of the control process. By deducing the deviation of the real system states and the nominal system states, we can predict the next real state set at the current instant. And by using this state set as the initial condition, we can solve the next OCP ahead and store the <span class="search-hit mathjax">optimal</span> controls based on the nominal system states, which eliminates the delay. Furthermore, we linearize the nonlinear system with a given upper bound of error, <span class="search-hit mathjax">reducing</span> the complexity of the OCP and <span class="search-hit mathjax">improving</span> the response speed. Based on the theoretical framework of tube MPC, we prove that the control strategy is recursively feasible and closed-loop stable with the constraints and disturbances. Numerical simulations have verified the efficacy of the designed approach compared with the conventional MPC.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09693v1-abstract-full').style.display = 'none'; document.getElementById('2103.09693v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.09595">arXiv:2103.09595</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.09595">pdf</a>, <a href="https://arxiv.org/format/2103.09595">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Assessing Smart Contracts Security Technical Debts
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ahmadjee%2C+S">Sabreen Ahmadjee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mera-G%C3%B3mez%2C+C">Carlos Mera-Gómez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bahsoon%2C+R">Rami Bahsoon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.09595v1-abstract-short" style="display: inline;">
        &hellip;of the identified vulnerabilities leveraging the technical debt metaphor, its principal and interest. We use examples of vulnerable contracts to demonstrate the <span class="search-hit mathjax">applicability</span> of our approach. The results show that our assessment approach increases the visibility of security design issues. It also allows developers to concentrate on resolving smart contract v&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09595v1-abstract-full').style.display = 'inline'; document.getElementById('2103.09595v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.09595v1-abstract-full" style="display: none;">
        Smart contracts are self-enforcing agreements that are employed to exchange assets without the approval of trusted third parties. This feature has encouraged various sectors to make use of smart contracts when transacting. Experience shows that many deployed contracts are vulnerable to exploitation due to their poor design, which allows attackers to steal valuable assets from the involved parties. Therefore, an assessment approach that allows developers to recognise the consequences of deploying vulnerable contracts is needed. In this paper, we propose a debt-aware approach for assessing security design vulnerabilities in smart contracts. Our assessment approach involves two main steps: (i) identification of design vulnerabilities using security analysis techniques and (ii) an estimation of the ramifications of the identified vulnerabilities leveraging the technical debt metaphor, its principal and interest. We use examples of vulnerable contracts to demonstrate the <span class="search-hit mathjax">applicability</span> of our approach. The results show that our assessment approach increases the visibility of security design issues. It also allows developers to concentrate on resolving smart contract vulnerabilities through technical debt impact analysis and prioritisation. Developers can use our approach to inform the design of more secure contracts and for <span class="search-hit mathjax">reducing</span> unintentional debts caused by a lack of awareness of security issues.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09595v1-abstract-full').style.display = 'none'; document.getElementById('2103.09595v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.09019">arXiv:2103.09019</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.09019">pdf</a>, <a href="https://arxiv.org/format/2103.09019">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.jpdc.2021.02.010">10.1016/j.jpdc.2021.02.010 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intelligent colocation of HPC workloads
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zacarias%2C+F+V">Felippe V. Zacarias</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petrucci%2C+V">Vinicius Petrucci</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nishtala%2C+R">Rajiv Nishtala</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carpenter%2C+P">Paul Carpenter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moss%C3%A9%2C+D">Daniel Mossé</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.09019v1-abstract-short" style="display: inline;">
        Many HPC <span class="search-hit mathjax">applications</span> suffer from a bottleneck in the shared caches, instruction execution units, I/O or memory bandwidth, even though the remaining resources may be underutilized. It is hard for developers and runtime systems to ensure that all critical resources are fully exploited by a single&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09019v1-abstract-full').style.display = 'inline'; document.getElementById('2103.09019v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.09019v1-abstract-full" style="display: none;">
        Many HPC <span class="search-hit mathjax">applications</span> suffer from a bottleneck in the shared caches, instruction execution units, I/O or memory bandwidth, even though the remaining resources may be underutilized. It is hard for developers and runtime systems to ensure that all critical resources are fully exploited by a single <span class="search-hit mathjax">application</span>, so an attractive technique for increasing HPC system utilization is to colocate multiple <span class="search-hit mathjax">applications</span> on the same server. When <span class="search-hit mathjax">applications</span> share critical resources, however, contention on shared resources may lead to <span class="search-hit mathjax">reduced</span> <span class="search-hit mathjax">application</span> <span class="search-hit mathjax">performance</span>.
  In this paper, we show that server <span class="search-hit mathjax">efficiency</span> can be <span class="search-hit mathjax">improved</span> by first modeling the expected <span class="search-hit mathjax">performance</span> degradation of colocated <span class="search-hit mathjax">applications</span> based on measured hardware <span class="search-hit mathjax">performance</span> counters, and then exploiting the model to determine an <span class="search-hit mathjax">optimized</span> mix of colocated <span class="search-hit mathjax">applications</span>. This paper presents a new intelligent resource manager and makes the following contributions: (1) a new machine learning model to predict the <span class="search-hit mathjax">performance</span> degradation of colocated <span class="search-hit mathjax">applications</span> based on hardware counters and (2) an intelligent scheduling scheme deployed on an existing resource manager to enable <span class="search-hit mathjax">application</span> co-scheduling with minimum <span class="search-hit mathjax">performance</span> degradation. Our results show that our approach achieves <span class="search-hit mathjax">performance</span> <span class="search-hit mathjax">improvements</span> of 7% (avg) and 12% (max) compared to the standard policy commonly used by existing job managers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09019v1-abstract-full').style.display = 'none'; document.getElementById('2103.09019v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to Journal of Parallel and Distributed Computing</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Volume 151, May 2021, Pages 125-137
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.07286">arXiv:2103.07286</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.07286">pdf</a>, <a href="https://arxiv.org/format/2103.07286">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Integration of Convolutional Neural Networks in Mobile <span class="search-hit mathjax">Applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Castanyer%2C+R+C">Roger Creus Castanyer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mart%C3%ADnez-Fern%C3%A1ndez%2C+S">Silverio Martínez-Fernández</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Franch%2C+X">Xavier Franch</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.07286v1-abstract-short" style="display: inline;">
        When building Deep Learning (DL) models, data scientists and <span class="search-hit mathjax">software</span> engineers manage the trade-off between their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.07286v1-abstract-full').style.display = 'inline'; document.getElementById('2103.07286v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.07286v1-abstract-full" style="display: none;">
        When building Deep Learning (DL) models, data scientists and <span class="search-hit mathjax">software</span> engineers manage the trade-off between their <span class="search-hit mathjax">accuracy</span>, or any other suitable success criteria, and their complexity. In an environment with high computational power, a common practice is making the models go deeper by designing more sophisticated architectures. However, in the context of mobile devices, which possess less computational power, keeping complexity under control is a must. In this paper, we study the <span class="search-hit mathjax">performance</span> of a system that integrates a DL model as a trade-off between the <span class="search-hit mathjax">accuracy</span> and the complexity. At the same time, we relate the complexity to the <span class="search-hit mathjax">efficiency</span> of the system. With this, we present a practical study that aims to explore the challenges met when <span class="search-hit mathjax">optimizing</span> the <span class="search-hit mathjax">performance</span> of DL models becomes a requirement. Concretely, we aim to identify: (i) the most concerning challenges when deploying DL-based <span class="search-hit mathjax">software</span> in mobile <span class="search-hit mathjax">applications</span>; and (ii) the path for <span class="search-hit mathjax">optimizing</span> the <span class="search-hit mathjax">performance</span> trade-off. We obtain results that verify many of the identified challenges in the related work such as the availability of frameworks and the <span class="search-hit mathjax">software</span>-data dependency. We provide a documentation of our experience when facing the identified challenges together with the discussion of possible solutions to them. Additionally, we implement a solution to the sustainability of the DL models when deployed in order to <span class="search-hit mathjax">reduce</span> the severity of other identified challenges. Moreover, we relate the <span class="search-hit mathjax">performance</span> trade-off to a new defined challenge featuring the impact of the complexity in the obtained <span class="search-hit mathjax">accuracy</span>. Finally, we discuss and motivate future work that aims to provide solutions to the more open challenges found.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.07286v1-abstract-full').style.display = 'none'; document.getElementById('2103.07286v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Pre-print. Accepted and to be published in WAIN@ICSE 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.06757">arXiv:2103.06757</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.06757">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Auto-COP: Adaptation Generation in Context-Oriented <span class="search-hit mathjax">Programming</span> using Reinforcement Learning Options
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cardozo%2C+N">Nicolás Cardozo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dusparic%2C+I">Ivana Dusparic</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.06757v1-abstract-short" style="display: inline;">
        Self-adaptive <span class="search-hit mathjax">software</span> systems continuously adapt in response to internal and external changes in their execution environment, captured as contexts. The COP paradigm posits a technique for the development of self-adaptive systems, capturing their main characteristics with specialized&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.06757v1-abstract-full').style.display = 'inline'; document.getElementById('2103.06757v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.06757v1-abstract-full" style="display: none;">
        Self-adaptive <span class="search-hit mathjax">software</span> systems continuously adapt in response to internal and external changes in their execution environment, captured as contexts. The COP paradigm posits a technique for the development of self-adaptive systems, capturing their main characteristics with specialized <span class="search-hit mathjax">programming</span> language constructs. COP adaptations are specified as independent modules composed in and out of the base system as contexts are activated and deactivated in response to sensed circumstances from the surrounding environment. However, the definition of adaptations, their contexts and associated specialized behavior, need to be specified at design time. In complex CPS this is intractable due to new unpredicted operating conditions. We propose Auto-COP, a new technique to enable generation of adaptations at run time. Auto-COP uses RL options to build action sequences, based on the previous instances of the system execution. Options are explored in interaction with the environment, and the most suitable options for each context are used to generate adaptations exploiting COP. To validate Auto-COP, we present two case studies exhibiting different system characteristics and <span class="search-hit mathjax">application</span> domains: a driving assistant and a robot delivery system. We present examples of Auto-COP <span class="search-hit mathjax">code</span> generated at run time, to illustrate the types of circumstances (contexts) requiring adaptation, and the corresponding generated adaptations for each context. We confirm that the generated adaptations exhibit correct system behavior measured by domain-specific <span class="search-hit mathjax">performance</span> metrics, while <span class="search-hit mathjax">reducing</span> the number of required execution/actuation steps by a factor of two showing that the adaptations are regularly selected by the running system as adaptive behavior is more appropriate than the execution of primitive actions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.06757v1-abstract-full').style.display = 'none'; document.getElementById('2103.06757v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to The Art, Science, and Engineering of Programming Journal. 22 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.05363">arXiv:2103.05363</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.05363">pdf</a>, <a href="https://arxiv.org/format/2103.05363">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MWQ: Multiscale Wavelet Quantized Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Q">Qigong Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Y">Yan Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiao%2C+L">Licheng Jiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiufang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shang%2C+F">Fanhua Shang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+F">Fang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.05363v1-abstract-short" style="display: inline;">
        Model quantization can <span class="search-hit mathjax">reduce</span> the model size and computational latency, it has become an essential technique for the deployment of deep neural networks on resourceconstrained hardware (e.g., mobile phones and embedded devices). The existing quantization methods mainly consider the numerical elements of the weights and activation values, ignoring the relation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.05363v1-abstract-full').style.display = 'inline'; document.getElementById('2103.05363v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.05363v1-abstract-full" style="display: none;">
        Model quantization can <span class="search-hit mathjax">reduce</span> the model size and computational latency, it has become an essential technique for the deployment of deep neural networks on resourceconstrained hardware (e.g., mobile phones and embedded devices). The existing quantization methods mainly consider the numerical elements of the weights and activation values, ignoring the relationship between elements. The decline of representation ability and information loss usually lead to the <span class="search-hit mathjax">performance</span> degradation. Inspired by the characteristics of images in the frequency domain, we propose a novel multiscale wavelet quantization (MWQ) method. This method decomposes original data into multiscale frequency components by wavelet transform, and then quantizes the components of different scales, respectively. It exploits the multiscale frequency and spatial information to alleviate the information loss caused by quantization in the spatial domain. Because of the flexibility of MWQ, we demonstrate three <span class="search-hit mathjax">applications</span> (e.g., model compression, quantized network <span class="search-hit mathjax">optimization</span>, and information enhancement) on the ImageNet and COCO datasets. Experimental results show that our method has stronger representation ability and can play an <span class="search-hit mathjax">effective</span> role in quantized neural networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.05363v1-abstract-full').style.display = 'none'; document.getElementById('2103.05363v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.05244">arXiv:2103.05244</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.05244">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Symbolic Computation">cs.SC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ModelingToolkit: A Composable Graph Transformation System For Equation-Based Modeling
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Y">Yingbo Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gowda%2C+S">Shashi Gowda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Anantharaman%2C+R">Ranjan Anantharaman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Laughman%2C+C">Chris Laughman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+V">Viral Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rackauckas%2C+C">Chris Rackauckas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.05244v2-abstract-short" style="display: inline;">
        Getting good <span class="search-hit mathjax">performance</span> out of numerical equation solvers requires that the user has provided stable and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.05244v2-abstract-full').style.display = 'inline'; document.getElementById('2103.05244v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.05244v2-abstract-full" style="display: none;">
        Getting good <span class="search-hit mathjax">performance</span> out of numerical equation solvers requires that the user has provided stable and <span class="search-hit mathjax">efficient</span> functions representing their model. However, users should not be trusted to write good <span class="search-hit mathjax">code</span>. In this manuscript we describe ModelingToolkit (MTK), a symbolic equation-based modeling system which allows for composable transformations to generate stable, <span class="search-hit mathjax">efficient</span>, and parallelized model implementations. MTK blurs the lines of traditional symbolic computing by acting directly on a user&#39;s numerical <span class="search-hit mathjax">code</span>. We show the ability to apply graph algorithms for <span class="search-hit mathjax">automatically</span> parallelizing and <span class="search-hit mathjax">performing</span> index reduction on <span class="search-hit mathjax">code</span> written for differential-algebraic equation (DAE) solvers, &#34;fixing&#34; the <span class="search-hit mathjax">performance</span> and stability of the model without requiring any changes to on the user&#39;s part. We demonstrate how composable model transformations can be combined with <span class="search-hit mathjax">automated</span> data-driven surrogate generation techniques, allowing machine learning methods to generate accelerated approximate models within an acausal modeling framework. These <span class="search-hit mathjax">reduced</span> models are shown to outperform the Dymola Modelica compiler on an HVAC model by 590x at 3\% error. Together, this demonstrates MTK as a system for bringing the latest research in graph transformations directly to modeling <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.05244v2-abstract-full').style.display = 'none'; document.getElementById('2103.05244v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 3 figures, 1 table</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.04784">arXiv:2103.04784</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.04784">pdf</a>, <a href="https://arxiv.org/ps/2103.04784">ps</a>, <a href="https://arxiv.org/format/2103.04784">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Spatial Equalization Before Reception: Reconfigurable Intelligent Surfaces for Multi-path Mitigation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Hongliang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+L">Lingyang Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Z">Zhu Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Poor%2C+H+V">H. Vincent Poor</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.04784v1-abstract-short" style="display: inline;">
        &hellip;equalizer to address the well-known multi-path fading phenomenon. By introducing some controllable paths artificially against the multi-path fading through the RIS, we can <span class="search-hit mathjax">perform</span> equalization during the transmission process instead of at the receiver, and thus all the users can share the same equalizer. Unlike the beamforming&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.04784v1-abstract-full').style.display = 'inline'; document.getElementById('2103.04784v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.04784v1-abstract-full" style="display: none;">
        Reconfigurable intelligent surfaces (RISs), which enable tunable anomalous reflection, have appeared as a promising method to enhance wireless systems. In this paper, we propose to use an RIS as a spatial equalizer to address the well-known multi-path fading phenomenon. By introducing some controllable paths artificially against the multi-path fading through the RIS, we can <span class="search-hit mathjax">perform</span> equalization during the transmission process instead of at the receiver, and thus all the users can share the same equalizer. Unlike the beamforming <span class="search-hit mathjax">application</span> of the RIS, which aims to maximize the received energy at receivers, the objective of the equalization <span class="search-hit mathjax">application</span> is to <span class="search-hit mathjax">reduce</span> the inter-symbol interference (ISI), which makes phase shifts at the RIS different. To this end, we formulate the phase shift <span class="search-hit mathjax">optimization</span> problem and propose an iterative algorithm to solve it. Simulation results show that the multi-path fading <span class="search-hit mathjax">effect</span> can be eliminated <span class="search-hit mathjax">effectively</span> compared to benchmark schemes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.04784v1-abstract-full').style.display = 'none'; document.getElementById('2103.04784v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IEEE ICASSP 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.04674">arXiv:2103.04674</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.04674">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Structural Coupling for Microservices
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Panichella%2C+S">Sebastiano Panichella</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rahman%2C+M+I">Mohammad Imranur Rahman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Taibi%2C+D">Davide Taibi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.04674v1-abstract-short" style="display: inline;">
        Cloud-native <span class="search-hit mathjax">Applications</span> are &#39;distributed, elastic and horizontal-scalable systems composed of (micro)services which isolate states in a minimum of stateful components&#39;. Hence, an important property is to ensure a low coupling and a high cohesion among the (micro)services composing the cloud-native&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.04674v1-abstract-full').style.display = 'inline'; document.getElementById('2103.04674v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.04674v1-abstract-full" style="display: none;">
        Cloud-native <span class="search-hit mathjax">Applications</span> are &#39;distributed, elastic and horizontal-scalable systems composed of (micro)services which isolate states in a minimum of stateful components&#39;. Hence, an important property is to ensure a low coupling and a high cohesion among the (micro)services composing the cloud-native <span class="search-hit mathjax">application</span>. Loosely coupled and highly cohesive services allow development teams to work in parallel, <span class="search-hit mathjax">reducing</span> the communication overhead between teams. However, despite both practitioners and researchers agree on the importance of this general property, there are no validated metrics to <span class="search-hit mathjax">effectively</span> measure or test the actual coupling level between services. In this work, we propose ways to compute and visualize the coupling between microservices, by extending and adapting the concepts behind the computation of the traditional structural coupling. We validate these measures with a case study involving 17 open-source projects and we provide an <span class="search-hit mathjax">automatic</span> approach to measure them. The results of this study highlight how these metrics provide to practitioners a quantitative and visual view of services compositions, which can be useful to conceive advanced systems to monitor the evolution of the service.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.04674v1-abstract-full').style.display = 'none'; document.getElementById('2103.04674v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11th International Conference on Cloud Computing and Services Science, CLOSER 2021</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        11th International Conference on Cloud Computing and Services Science, CLOSER 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.03239">arXiv:2103.03239</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.03239">pdf</a>, <a href="https://arxiv.org/format/2103.03239">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Moshpit SGD: Communication-<span class="search-hit mathjax">Efficient</span> Decentralized Training on Heterogeneous Unreliable Devices
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ryabinin%2C+M">Max Ryabinin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gorbunov%2C+E">Eduard Gorbunov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Plokhotnyuk%2C+V">Vsevolod Plokhotnyuk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pekhimenko%2C+G">Gennady Pekhimenko</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.03239v1-abstract-short" style="display: inline;">
        &hellip;by using multiple compute nodes. This approach, known as distributed training, can utilize hundreds of computers via specialized message-passing protocols such as Ring All-<span class="search-hit mathjax">Reduce</span>. However, running these protocols at scale requires reliable high-speed networking that is only available in dedicated clusters. In contrast, many real-world&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.03239v1-abstract-full').style.display = 'inline'; document.getElementById('2103.03239v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.03239v1-abstract-full" style="display: none;">
        Training deep neural networks on large datasets can often be accelerated by using multiple compute nodes. This approach, known as distributed training, can utilize hundreds of computers via specialized message-passing protocols such as Ring All-<span class="search-hit mathjax">Reduce</span>. However, running these protocols at scale requires reliable high-speed networking that is only available in dedicated clusters. In contrast, many real-world <span class="search-hit mathjax">applications</span>, such as federated learning and cloud-based distributed training, operate on unreliable devices with unstable network bandwidth. As a result, these <span class="search-hit mathjax">applications</span> are restricted to using parameter servers or gossip-based averaging protocols. In this work, we lift that restriction by proposing Moshpit All-<span class="search-hit mathjax">Reduce</span> -- an iterative averaging protocol that exponentially converges to the global average. We demonstrate the <span class="search-hit mathjax">efficiency</span> of our protocol for distributed <span class="search-hit mathjax">optimization</span> with strong theoretical guarantees. The experiments show 1.3x speedup for ResNet-50 training on ImageNet compared to competitive gossip-based strategies and 1.5x speedup when training ALBERT-large from scratch using preemptible compute nodes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.03239v1-abstract-full').style.display = 'none'; document.getElementById('2103.03239v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">41 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.02904">arXiv:2103.02904</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.02904">pdf</a>, <a href="https://arxiv.org/format/2103.02904">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Effective</span> and Fast: A Novel Sequential Single Path Search for Mixed-<span class="search-hit mathjax">Precision</span> Quantization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Q">Qigong Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiao%2C+L">Licheng Jiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Y">Yan Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiufang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shang%2C+F">Fanhua Shang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+F">Fang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.02904v1-abstract-short" style="display: inline;">
        Since model quantization helps to <span class="search-hit mathjax">reduce</span> the model size and computation latency, it has been successfully applied in many&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.02904v1-abstract-full').style.display = 'inline'; document.getElementById('2103.02904v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.02904v1-abstract-full" style="display: none;">
        Since model quantization helps to <span class="search-hit mathjax">reduce</span> the model size and computation latency, it has been successfully applied in many <span class="search-hit mathjax">applications</span> of mobile phones, embedded devices and smart chips. The mixed-<span class="search-hit mathjax">precision</span> quantization model can match different quantization bit-<span class="search-hit mathjax">precisions</span> according to the sensitivity of different layers to achieve great <span class="search-hit mathjax">performance</span>. However, it is a difficult problem to quickly determine the quantization bit-<span class="search-hit mathjax">precision</span> of each layer in deep neural networks according to some constraints (e.g., hardware resources, energy consumption, model size and computation latency). To address this issue, we propose a novel sequential single path search (SSPS) method for mixed-<span class="search-hit mathjax">precision</span> quantization,in which the given constraints are introduced into its loss function to guide searching process. A single path search cell is used to combine a fully differentiable supernet, which can be <span class="search-hit mathjax">optimized</span> by gradient-based algorithms. Moreover, we sequentially determine the candidate <span class="search-hit mathjax">precisions</span> according to the selection certainties to exponentially <span class="search-hit mathjax">reduce</span> the search space and speed up the convergence of searching process. Experiments show that our method can <span class="search-hit mathjax">efficiently</span> search the mixed-<span class="search-hit mathjax">precision</span> models for different architectures (e.g., ResNet-20, 18, 34, 50 and MobileNet-V2) and datasets (e.g., CIFAR-10, ImageNet and COCO) under given constraints, and our experimental results verify that SSPS significantly outperforms their uniform counterparts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.02904v1-abstract-full').style.display = 'none'; document.getElementById('2103.02904v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.01516">arXiv:2103.01516</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.01516">pdf</a>, <a href="https://arxiv.org/ps/2103.01516">ps</a>, <a href="https://arxiv.org/format/2103.01516">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Private Stochastic Convex <span class="search-hit mathjax">Optimization</span>: <span class="search-hit mathjax">Optimal</span> Rates in $\ell_1$ Geometry
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Asi%2C+H">Hilal Asi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feldman%2C+V">Vitaly Feldman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koren%2C+T">Tomer Koren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.01516v1-abstract-short" style="display: inline;">
        Stochastic convex <span class="search-hit mathjax">optimization</span> over an $\ell_1$-bounded domain is ubiquitous in machine learning <span class="search-hit mathjax">applications</span> such as LASSO but remains poorly understood when learning with differential privacy. We show that, up to logarithmic factors the <span class="search-hit mathjax">optimal</span> excess population loss of any&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.01516v1-abstract-full').style.display = 'inline'; document.getElementById('2103.01516v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.01516v1-abstract-full" style="display: none;">
        Stochastic convex <span class="search-hit mathjax">optimization</span> over an $\ell_1$-bounded domain is ubiquitous in machine learning <span class="search-hit mathjax">applications</span> such as LASSO but remains poorly understood when learning with differential privacy. We show that, up to logarithmic factors the <span class="search-hit mathjax">optimal</span> excess population loss of any $(\varepsilon,δ)$-differentially private <span class="search-hit mathjax">optimizer</span> is $\sqrt{\log(d)/n} + \sqrt{d}/\varepsilon n.$ The upper bound is based on a new algorithm that combines the iterative localization approach of~\citet{FeldmanKoTa20} with a new analysis of private regularized mirror descent. It applies to $\ell_p$ bounded domains for $p\in [1,2]$ and queries at most $n^{3/2}$ gradients <span class="search-hit mathjax">improving</span> over the best previously known algorithm for the $\ell_2$ case which needs $n^2$ gradients. Further, we show that when the loss functions satisfy additional smoothness assumptions, the excess loss is upper bounded (up to logarithmic factors) by $\sqrt{\log(d)/n} + (\log(d)/\varepsilon n)^{2/3}.$ This bound is achieved by a new variance-<span class="search-hit mathjax">reduced</span> version of the Frank-Wolfe algorithm that requires just a single pass over the data. We also show that the lower bound in this case is the minimum of the two rates mentioned above.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.01516v1-abstract-full').style.display = 'none'; document.getElementById('2103.01516v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.01447">arXiv:2103.01447</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.01447">pdf</a>, <a href="https://arxiv.org/format/2103.01447">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ZeroSARAH: <span class="search-hit mathjax">Efficient</span> Nonconvex Finite-Sum <span class="search-hit mathjax">Optimization</span> with Zero Full Gradient Computation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhize Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Richt%C3%A1rik%2C+P">Peter Richtárik</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.01447v1-abstract-short" style="display: inline;">
        We propose ZeroSARAH -- a novel variant of the variance-<span class="search-hit mathjax">reduced</span> method SARAH (Nguyen et al., 2017) -- for minimizing the average of a large number of nonconvex functions $\frac{1}{n}\sum_{i=1}^{n}f_i(x)$. To the best of our knowledge, in this nonconvex finite-sum regime, all existing variance-<span class="search-hit mathjax">reduced</span> methods, including&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.01447v1-abstract-full').style.display = 'inline'; document.getElementById('2103.01447v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.01447v1-abstract-full" style="display: none;">
        We propose ZeroSARAH -- a novel variant of the variance-<span class="search-hit mathjax">reduced</span> method SARAH (Nguyen et al., 2017) -- for minimizing the average of a large number of nonconvex functions $\frac{1}{n}\sum_{i=1}^{n}f_i(x)$. To the best of our knowledge, in this nonconvex finite-sum regime, all existing variance-<span class="search-hit mathjax">reduced</span> methods, including SARAH, SVRG, SAGA and their variants, need to compute the full gradient over all $n$ data samples at the initial point $x^0$, and then periodically compute the full gradient once every few iterations (for SVRG, SARAH and their variants). Moreover, SVRG, SAGA and their variants typically achieve weaker convergence results than variants of SARAH: $n^{2/3}/ε^2$ vs. $n^{1/2}/ε^2$. ZeroSARAH is the first variance-<span class="search-hit mathjax">reduced</span> method which does not require any full gradient computations, not even for the initial point. Moreover, ZeroSARAH obtains new state-of-the-art convergence results, which can <span class="search-hit mathjax">improve</span> the previous best-known result (given by e.g., SPIDER, SpiderBoost, SARAH, SSRGD and PAGE) in certain regimes. Avoiding any full gradient computations (which is a time-consuming step) is important in many <span class="search-hit mathjax">applications</span> as the number of data samples $n$ usually is very large. Especially in the distributed setting, periodic computation of full gradient over all data samples needs to periodically synchronize all machines/devices, which may be impossible or very hard to achieve. Thus, we expect that ZeroSARAH will have a practical impact in distributed and federated learning where full device participation is impractical.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.01447v1-abstract-full').style.display = 'none'; document.getElementById('2103.01447v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.00654">arXiv:2103.00654</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.00654">pdf</a>, <a href="https://arxiv.org/format/2103.00654">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Feedback <span class="search-hit mathjax">Coding</span> for Active Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Canal%2C+G">Gregory Canal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bloch%2C+M">Matthieu Bloch</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rozell%2C+C">Christopher Rozell</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.00654v1-abstract-short" style="display: inline;">
        The iterative selection of examples for labeling in active machine learning is conceptually similar to feedback channel <span class="search-hit mathjax">coding</span> in information theory: in both tasks, the objective is to seek a minimal sequence of actions to encode information in the presence of noise. While this high-level overlap has been previously noted, there remain open questions on how&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.00654v1-abstract-full').style.display = 'inline'; document.getElementById('2103.00654v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.00654v1-abstract-full" style="display: none;">
        The iterative selection of examples for labeling in active machine learning is conceptually similar to feedback channel <span class="search-hit mathjax">coding</span> in information theory: in both tasks, the objective is to seek a minimal sequence of actions to encode information in the presence of noise. While this high-level overlap has been previously noted, there remain open questions on how to best formulate active learning as a communications system to leverage existing analysis and algorithms in feedback <span class="search-hit mathjax">coding</span>. In this work, we formally identify and leverage the structural commonalities between the two problems, including the characterization of encoder and noisy channel components, to design a new algorithm. Specifically, we develop an <span class="search-hit mathjax">optimal</span> transport-based feedback <span class="search-hit mathjax">coding</span> scheme called Approximate Posterior Matching (APM) for the task of active example selection and explore its <span class="search-hit mathjax">application</span> to Bayesian logistic regression, a popular model in active learning. We evaluate APM on a variety of datasets and demonstrate learning <span class="search-hit mathjax">performance</span> comparable to existing active learning methods, at a <span class="search-hit mathjax">reduced</span> computational cost. These results demonstrate the potential of directly deploying concepts from feedback channel <span class="search-hit mathjax">coding</span> to design <span class="search-hit mathjax">efficient</span> active learning strategies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.00654v1-abstract-full').style.display = 'none'; document.getElementById('2103.00654v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">AISTATS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.00025">arXiv:2103.00025</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.00025">pdf</a>, <a href="https://arxiv.org/ps/2103.00025">ps</a>, <a href="https://arxiv.org/format/2103.00025">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TEC: Tensor Ensemble Classifier for Big Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+P">Peide Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Karim%2C+R">Rejaul Karim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maiti%2C+T">Tapabrata Maiti</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.00025v1-abstract-short" style="display: inline;">
        Tensor (multidimensional array) classification problem has become very popular in modern <span class="search-hit mathjax">applications</span> such as image recognition and high dimensional spatio-temporal data analysis. Support Tensor Machine (STM) classifier, which is extended from the support vector machine, takes CANDECOMP / Parafac (CP) form of tensor data as input and predicts the data labels&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.00025v1-abstract-full').style.display = 'inline'; document.getElementById('2103.00025v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.00025v1-abstract-full" style="display: none;">
        Tensor (multidimensional array) classification problem has become very popular in modern <span class="search-hit mathjax">applications</span> such as image recognition and high dimensional spatio-temporal data analysis. Support Tensor Machine (STM) classifier, which is extended from the support vector machine, takes CANDECOMP / Parafac (CP) form of tensor data as input and predicts the data labels. The distribution-free and statistically consistent properties of STM highlight its potential in successfully handling wide varieties of data <span class="search-hit mathjax">applications</span>. Training a STM can be computationally expensive with high-dimensional tensors. However, <span class="search-hit mathjax">reducing</span> the size of tensor with a random projection technique can <span class="search-hit mathjax">reduce</span> the computational time and cost, making it feasible to handle large size tensors on regular machines. We name an STM estimated with randomly projected tensor as Random Projection-based Support Tensor Machine (RPSTM). In this work, we propose a Tensor Ensemble Classifier (TEC), which aggregates multiple RPSTMs for big tensor classification. TEC utilizes the ensemble idea to minimize the excessive classification risk brought by random projection, providing statistically consistent predictions while taking the computational advantage of RPSTM. Since each RPSTM can be estimated independently, TEC can further take advantage of parallel computing techniques and be more computationally <span class="search-hit mathjax">efficient</span>. The theoretical and numerical results demonstrate the decent <span class="search-hit mathjax">performance</span> of TEC model in high-dimensional tensor classification problems. The model prediction is statistically consistent as its risk is shown to converge to the <span class="search-hit mathjax">optimal</span> Bayes risk. Besides, we highlight the trade-off between the computational cost and the prediction risk for TEC model. The method is validated by extensive simulation and a real data example. We prepare a python package for applying TEC, which is available at our GitHub.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.00025v1-abstract-full').style.display = 'none'; document.getElementById('2103.00025v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.13410">arXiv:2102.13410</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.13410">pdf</a>, <a href="https://arxiv.org/ps/2102.13410">ps</a>, <a href="https://arxiv.org/format/2102.13410">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Variable Vector Length SIMD Architecture for HW/SW Co-designed Processors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+R">Rakesh Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martinez%2C+A">Alejandro Martinez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gonzalez%2C+A">Antonio Gonzalez</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.13410v1-abstract-short" style="display: inline;">
        Hardware/<span class="search-hit mathjax">Software</span> (HW/SW) co-designed processors provide a promising solution to the power and complexity problems of the modern microprocessors by keeping their hardware simple. Moreover, they employ several runtime&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.13410v1-abstract-full').style.display = 'inline'; document.getElementById('2102.13410v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.13410v1-abstract-full" style="display: none;">
        Hardware/<span class="search-hit mathjax">Software</span> (HW/SW) co-designed processors provide a promising solution to the power and complexity problems of the modern microprocessors by keeping their hardware simple. Moreover, they employ several runtime <span class="search-hit mathjax">optimizations</span> to <span class="search-hit mathjax">improve</span> the <span class="search-hit mathjax">performance</span>. One of the most potent <span class="search-hit mathjax">optimizations</span>, vectorization, has been utilized by modern microprocessors, to exploit the data level parallelism through SIMD accelerators. Due to their hardware simplicity, these accelerators have evolved in terms of width from 64-bit vectors in Intel MMX to 512-bit wide vector units in Intel Xeon Phi and AVX-512. Although SIMD accelerators are simple in terms of hardware design, <span class="search-hit mathjax">code</span> generation for them has always been a challenge. Moreover, increasing vector lengths with each new generation add to this complexity.
  This paper explores the scalability of SIMD accelerators from the <span class="search-hit mathjax">code</span> generation point of view. We discover that the SIMD accelerators remain underutilized at higher vector lengths mainly due to: a) <span class="search-hit mathjax">reduced</span> dynamic instruction stream coverage for vectorization and b) increase in permutations. Both of these factors can be attributed to the rigidness of the SIMD architecture. We propose a novel SIMD architecture that possesses the flexibility needed to support higher vector lengths. Furthermore, we propose Variable Length Vectorization and Selective Writing in a HW/SW co-designed environment to transparently target the flexibility of the proposed architecture. We evaluate our proposals using a set of SPECFP2006 and Physicsbench <span class="search-hit mathjax">applications</span>. Our experimental results show an average dynamic instruction reduction of 31% and 40% and an average speed up of 13% and 10% for SPECFP2006 and Physicsbench respectively, for 512-bit vector length, over the scalar baseline <span class="search-hit mathjax">code</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.13410v1-abstract-full').style.display = 'none'; document.getElementById('2102.13410v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.13403">arXiv:2102.13403</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.13403">pdf</a>, <a href="https://arxiv.org/format/2102.13403">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-fidelity regression using artificial neural networks: <span class="search-hit mathjax">efficient</span> approximation of parameter-dependent output quantities
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+M">Mengwu Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Manzoni%2C+A">Andrea Manzoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amendt%2C+M">Maurice Amendt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Conti%2C+P">Paolo Conti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hesthaven%2C+J+S">Jan S. Hesthaven</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.13403v1-abstract-short" style="display: inline;">
        &hellip;regression. The introduced models are compared against a traditional multi-fidelity scheme, co-kriging. A collection of artificial benchmarks are presented to measure the <span class="search-hit mathjax">performance</span> of the analyzed models. The results show that cross-validation in combination with Bayesian&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.13403v1-abstract-full').style.display = 'inline'; document.getElementById('2102.13403v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.13403v1-abstract-full" style="display: none;">
        Highly accurate numerical or physical experiments are often time-consuming or expensive to obtain. When time or budget restrictions prohibit the generation of additional data, the amount of available samples may be too limited to provide satisfactory model results. Multi-fidelity methods deal with such problems by incorporating information from other sources, which are ideally well-correlated with the high-fidelity data, but can be obtained at a lower cost. By leveraging correlations between different data sets, multi-fidelity methods often yield superior generalization when compared to models based solely on a small amount of high-fidelity data. In this work, we present the use of artificial neural networks applied to multi-fidelity regression problems. By elaborating a few existing approaches, we propose new neural network architectures for multi-fidelity regression. The introduced models are compared against a traditional multi-fidelity scheme, co-kriging. A collection of artificial benchmarks are presented to measure the <span class="search-hit mathjax">performance</span> of the analyzed models. The results show that cross-validation in combination with Bayesian <span class="search-hit mathjax">optimization</span> consistently leads to neural network models that outperform the co-kriging scheme. Additionally, we show an <span class="search-hit mathjax">application</span> of multi-fidelity regression to an engineering problem. The propagation of a pressure wave into an acoustic horn with parametrized shape and frequency is considered, and the index of reflection intensity is approximated using the multi-fidelity models. A finite element model and a <span class="search-hit mathjax">reduced</span> basis model are adopted as the high- and low-fidelity, respectively. It is shown that the multi-fidelity neural network returns outputs that achieve a comparable <span class="search-hit mathjax">accuracy</span> to those from the expensive, full-order model, using only very few full-order evaluations combined with a larger amount of inaccurate but cheap evaluations of a <span class="search-hit mathjax">reduced</span> order model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.13403v1-abstract-full').style.display = 'none'; document.getElementById('2102.13403v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.12466">arXiv:2102.12466</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.12466">pdf</a>, <a href="https://arxiv.org/format/2102.12466">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Information Directed Reward Learning for Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lindner%2C+D">David Lindner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Turchetta%2C+M">Matteo Turchetta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tschiatschek%2C+S">Sebastian Tschiatschek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ciosek%2C+K">Kamil Ciosek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Krause%2C+A">Andreas Krause</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.12466v1-abstract-short" style="display: inline;">
        For many reinforcement learning (RL) <span class="search-hit mathjax">applications</span>, specifying a reward is difficult. In this paper, we consider an RL setting where the agent can obtain information about the reward only by querying an expert that can, for example, evaluate individual states or provide binary preferences over trajectories. From such expensive feedback, we aim to learn a mode&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.12466v1-abstract-full').style.display = 'inline'; document.getElementById('2102.12466v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.12466v1-abstract-full" style="display: none;">
        For many reinforcement learning (RL) <span class="search-hit mathjax">applications</span>, specifying a reward is difficult. In this paper, we consider an RL setting where the agent can obtain information about the reward only by querying an expert that can, for example, evaluate individual states or provide binary preferences over trajectories. From such expensive feedback, we aim to learn a model of the reward function that allows standard RL algorithms to achieve high expected return with as few expert queries as possible. For this purpose, we propose Information Directed Reward Learning (IDRL), which uses a Bayesian model of the reward function and selects queries that maximize the information gain about the difference in return between potentially <span class="search-hit mathjax">optimal</span> policies. In contrast to prior active reward learning methods designed for specific types of queries, IDRL naturally accommodates different query types. Moreover, by shifting the focus from <span class="search-hit mathjax">reducing</span> the reward approximation error to <span class="search-hit mathjax">improving</span> the policy induced by the reward model, it achieves similar or better <span class="search-hit mathjax">performance</span> with significantly fewer queries. We support our findings with extensive evaluations in multiple environments and with different types of queries.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.12466v1-abstract-full').style.display = 'none'; document.getElementById('2102.12466v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.12086">arXiv:2102.12086</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.12086">pdf</a>, <a href="https://arxiv.org/format/2102.12086">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Dynamical Systems">math.DS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Modern Koopman Theory for Dynamical Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Brunton%2C+S+L">Steven L. Brunton</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Budi%C5%A1i%C4%87%2C+M">Marko Budišić</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaiser%2C+E">Eurika Kaiser</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kutz%2C+J+N">J. Nathan Kutz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.12086v1-abstract-short" style="display: inline;">
        &hellip;big-data and machine learning techniques, and 3) simple, yet powerful numerical algorithms, such as the dynamic mode decomposition (DMD), have been developed and extended to <span class="search-hit mathjax">reduce</span> Koopman theory to practice in real-world&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.12086v1-abstract-full').style.display = 'inline'; document.getElementById('2102.12086v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.12086v1-abstract-full" style="display: none;">
        The field of dynamical systems is being transformed by the mathematical tools and algorithms emerging from modern computing and data science. First-principles derivations and asymptotic reductions are giving way to data-driven approaches that formulate models in operator theoretic or probabilistic frameworks. Koopman spectral theory has emerged as a dominant perspective over the past decade, in which nonlinear dynamics are represented in terms of an infinite-dimensional linear operator acting on the space of all possible measurement functions of the system. This linear representation of nonlinear dynamics has tremendous potential to enable the prediction, estimation, and control of nonlinear systems with standard textbook methods developed for linear systems. However, obtaining finite-dimensional coordinate systems and embeddings in which the dynamics appear approximately linear remains a central open challenge. The success of Koopman analysis is due primarily to three key factors: 1) there exists rigorous theory connecting it to classical geometric approaches for dynamical systems, 2) the approach is formulated in terms of measurements, making it ideal for leveraging big-data and machine learning techniques, and 3) simple, yet powerful numerical algorithms, such as the dynamic mode decomposition (DMD), have been developed and extended to <span class="search-hit mathjax">reduce</span> Koopman theory to practice in real-world <span class="search-hit mathjax">applications</span>. In this review, we provide an overview of modern Koopman operator theory, describing recent theoretical and algorithmic developments and highlighting these methods with a diverse range of <span class="search-hit mathjax">applications</span>. We also discuss key advances and challenges in the rapidly growing field of machine learning that are likely to drive future developments and significantly transform the theoretical landscape of dynamical systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.12086v1-abstract-full').style.display = 'none'; document.getElementById('2102.12086v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">96 pages, 27 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          34A34; 37A30; 37C10; 37M10; 37M99; 37N35; 47A35; 47B33
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.11559">arXiv:2102.11559</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.11559">pdf</a>, <a href="https://arxiv.org/format/2102.11559">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Toward Speeding up Mutation Analysis by Memoizing Expensive Methods
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ghanbari%2C+A">Ali Ghanbari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marcus%2C+A">Andrian Marcus</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.11559v1-abstract-short" style="display: inline;">
        Mutation analysis has many <span class="search-hit mathjax">applications</span>, such as assessing the quality of test cases, fault localization, test input generation, security analysis, etc. Such&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11559v1-abstract-full').style.display = 'inline'; document.getElementById('2102.11559v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.11559v1-abstract-full" style="display: none;">
        Mutation analysis has many <span class="search-hit mathjax">applications</span>, such as assessing the quality of test cases, fault localization, test input generation, security analysis, etc. Such <span class="search-hit mathjax">applications</span> involve running test suite against a large number of <span class="search-hit mathjax">program</span> mutants leading to poor scalability. Much research has been aimed at speeding up this process, focusing on <span class="search-hit mathjax">reducing</span> the number of mutants, the number of executed tests, or the execution time of the mutants. This paper presents a novel approach, named MeMu, for <span class="search-hit mathjax">reducing</span> the execution time of the mutants, by memoizing the most expensive methods in the system. Memoization is an <span class="search-hit mathjax">optimization</span> technique that allows bypassing the execution of expensive methods, when repeated inputs are detected. MeMu can be used in conjunction with existing acceleration techniques. We implemented MeMu on top of PITest, a well-known JVM bytecode-level mutation analysis system, and obtained, on average, an 18.15% speed-up over PITest, in the execution time of the mutants for 12 real-world <span class="search-hit mathjax">programs</span>. These promising results and the fact that MeMu could also be used for other <span class="search-hit mathjax">applications</span> that involve repeated execution of tests (e.g., <span class="search-hit mathjax">automatic</span> <span class="search-hit mathjax">program</span> repair and regression testing), strongly support future research for <span class="search-hit mathjax">improving</span> its <span class="search-hit mathjax">efficiency</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11559v1-abstract-full').style.display = 'none'; document.getElementById('2102.11559v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In proceedings of the 43rd ACM/IEEE International Conference on <span class="search-hit mathjax">Software</span> Engineering (ICSE'21) NIER</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.11492">arXiv:2102.11492</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.11492">pdf</a>, <a href="https://arxiv.org/format/2102.11492">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DeepThermal: Combustion <span class="search-hit mathjax">Optimization</span> for Thermal Power Generating Units Using Offline Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhan%2C+X">Xianyuan Zhan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Haoran Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yue Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huo%2C+Y">Yusen Huo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+X">Xiangyu Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+H">Honglei Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Y">Yu Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.11492v2-abstract-short" style="display: inline;">
        Thermal power generation plays a dominant role in the world&#39;s electricity supply. It consumes large amounts of coal worldwide, and causes serious air pollution. <span class="search-hit mathjax">Optimizing</span> the combustion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11492v2-abstract-full').style.display = 'inline'; document.getElementById('2102.11492v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.11492v2-abstract-full" style="display: none;">
        Thermal power generation plays a dominant role in the world&#39;s electricity supply. It consumes large amounts of coal worldwide, and causes serious air pollution. <span class="search-hit mathjax">Optimizing</span> the combustion <span class="search-hit mathjax">efficiency</span> of a thermal power generating unit (TPGU) is a highly challenging and critical task in the energy industry. We develop a new data-driven AI system, namely DeepThermal, to <span class="search-hit mathjax">optimize</span> the combustion control strategy for TPGUs. At its core, is a new model-based offline reinforcement learning (RL) framework, called MORE, which leverages logged historical operational data of a TPGU to solve a highly complex constrained Markov decision process problem via purely offline training. MORE aims at simultaneously <span class="search-hit mathjax">improving</span> the long-term reward (increase combustion <span class="search-hit mathjax">efficiency</span> and <span class="search-hit mathjax">reduce</span> pollutant emission) and controlling operational risks (safety constraints satisfaction). In DeepThermal, we first learn a data-driven combustion process simulator from the offline dataset. The RL agent of MORE is then trained by combining real historical data as well as carefully filtered and processed simulation data through a novel restrictive exploration scheme. DeepThermal has been successfully deployed in four large coal-fired thermal power plants in China. Real-world experiments show that DeepThermal <span class="search-hit mathjax">effectively</span> <span class="search-hit mathjax">improves</span> the combustion <span class="search-hit mathjax">efficiency</span> of a TPGU. We also report and demonstrate the superior <span class="search-hit mathjax">performance</span> of MORE by comparing with the state-of-the-art algorithms on the standard offline RL benchmarks. To the best knowledge of the authors, DeepThermal is the first AI <span class="search-hit mathjax">application</span> that has been used to solve real-world complex mission-critical control tasks using the offline RL approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11492v2-abstract-full').style.display = 'none'; document.getElementById('2102.11492v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 February, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.11289">arXiv:2102.11289</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.11289">pdf</a>, <a href="https://arxiv.org/format/2102.11289">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Analysis, Statistics and Probability">physics.data-an</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Instrumentation and Detectors">physics.ins-det</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ps and Qs: Quantization-aware pruning for <span class="search-hit mathjax">efficient</span> low latency neural network inference
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hawks%2C+B">Benjamin Hawks</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duarte%2C+J">Javier Duarte</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fraser%2C+N+J">Nicholas J. Fraser</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pappalardo%2C+A">Alessandro Pappalardo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tran%2C+N">Nhan Tran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Umuroglu%2C+Y">Yaman Umuroglu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.11289v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Efficient</span> machine learning implementations&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11289v1-abstract-full').style.display = 'inline'; document.getElementById('2102.11289v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.11289v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Efficient</span> machine learning implementations <span class="search-hit mathjax">optimized</span> for inference in hardware have wide-ranging benefits depending on the <span class="search-hit mathjax">application</span> from lower inference latencies to higher data throughputs to more <span class="search-hit mathjax">efficient</span> energy consumption. Two popular techniques for <span class="search-hit mathjax">reducing</span> computation in neural networks are pruning, removing insignificant synapses, and quantization, <span class="search-hit mathjax">reducing</span> the <span class="search-hit mathjax">precision</span> of the calculations. In this work, we explore the interplay between pruning and quantization during the training of neural networks for ultra low latency <span class="search-hit mathjax">applications</span> targeting high energy physics use cases. However, techniques developed for this study have potential <span class="search-hit mathjax">application</span> across many other domains. We study various configurations of pruning during quantization-aware training, which we term \emph{quantization-aware pruning} and the <span class="search-hit mathjax">effect</span> of techniques like regularization, batch normalization, and different pruning schemes on multiple computational or neural <span class="search-hit mathjax">efficiency</span> metrics. We find that quantization-aware pruning yields more computationally <span class="search-hit mathjax">efficient</span> models than either pruning or quantization alone for our task. Further, quantization-aware pruning typically <span class="search-hit mathjax">performs</span> similar to or better in terms of computational <span class="search-hit mathjax">efficiency</span> compared to standard neural architecture <span class="search-hit mathjax">optimization</span> techniques. While the <span class="search-hit mathjax">accuracy</span> for the benchmark <span class="search-hit mathjax">application</span> may be similar, the information content of the network can vary significantly based on the training configuration.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11289v1-abstract-full').style.display = 'none'; document.getElementById('2102.11289v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">22 pages, 7 Figures, 1 Table</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          FERMILAB-PUB-21-056-SCD
        

        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.11222">arXiv:2102.11222</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.11222">pdf</a>, <a href="https://arxiv.org/format/2102.11222">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Learning for THz Drones with Flying Intelligent Surfaces: Beam and Handoff Prediction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abuzainab%2C+N">Nof Abuzainab</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alrabeiah%2C+M">Muhammad Alrabeiah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alkhateeb%2C+A">Ahmed Alkhateeb</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sagduyu%2C+Y+E">Yalin E. Sagduyu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.11222v1-abstract-short" style="display: inline;">
        &hellip;from operating in the THz band to achieve high data rates (such as considered for 6G). However, THz communications are highly susceptible to channel impairments and blockage <span class="search-hit mathjax">effects</span> that become extra challenging when accounting for drone mobility. RISs offer flexibility to extend coverage by adapting to channel dynamics. To integrate RISs into THz drone comm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11222v1-abstract-full').style.display = 'inline'; document.getElementById('2102.11222v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.11222v1-abstract-full" style="display: none;">
        We consider the problem of proactive handoff and beam selection in Terahertz (THz) drone communication networks assisted with reconfigurable intelligent surfaces (RIS). Drones have emerged as critical assets for next-generation wireless networks to provide seamless connectivity and extend the coverage, and can largely benefit from operating in the THz band to achieve high data rates (such as considered for 6G). However, THz communications are highly susceptible to channel impairments and blockage <span class="search-hit mathjax">effects</span> that become extra challenging when accounting for drone mobility. RISs offer flexibility to extend coverage by adapting to channel dynamics. To integrate RISs into THz drone communications, we propose a novel deep learning solution based on a recurrent neural network, namely the Gated Recurrent Unit (GRU), that proactively predicts the serving base station/RIS and the serving beam for each drone based on the prior observations of drone location/beam trajectories. This solution has the potential to extend the coverage of drones and enhance the reliability of next-generation wireless communications. Predicting future beams based on the drone beam/position trajectory significantly <span class="search-hit mathjax">reduces</span> the beam training overhead and its associated latency, and thus emerges as a viable solution to serve time-critical <span class="search-hit mathjax">applications</span>. Numerical results based on realistic 3D ray-tracing simulations show that the proposed deep learning solution is promising for future RIS-assisted THz networks by achieving near-<span class="search-hit mathjax">optimal</span> proactive hand-off <span class="search-hit mathjax">performance</span> and more than 90% <span class="search-hit mathjax">accuracy</span> for beam prediction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11222v1-abstract-full').style.display = 'none'; document.getElementById('2102.11222v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.11210">arXiv:2102.11210</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.11210">pdf</a>, <a href="https://arxiv.org/format/2102.11210">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Non-Convex <span class="search-hit mathjax">Optimization</span> with Spectral Radius Regularization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sandler%2C+A">Adam Sandler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Klabjan%2C+D">Diego Klabjan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+Y">Yuan Luo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.11210v1-abstract-short" style="display: inline;">
        &hellip;allowing models to better generalize to real word test data, which may be distributed differently from the training data. Specifically, we propose a method of regularized <span class="search-hit mathjax">optimization</span> to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11210v1-abstract-full').style.display = 'inline'; document.getElementById('2102.11210v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.11210v1-abstract-full" style="display: none;">
        We develop a regularization method which finds flat minima during the training of deep neural networks and other machine learning models. These minima generalize better than sharp minima, allowing models to better generalize to real word test data, which may be distributed differently from the training data. Specifically, we propose a method of regularized <span class="search-hit mathjax">optimization</span> to <span class="search-hit mathjax">reduce</span> the spectral radius of the Hessian of the loss function. Additionally, we derive algorithms to <span class="search-hit mathjax">efficiently</span> <span class="search-hit mathjax">perform</span> this <span class="search-hit mathjax">optimization</span> on neural networks and prove convergence results for these algorithms. Furthermore, we demonstrate that our algorithm works <span class="search-hit mathjax">effectively</span> on multiple real world <span class="search-hit mathjax">applications</span> in multiple domains including healthcare. In order to show our models generalize well, we introduce different methods of testing generalizability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11210v1-abstract-full').style.display = 'none'; document.getElementById('2102.11210v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.10846">arXiv:2102.10846</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.10846">pdf</a>, <a href="https://arxiv.org/ps/2102.10846">ps</a>, <a href="https://arxiv.org/format/2102.10846">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Expanding boundaries of Gap Safe screening
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dantas%2C+C">Cassio Dantas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soubies%2C+E">Emmanuel Soubies</a>, 
      
      <a href="/search/?searchtype=author&amp;query=F%C3%A9votte%2C+C">Cédric Févotte</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.10846v1-abstract-short" style="display: inline;">
        Sparse <span class="search-hit mathjax">optimization</span> problems are ubiquitous in many fields such as statistics, signal/image processing and machine learning. This has led to the birth of many iterative algorithms to solve them. A powerful strategy to boost the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.10846v1-abstract-full').style.display = 'inline'; document.getElementById('2102.10846v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.10846v1-abstract-full" style="display: none;">
        Sparse <span class="search-hit mathjax">optimization</span> problems are ubiquitous in many fields such as statistics, signal/image processing and machine learning. This has led to the birth of many iterative algorithms to solve them. A powerful strategy to boost the <span class="search-hit mathjax">performance</span> of these algorithms is known as safe screening: it allows the early identification of zero coordinates in the solution, which can then be eliminated to <span class="search-hit mathjax">reduce</span> the problem&#39;s size and accelerate convergence. In this work, we extend the existing Gap Safe screening framework by relaxing the global strong-concavity assumption on the dual cost function. Instead, we exploit local regularity properties, that is, strong concavity on well-chosen subsets of the domain. The non-negativity constraint is also integrated to the existing framework. Besides making safe screening possible to a broader class of functions that includes beta-divergences (e.g., the Kullback-Leibler divergence), the proposed approach also <span class="search-hit mathjax">improves</span> upon the existing Gap Safe screening rules on previously <span class="search-hit mathjax">applicable</span> cases (e.g., logistic regression). The proposed general framework is exemplified by some notable particular cases: logistic function, beta = 1.5 and Kullback-Leibler divergences. Finally, we showcase the <span class="search-hit mathjax">effectiveness</span> of the proposed screening rules with different solvers (coordinate descent, multiplicative-update and proximal gradient algorithms) and different data sets (binary classification, hyperspectral and count data).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.10846v1-abstract-full').style.display = 'none'; document.getElementById('2102.10846v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.10800">arXiv:2102.10800</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.10800">pdf</a>, <a href="https://arxiv.org/format/2102.10800">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Characterizing and <span class="search-hit mathjax">Optimizing</span> EDA Flows for the Cloud
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hosny%2C+A">Abdelrahman Hosny</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Reda%2C+S">Sherief Reda</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.10800v1-abstract-short" style="display: inline;">
        &hellip;been little to no public information on these characteristics. Thus, in this paper, we formulate the problem of migrating EDA jobs to the cloud. First, we characterize the <span class="search-hit mathjax">performance</span> of four main EDA&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.10800v1-abstract-full').style.display = 'inline'; document.getElementById('2102.10800v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.10800v1-abstract-full" style="display: none;">
        Cloud computing accelerates design space exploration in logic synthesis, and parameter tuning in physical design. However, deploying EDA jobs on the cloud requires EDA teams to deeply understand the characteristics of their jobs in cloud environments. Unfortunately, there has been little to no public information on these characteristics. Thus, in this paper, we formulate the problem of migrating EDA jobs to the cloud. First, we characterize the <span class="search-hit mathjax">performance</span> of four main EDA <span class="search-hit mathjax">applications</span>, namely: synthesis, placement, routing and static timing analysis. We show that different EDA jobs require different machine configurations. Second, using observations from our characterization, we propose a novel model based on Graph Convolutional Networks to predict the total runtime of a given <span class="search-hit mathjax">application</span> on different machine configurations. Our model achieves a prediction <span class="search-hit mathjax">accuracy</span> of 87%. Third, we develop a new formulation for <span class="search-hit mathjax">optimizing</span> cloud deployments in order to <span class="search-hit mathjax">reduce</span> deployment costs while meeting deadline constraints. We present a pseudo-polynomial <span class="search-hit mathjax">optimal</span> solution using a multi-choice knapsack mapping that <span class="search-hit mathjax">reduces</span> costs by 35.29%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.10800v1-abstract-full').style.display = 'none'; document.getElementById('2102.10800v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at DATE2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.10707">arXiv:2102.10707</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.10707">pdf</a>, <a href="https://arxiv.org/format/2102.10707">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box <span class="search-hit mathjax">Optimization</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">HanQin Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lou%2C+Y">Yuchen Lou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McKenzie%2C+D">Daniel McKenzie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+W">Wotao Yin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.10707v2-abstract-short" style="display: inline;">
        We consider the zeroth-order <span class="search-hit mathjax">optimization</span> problem in the huge-scale setting, where the dimension of the problem is so large that&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.10707v2-abstract-full').style.display = 'inline'; document.getElementById('2102.10707v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.10707v2-abstract-full" style="display: none;">
        We consider the zeroth-order <span class="search-hit mathjax">optimization</span> problem in the huge-scale setting, where the dimension of the problem is so large that <span class="search-hit mathjax">performing</span> even basic vector operations on the decision variables is infeasible. In this paper, we propose a novel algorithm, coined ZO-BCD, that exhibits favorable overall query complexity and has a much smaller per-iteration computational complexity. In addition, we discuss how the memory footprint of ZO-BCD can be <span class="search-hit mathjax">reduced</span> even further by the clever use of circulant measurement matrices. As an <span class="search-hit mathjax">application</span> of our new method, we propose the idea of crafting adversarial attacks on neural network based classifiers in a wavelet domain, which can result in problem dimensions of over 1.7 million. In particular, we show that crafting adversarial examples to audio classifiers in a wavelet domain can achieve the state-of-the-art attack success rate of 97.9%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.10707v2-abstract-full').style.display = 'none'; document.getElementById('2102.10707v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICML 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.10294">arXiv:2102.10294</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.10294">pdf</a>, <a href="https://arxiv.org/format/2102.10294">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An unbiased ray-marching transmittance estimator
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kettunen%2C+M">Markus Kettunen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=d%27Eon%2C+E">Eugene d&#39;Eon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pantaleoni%2C+J">Jacopo Pantaleoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Novak%2C+J">Jan Novak</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.10294v1-abstract-short" style="display: inline;">
        We present an in-depth analysis of the sources of variance in state-of-the-art unbiased volumetric transmittance estimators, and propose several new methods for <span class="search-hit mathjax">improving</span> their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.10294v1-abstract-full').style.display = 'inline'; document.getElementById('2102.10294v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.10294v1-abstract-full" style="display: none;">
        We present an in-depth analysis of the sources of variance in state-of-the-art unbiased volumetric transmittance estimators, and propose several new methods for <span class="search-hit mathjax">improving</span> their <span class="search-hit mathjax">efficiency</span>. These combine to produce a single estimator that is universally <span class="search-hit mathjax">optimal</span> relative to prior work, with up to several orders of magnitude lower variance at the same cost, and has zero variance for any ray with non-varying extinction. We first <span class="search-hit mathjax">reduce</span> the variance of truncated power-series estimators using a novel <span class="search-hit mathjax">efficient</span> <span class="search-hit mathjax">application</span> of U-statistics. We then greatly <span class="search-hit mathjax">reduce</span> the average expansion order of the power series and redistribute density evaluations to filter the optical depth estimates with an equidistant sampling comb. Combined with the use of an online control variate built from a sampled mean density estimate, the resulting estimator <span class="search-hit mathjax">effectively</span> <span class="search-hit mathjax">performs</span> ray marching most of the time while using rarely-sampled higher order terms to correct the bias.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.10294v1-abstract-full').style.display = 'none'; document.getElementById('2102.10294v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.09336">arXiv:2102.09336</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.09336">pdf</a>, <a href="https://arxiv.org/format/2102.09336">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FIXME: Enhance <span class="search-hit mathjax">Software</span> Reliability with Hybrid Approaches in Cloud
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hwang%2C+J">Jinho Hwang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shwartz%2C+L">Larisa Shwartz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Qing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batta%2C+R">Raghav Batta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+H">Harshit Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nidd%2C+M">Michael Nidd</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.09336v1-abstract-short" style="display: inline;">
        &hellip;integration/deployment (CICD) in cloud connects developers who need to deliver value faster and more transparently with site reliability engineers (SREs) who need to manage <span class="search-hit mathjax">applications</span> reliably. SREs feed back development issues to developers, and developers commit fixes and trigger CICD to redeploy. The release cycle is more continuous than ever, thus the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.09336v1-abstract-full').style.display = 'inline'; document.getElementById('2102.09336v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.09336v1-abstract-full" style="display: none;">
        With the promise of reliability in cloud, more enterprises are migrating to cloud. The process of continuous integration/deployment (CICD) in cloud connects developers who need to deliver value faster and more transparently with site reliability engineers (SREs) who need to manage <span class="search-hit mathjax">applications</span> reliably. SREs feed back development issues to developers, and developers commit fixes and trigger CICD to redeploy. The release cycle is more continuous than ever, thus the <span class="search-hit mathjax">code</span> to production is faster and more <span class="search-hit mathjax">automated</span>. To provide this higher level agility, the cloud platforms become more complex in the face of flexibility with deeper layers of virtualization. However, reliability does not come for free with all these complexities. <span class="search-hit mathjax">Software</span> engineers and SREs need to deal with wider information spectrum from virtualized layers. Therefore, providing correlated information with true positive evidences is critical to identify the root cause of issues quickly in order to <span class="search-hit mathjax">reduce</span> mean time to recover (MTTR), <span class="search-hit mathjax">performance</span> metrics for SREs. Similarity, knowledge, or statistics driven approaches have been <span class="search-hit mathjax">effective</span>, but with increasing data volume and types, an individual approach is limited to correlate semantic relations of different data sources. In this paper, we introduce FIXME to enhance <span class="search-hit mathjax">software</span> reliability with hybrid diagnosis approaches for enterprises. Our evaluation results show using hybrid diagnosis approach is about 17% better in <span class="search-hit mathjax">precision</span>. The results are helpful for both practitioners and researchers to develop hybrid diagnosis in the highly dynamic cloud environment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.09336v1-abstract-full').style.display = 'none'; document.getElementById('2102.09336v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICSE SEIP, 2021</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.1
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.09050">arXiv:2102.09050</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.09050">pdf</a>, <a href="https://arxiv.org/ps/2102.09050">ps</a>, <a href="https://arxiv.org/format/2102.09050">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        End-to-end learnable EEG channel selection for deep neural networks with Gumbel-softmax
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Strypsteen%2C+T">Thomas Strypsteen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertrand%2C+A">Alexander Bertrand</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.09050v3-abstract-short" style="display: inline;">
        Many electroencephalography (EEG) <span class="search-hit mathjax">applications</span> rely on channel selection methods to remove the least informative channels, e.g., to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.09050v3-abstract-full').style.display = 'inline'; document.getElementById('2102.09050v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.09050v3-abstract-full" style="display: none;">
        Many electroencephalography (EEG) <span class="search-hit mathjax">applications</span> rely on channel selection methods to remove the least informative channels, e.g., to <span class="search-hit mathjax">reduce</span> the amount of electrodes to be mounted, to decrease the computational load, or to <span class="search-hit mathjax">reduce</span> overfitting <span class="search-hit mathjax">effects</span> and <span class="search-hit mathjax">improve</span> <span class="search-hit mathjax">performance</span>. Wrapper-based channel selection methods aim to match the channel selection step to the target model, yet they require to re-train the model multiple times on different candidate channel subsets, which often leads to an unacceptably high computational cost, especially when said model is a (deep) neural network. To alleviate this, we propose a framework to embed the EEG channel selection in the neural network itself to jointly learn the network weights and <span class="search-hit mathjax">optimal</span> channels in an end-to-end manner by traditional backpropagation algorithms. We deal with the discrete nature of this new <span class="search-hit mathjax">optimization</span> problem by employing continuous relaxations of the discrete channel selection parameters based on the Gumbel-softmax trick. We also propose a regularization method that discourages selecting channels more than once. This generic approach is evaluated on two different EEG tasks: motor imagery brain-computer interfaces and auditory attention decoding. The results demonstrate that our framework is generally <span class="search-hit mathjax">applicable</span>, while being competitive with state-of-the art EEG channel selection methods, tailored to these tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.09050v3-abstract-full').style.display = 'none'; document.getElementById('2102.09050v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Updated revisions</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.08571">arXiv:2102.08571</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.08571">pdf</a>, <a href="https://arxiv.org/ps/2102.08571">ps</a>, <a href="https://arxiv.org/format/2102.08571">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Triggered Markov Decision Processes
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Yunhan Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Q">Quanyan Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.08571v1-abstract-short" style="display: inline;">
        &hellip;Markov Decision Processes (MDPs) with self-triggered strategies, where the idea of self-triggered control is extended to more generic MDP models. This extension broadens the <span class="search-hit mathjax">application</span> of self-triggering policies to a broader range of systems. We study the co-design problems of the control policy and the triggering policy to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.08571v1-abstract-full').style.display = 'inline'; document.getElementById('2102.08571v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.08571v1-abstract-full" style="display: none;">
        In this paper, we study Markov Decision Processes (MDPs) with self-triggered strategies, where the idea of self-triggered control is extended to more generic MDP models. This extension broadens the <span class="search-hit mathjax">application</span> of self-triggering policies to a broader range of systems. We study the co-design problems of the control policy and the triggering policy to <span class="search-hit mathjax">optimize</span> two pre-specified cost criteria. The first cost criterion is introduced by incorporating a pre-specified update penalty into the traditional MDP cost criteria to <span class="search-hit mathjax">reduce</span> the use of communication resources. Under this criteria, a novel dynamic <span class="search-hit mathjax">programming</span> (DP) equation called DP equation with <span class="search-hit mathjax">optimized</span> lookahead to proposed to solve for the self-triggering policy under this criteria. The second self-triggering policy is to maximize the triggering time while still guaranteeing a pre-specified level of sub-<span class="search-hit mathjax">optimality</span>. Theoretical underpinnings are established for the computation and implementation of both policies. Through a gridworld numerical example, we illustrate the two policies&#39; <span class="search-hit mathjax">effectiveness</span> in <span class="search-hit mathjax">reducing</span> sources consumption and demonstrate the trade-offs between resource consumption and system <span class="search-hit mathjax">performance</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.08571v1-abstract-full').style.display = 'none'; document.getElementById('2102.08571v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.07959">arXiv:2102.07959</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.07959">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ReGraphX: NoC-enabled 3D Heterogeneous ReRAM Architecture for Training Graph Neural Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Arka%2C+A+I">Aqeeb Iqbal Arka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Joardar%2C+B+K">Biresh Kumar Joardar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Doppa%2C+J+R">Janardhan Rao Doppa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pande%2C+P+P">Partha Pratim Pande</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chakrabarty%2C+K">Krishnendu Chakrabarty</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.07959v1-abstract-short" style="display: inline;">
        &hellip;of Deep Neural Networks (DNNs) operating on graphs. However, GNNs are more complex compared to traditional DNNs as they simultaneously exhibit features of both DNN and graph <span class="search-hit mathjax">applications</span>. As a result, architectures specifically&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.07959v1-abstract-full').style.display = 'inline'; document.getElementById('2102.07959v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.07959v1-abstract-full" style="display: none;">
        Graph Neural Network (GNN) is a variant of Deep Neural Networks (DNNs) operating on graphs. However, GNNs are more complex compared to traditional DNNs as they simultaneously exhibit features of both DNN and graph <span class="search-hit mathjax">applications</span>. As a result, architectures specifically <span class="search-hit mathjax">optimized</span> for either DNNs or graph <span class="search-hit mathjax">applications</span> are not suited for GNN training. In this work, we propose a 3D heterogeneous manycore architecture for on-chip GNN training to address this problem. The proposed architecture, ReGraphX, involves heterogeneous ReRAM crossbars to fulfill the disparate requirements of both DNN and graph computations simultaneously. The ReRAM-based architecture is complemented with a multicast-enabled 3D NoC to <span class="search-hit mathjax">improve</span> the overall achievable <span class="search-hit mathjax">performance</span>. We demonstrate that ReGraphX outperforms conventional GPUs by up to 3.5X (on an average 3X) in terms of execution time, while <span class="search-hit mathjax">reducing</span> energy consumption by as much as 11X.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.07959v1-abstract-full').style.display = 'none'; document.getElementById('2102.07959v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been accepted and presented at Design Automation and Test in Europe (DATE) 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.07313">arXiv:2102.07313</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.07313">pdf</a>, <a href="https://arxiv.org/format/2102.07313">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Field Evaluations of A Deep Learning-based Intelligent Spraying Robot with Flow Control for Pear Orchards
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Seol%2C+J">Jaehwi Seol</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Jeongeun Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Son%2C+H+I">Hyoung Il Son</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.07313v1-abstract-short" style="display: inline;">
        &hellip;designed to examine the linear relationship of the flow rate modeling. Through a preliminary experiment, the parameters of the pulse width modulation (PWM) controller were <span class="search-hit mathjax">optimized</span>, and an actual field experiment was conducted to confirm the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.07313v1-abstract-full').style.display = 'inline'; document.getElementById('2102.07313v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.07313v1-abstract-full" style="display: none;">
        This paper proposes a variable flow control system in real time with deep learning using the segmentation of fruit trees in a pear orchard. The flow rate control in real time, undesired pressure fluctuation and theoretical modeling may differ from those in the real world. Therefore, two types of preliminary experiments were designed to examine the linear relationship of the flow rate modeling. Through a preliminary experiment, the parameters of the pulse width modulation (PWM) controller were <span class="search-hit mathjax">optimized</span>, and an actual field experiment was conducted to confirm the <span class="search-hit mathjax">performance</span> of the variable flow rate control system. As a result of the field experiment, the <span class="search-hit mathjax">performance</span> of the proposed system was satisfactory, as it showed that it could <span class="search-hit mathjax">reduce</span> pesticide use and the risk of pesticide exposure. Especially, since the field experiment was conducted in an unstructured environment, the proposed variable flow control system is expected to be sufficiently <span class="search-hit mathjax">applicable</span> to other orchards.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.07313v1-abstract-full').style.display = 'none'; document.getElementById('2102.07313v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 18 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.06311">arXiv:2102.06311</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.06311">pdf</a>, <a href="https://arxiv.org/ps/2102.06311">ps</a>, <a href="https://arxiv.org/format/2102.06311">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Does Culture Matter? Impact of Individualism and Uncertainty Avoidance on App Reviews
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fischer%2C+R+A">Ricarda Anna-Lena Fischer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Walczuch%2C+R">Rita Walczuch</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guzman%2C+E">Emitza Guzman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.06311v2-abstract-short" style="display: inline;">
        Mobile <span class="search-hit mathjax">applications</span> are often used by an international audience and therefore receive a high daily amount of user reviews from various countries. Previous work found evidence that app store reviews contain helpful information for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.06311v2-abstract-full').style.display = 'inline'; document.getElementById('2102.06311v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.06311v2-abstract-full" style="display: none;">
        Mobile <span class="search-hit mathjax">applications</span> are often used by an international audience and therefore receive a high daily amount of user reviews from various countries. Previous work found evidence that app store reviews contain helpful information for <span class="search-hit mathjax">software</span> evolution processes. However, the cultural diversity of the reviews and its consequences on specific user feedback characteristics has only been researched to a limited extent so far. In this paper, we examine the influence of two cultural dimensions, Individualism and Uncertainty Avoidance on user feedback in Apple app store reviews written in different languages. For this purpose, we collected 647,141 reviews from eight countries and written in five languages over a period of six months. We then used manual content analysis and <span class="search-hit mathjax">automated</span> processing to examine a sample of 3,120 reviews. The results show that there is a statistically significant influence of Individualism and Uncertainty Avoidance on user feedback characteristics. The results of this study will help researchers and practitioners to <span class="search-hit mathjax">reduce</span> algorithm bias caused by less diversified training and test data and to raise awareness of the importance of analyzing diversified user feedback.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.06311v2-abstract-full').style.display = 'none'; document.getElementById('2102.06311v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.05333">arXiv:2102.05333</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.05333">pdf</a>, <a href="https://arxiv.org/format/2102.05333">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intelligent Reflecting Surface-assisted MU-MISO Systems with Imperfect Hardware: Channel Estimation, Beamforming Design
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papazafeiropoulos%2C+A">Anastasios Papazafeiropoulos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+C">Cunhua Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kourtessis%2C+P">Pandelis Kourtessis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatzinotas%2C+S">Symeon Chatzinotas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Senior%2C+J+M">John M. Senior</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.05333v1-abstract-short" style="display: inline;">
        &hellip;IRS-assisted multi-user (MU) multiple-input single-output (MISO) system with imperfect CSI and correlated Rayleigh fading. In parallel, we present a general computationally <span class="search-hit mathjax">efficient</span> methodology for IRS reflect beamforming (RB)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.05333v1-abstract-full').style.display = 'inline'; document.getElementById('2102.05333v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.05333v1-abstract-full" style="display: none;">
        Most works in IRS-assisted systems have ignored the impact of the inevitable residual hardware impairments (HWIs) at both the transceiver hardware and the IRS while any relevant works have addressed only simple scenarios, e.g., with single-antenna network nodes and/or without taking the randomness of phase noise at the IRS into account. In this work, we aim at filling up this gap by considering a general IRS-assisted multi-user (MU) multiple-input single-output (MISO) system with imperfect CSI and correlated Rayleigh fading. In parallel, we present a general computationally <span class="search-hit mathjax">efficient</span> methodology for IRS reflect beamforming (RB) <span class="search-hit mathjax">optimization</span>. Specifically, we introduce an advantageous channel estimation (CE) method for such systems accounting for the HWIs. Moreover, we derive the uplink achievable spectral <span class="search-hit mathjax">efficiency</span> (SE) with maximal-ratio combining (MRC) receiver, displaying three significant advantages being: 1) its closed-form expression, 2) its dependence only on large-scale statistics, and 3) its low training overhead. Notably, by exploiting the first two benefits, we achieve to <span class="search-hit mathjax">perform</span> <span class="search-hit mathjax">optimization</span> with respect to the reflect beamforming matrix (RBM) that can take place only at every several coherence intervals, and thus, <span class="search-hit mathjax">reduces</span> significantly the computational cost compared to other methods which require frequent phase <span class="search-hit mathjax">optimization</span>. Among the insightful observations, we highlight that uncorrelated Rayleigh fading does not allow <span class="search-hit mathjax">optimization</span> of the SE, which makes the <span class="search-hit mathjax">application</span> of an IRS ineffective. Also, in the case that the phase drifts, describing the distortion of the phases in the RBM, are uniformly distributed, the presence of an IRS provides no advantage. The analytical results outperform previous works and are verified by Monte-Carlo (MC) simulations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.05333v1-abstract-full').style.display = 'none'; document.getElementById('2102.05333v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">30 pages, 5 figures, This work is submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.03877">arXiv:2102.03877</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.03877">pdf</a>, <a href="https://arxiv.org/format/2102.03877">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Noise Reduction in X-ray Photon Correlation Spectroscopy with Convolutional Neural Networks Encoder-Decoder Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Konstantinova%2C+T">Tatiana Konstantinova</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wiegart%2C+L">Lutz Wiegart</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rakitin%2C+M">Maksim Rakitin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=DeGennaro%2C+A+M">Anthony M. DeGennaro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barbour%2C+A+M">Andi M. Barbour</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.03877v1-abstract-short" style="display: inline;">
        &hellip;the intrinsic dynamics of a sample. Simultaneously addressing the disparate origins of noise in the experimental data is challenging. We propose a computational approach for <span class="search-hit mathjax">improving</span> the signal-to-noise ratio in two-time correlation functions that is based on Convolutional Neural Network Encoder-Decoder (CNN-ED) models. Such models extract features from an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.03877v1-abstract-full').style.display = 'inline'; document.getElementById('2102.03877v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.03877v1-abstract-full" style="display: none;">
        Like other experimental techniques, X-ray Photon Correlation Spectroscopy is a subject to various kinds of noise. Random and correlated fluctuations and heterogeneities can be present in a two-time correlation function and obscure the information about the intrinsic dynamics of a sample. Simultaneously addressing the disparate origins of noise in the experimental data is challenging. We propose a computational approach for <span class="search-hit mathjax">improving</span> the signal-to-noise ratio in two-time correlation functions that is based on Convolutional Neural Network Encoder-Decoder (CNN-ED) models. Such models extract features from an image via convolutional layers, project them to a low dimensional space and then reconstruct a clean image from this <span class="search-hit mathjax">reduced</span> representation via transposed convolutional layers. Not only are ED models a general tool for random noise removal, but their <span class="search-hit mathjax">application</span> to low signal-to-noise data can enhance the data quantitative usage since they are able to learn the functional form of the signal. We demonstrate that the CNN-ED models trained on real-world experimental data help to <span class="search-hit mathjax">effectively</span> extract equilibrium dynamics parameters from two-time correlation functions, containing statistical noise and dynamic heterogeneities. Strategies for <span class="search-hit mathjax">optimizing</span> the models <span class="search-hit mathjax">performance</span> and their <span class="search-hit mathjax">applicability</span> limits are discussed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.03877v1-abstract-full').style.display = 'none'; document.getElementById('2102.03877v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 10 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.03751">arXiv:2102.03751</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.03751">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DV-DVFS: Merging Data Variety and DVFS Technique to Manage the Energy Consumption of Big Data Processing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ahmadvand%2C+H">Hossein Ahmadvand</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Foroutan%2C+F">Fouzhan Foroutan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fathy%2C+M">Mahmood Fathy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.03751v1-abstract-short" style="display: inline;">
        &hellip;consumption. This issue has been overlooked in previous works. To overcome the mentioned problem, in the present work, we used Dynamic Voltage and Frequency Scaling (DVFS) to <span class="search-hit mathjax">reduce</span> the energy consumption of computation. To this goal, we consider two types of deadlines as our constraint. Before applying the DVFS technique to computer nodes, we estimate the p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.03751v1-abstract-full').style.display = 'inline'; document.getElementById('2102.03751v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.03751v1-abstract-full" style="display: none;">
        Data variety is one of the most important features of Big Data. Data variety is the result of aggregating data from multiple sources and uneven distribution of data. This feature of Big Data causes high variation in the consumption of processing resources such as CPU consumption. This issue has been overlooked in previous works. To overcome the mentioned problem, in the present work, we used Dynamic Voltage and Frequency Scaling (DVFS) to <span class="search-hit mathjax">reduce</span> the energy consumption of computation. To this goal, we consider two types of deadlines as our constraint. Before applying the DVFS technique to computer nodes, we estimate the processing time and the frequency needed to meet the deadline. In the evaluation phase, we have used a set of data sets and <span class="search-hit mathjax">applications</span>. The experimental results show that our proposed approach surpasses the other scenarios in processing real datasets. Based on the experimental results in this paper, DV-DVFS can achieve up to 15% <span class="search-hit mathjax">improvement</span> in energy consumption.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.03751v1-abstract-full').style.display = 'none'; document.getElementById('2102.03751v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.03236">arXiv:2102.03236</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.03236">pdf</a>, <a href="https://arxiv.org/format/2102.03236">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exact <span class="search-hit mathjax">Optimization</span> of Conformal Predictors via Incremental and Decremental Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cherubin%2C+G">Giovanni Cherubin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatzikokolakis%2C+K">Konstantinos Chatzikokolakis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jaggi%2C+M">Martin Jaggi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.03236v1-abstract-short" style="display: inline;">
        &hellip;They are suitable for a wide range of problems, from classification and regression to anomaly detection. Unfortunately, their high computational complexity limits their <span class="search-hit mathjax">applicability</span> to large datasets.
  In this work, we show that it is possible to speed up a CP classifier considerably, by studying it in conjunction with the underlying ML method, and by exp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.03236v1-abstract-full').style.display = 'inline'; document.getElementById('2102.03236v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.03236v1-abstract-full" style="display: none;">
        Conformal Predictors (CP) are wrappers around ML methods, providing error guarantees under weak assumptions on the data distribution. They are suitable for a wide range of problems, from classification and regression to anomaly detection. Unfortunately, their high computational complexity limits their <span class="search-hit mathjax">applicability</span> to large datasets.
  In this work, we show that it is possible to speed up a CP classifier considerably, by studying it in conjunction with the underlying ML method, and by exploiting incremental&amp;decremental learning. For methods such as k-NN, KDE, and kernel LS-SVM, our approach <span class="search-hit mathjax">reduces</span> the running time by one order of magnitude, whilst producing exact solutions. With similar ideas, we also achieve a linear speed up for the harder case of bootstrapping. Finally, we extend these techniques to <span class="search-hit mathjax">improve</span> upon an <span class="search-hit mathjax">optimization</span> of k-NN CP for regression.
  We evaluate our findings empirically, and discuss when methods are suitable for CP <span class="search-hit mathjax">optimization</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.03236v1-abstract-full').style.display = 'none'; document.getElementById('2102.03236v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.03012">arXiv:2102.03012</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.03012">pdf</a>, <a href="https://arxiv.org/format/2102.03012">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Serverless Cloud-Fog Platform for DNN-Based Video Analytics with Incremental Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Huaizheng Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+M">Meng Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Yizheng Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+Y">Yonggang Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+Y">Yong Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+G">Guanyu Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guan%2C+K">Kyle Guan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.03012v1-abstract-short" style="display: inline;">
        DNN-based video analytics have empowered many new <span class="search-hit mathjax">applications</span> (e.g.,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.03012v1-abstract-full').style.display = 'inline'; document.getElementById('2102.03012v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.03012v1-abstract-full" style="display: none;">
        DNN-based video analytics have empowered many new <span class="search-hit mathjax">applications</span> (e.g., <span class="search-hit mathjax">automated</span> retail). Meanwhile, the proliferation of fog devices provides developers with more design options to <span class="search-hit mathjax">improve</span> <span class="search-hit mathjax">performance</span> and save cost. To the best of our knowledge, this paper presents the first serverless system that takes full advantage of the client-fog-cloud synergy to better serve the DNN-based video analytics. Specifically, the system aims to achieve two goals: 1) Provide the <span class="search-hit mathjax">optimal</span> analytics results under the constraints of lower bandwidth usage and shorter round-trip time (RTT) by judiciously managing the computational and bandwidth resources deployed in the client, fog, and cloud environment. 2) Free developers from tedious administration and operation tasks, including DNN deployment, cloud and fog&#39;s resource management. To this end, we implement a holistic cloud-fog system referred to as VPaaS (Video-Platform-as-a-Service). VPaaS adopts serverless computing to enable developers to build a video analytics pipeline by simply <span class="search-hit mathjax">programming</span> a set of functions (e.g., model inference), which are then orchestrated to process videos through carefully designed modules. To save bandwidth and <span class="search-hit mathjax">reduce</span> RTT, VPaaS provides a new video streaming protocol that only sends low-quality video to the cloud. The state-of-the-art (SOTA) DNNs deployed at the cloud can identify regions of video frames that need further processing at the fog ends. At the fog ends, misidentified labels in these regions can be corrected using a light-weight DNN model. To address the data drift issues, we incorporate limited human feedback into the system to verify the results and adopt incremental learning to <span class="search-hit mathjax">improve</span> our system continuously. The evaluation demonstrates that VPaaS is superior to several SOTA systems: it maintains high <span class="search-hit mathjax">accuracy</span> while <span class="search-hit mathjax">reducing</span> bandwidth usage by up to 21%, RTT by up to 62.5%, and cloud monetary cost by up to 50%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.03012v1-abstract-full').style.display = 'none'; document.getElementById('2102.03012v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 16 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.02638">arXiv:2102.02638</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.02638">pdf</a>, <a href="https://arxiv.org/ps/2102.02638">ps</a>, <a href="https://arxiv.org/format/2102.02638">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Autodidactic Neurosurgeon: Collaborative Deep Inference for Mobile Edge Intelligence via Online Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Letian Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Lixing Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jie Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.02638v1-abstract-short" style="display: inline;">
        Recent breakthroughs in deep learning (DL) have led to the emergence of many intelligent mobile <span class="search-hit mathjax">applications</span> and services, but in the meanwhile also pose unprecedented computing challenges on resource-constrained mobile devices. This paper builds a collaborative deep inference system between a resource-constrained mobile device and a powerful edge server, ai&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.02638v1-abstract-full').style.display = 'inline'; document.getElementById('2102.02638v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.02638v1-abstract-full" style="display: none;">
        Recent breakthroughs in deep learning (DL) have led to the emergence of many intelligent mobile <span class="search-hit mathjax">applications</span> and services, but in the meanwhile also pose unprecedented computing challenges on resource-constrained mobile devices. This paper builds a collaborative deep inference system between a resource-constrained mobile device and a powerful edge server, aiming at joining the power of both on-device processing and computation offloading. The basic idea of this system is to partition a deep neural network (DNN) into a front-end part running on the mobile device and a back-end part running on the edge server, with the key challenge being how to locate the <span class="search-hit mathjax">optimal</span> partition point to minimize the end-to-end inference delay. Unlike existing efforts on DNN partitioning that rely heavily on a dedicated offline profiling stage to search for the <span class="search-hit mathjax">optimal</span> partition point, our system has a built-in online learning module, called Autodidactic Neurosurgeon (ANS), to <span class="search-hit mathjax">automatically</span> learn the <span class="search-hit mathjax">optimal</span> partition point on-the-fly. Therefore, ANS is able to closely follow the changes of the system environment by generating new knowledge for adaptive decision making. The core of ANS is a novel contextual bandit learning algorithm, called $μ$LinUCB, which not only has provable theoretical learning <span class="search-hit mathjax">performance</span> guarantee but also is ultra-lightweight for easy real-world implementation. We implement our system on a video stream object detection testbed to validate the design of ANS and evaluate its <span class="search-hit mathjax">performance</span>. The experiments show that ANS significantly outperforms state-of-the-art benchmarks in terms of tracking system changes and <span class="search-hit mathjax">reducing</span> the end-to-end inference delay.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.02638v1-abstract-full').style.display = 'none'; document.getElementById('2102.02638v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.02330">arXiv:2102.02330</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.02330">pdf</a>, <a href="https://arxiv.org/format/2102.02330">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1002/spe.2966">10.1002/spe.2966 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Function Delivery Network: Extending Serverless Computing for Heterogeneous Platforms
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jindal%2C+A">Anshul Jindal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gerndt%2C+M">Michael Gerndt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chadha%2C+M">Mohak Chadha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Podolskiy%2C+V">Vladimir Podolskiy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+P">Pengfei Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.02330v1-abstract-short" style="display: inline;">
        Serverless computing has rapidly grown following the launch of Amazon&#39;s Lambda platform. Function-as-a-Service (FaaS) a key enabler of serverless computing allows an <span class="search-hit mathjax">application</span> to be decomposed into simple, standalone functions that are executed on a FaaS platform. The FaaS platform is responsible for deploying and facilitating resources to the function&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.02330v1-abstract-full').style.display = 'inline'; document.getElementById('2102.02330v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.02330v1-abstract-full" style="display: none;">
        Serverless computing has rapidly grown following the launch of Amazon&#39;s Lambda platform. Function-as-a-Service (FaaS) a key enabler of serverless computing allows an <span class="search-hit mathjax">application</span> to be decomposed into simple, standalone functions that are executed on a FaaS platform. The FaaS platform is responsible for deploying and facilitating resources to the functions. Several of today&#39;s cloud <span class="search-hit mathjax">applications</span> spread over heterogeneous connected computing resources and are highly dynamic in their structure and resource requirements. However, FaaS platforms are limited to homogeneous clusters and homogeneous functions and do not account for the data access behavior of functions before scheduling.
  We introduce an extension of FaaS to heterogeneous clusters and to support heterogeneous functions through a network of distributed heterogeneous target platforms called Function Delivery Network (FDN). A target platform is a combination of a cluster of homogeneous nodes and a FaaS platform on top of it. FDN provides Function-Delivery-as-a-Service (FDaaS), delivering the function to the right target platform. We showcase the opportunities such as varied target platform&#39;s characteristics, possibility of collaborative execution between multiple target platforms, and localization of data that the FDN offers in fulfilling two objectives: Service Level Objective (SLO) requirements and energy <span class="search-hit mathjax">efficiency</span> when scheduling functions by evaluating over five distributed target platforms using the FDNInspector, a tool developed by us for benchmarking distributed target platforms. Scheduling functions on an edge target platform in our evaluation <span class="search-hit mathjax">reduced</span> the overall energy consumption by 17x without violating the SLO requirements in comparison to scheduling on a high-end target platform.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.02330v1-abstract-full').style.display = 'none'; document.getElementById('2102.02330v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Journal of <span class="search-hit mathjax">Software</span>: Practice and Experience</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.01256">arXiv:2102.01256</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.01256">pdf</a>, <a href="https://arxiv.org/format/2102.01256">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Atlas-aware ConvNetfor Accurate yet Robust Anatomical Segmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+Y">Yuan Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+W">Weinan Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jiawei Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+L">Liang Qiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+K">Kun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+L">Lei He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.01256v1-abstract-short" style="display: inline;">
        Convolutional networks (ConvNets) have achieved promising <span class="search-hit mathjax">accuracy</span> for various anatomical segmentation tasks. Despite the success, these methods can be sensitive to data appearance variations. Considering the large variability of scans caused by artifacts, pathologies, and scanning setups, robust ConvNets are vital for clinical&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.01256v1-abstract-full').style.display = 'inline'; document.getElementById('2102.01256v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.01256v1-abstract-full" style="display: none;">
        Convolutional networks (ConvNets) have achieved promising <span class="search-hit mathjax">accuracy</span> for various anatomical segmentation tasks. Despite the success, these methods can be sensitive to data appearance variations. Considering the large variability of scans caused by artifacts, pathologies, and scanning setups, robust ConvNets are vital for clinical <span class="search-hit mathjax">applications</span>, while have not been fully explored. In this paper, we propose to mitigate the challenge by enabling ConvNets&#39; awareness of the underlying anatomical invariances among imaging scans. Specifically, we introduce a fully convolutional Constraint Adoption Module (CAM) that incorporates probabilistic atlas priors as explicit constraints for predictions over a locally connected Conditional Random Field (CFR), which <span class="search-hit mathjax">effectively</span> reinforces the anatomical consistency of the labeling outputs. We design the CAM to be flexible for boosting various ConvNet, and compact for co-<span class="search-hit mathjax">optimizing</span> with ConvNets for fusion parameters that leads to the <span class="search-hit mathjax">optimal</span> <span class="search-hit mathjax">performance</span>. We show the advantage of such atlas priors fusion is two-fold with two brain parcellation tasks. First, our models achieve state-of-the-art <span class="search-hit mathjax">accuracy</span> among ConvNet-based methods on both datasets, by significantly <span class="search-hit mathjax">reducing</span> structural abnormalities of predictions. Second, we can largely boost the robustness of existing ConvNets, proved by: (i) testing on scans with synthetic pathologies, and (ii) training and evaluation on scans of different scanning setups across datasets. Our method is proposing to be easily adopted to existing ConvNets by fine-tuning with CAM plugged in for <span class="search-hit mathjax">accuracy</span> and robustness boosts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.01256v1-abstract-full').style.display = 'none'; document.getElementById('2102.01256v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.00755">arXiv:2102.00755</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.00755">pdf</a>, <a href="https://arxiv.org/format/2102.00755">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        High Resolution 3D Ultrasonic Breast Imaging by Time-Domain Full Waveform Inversion
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lucka%2C+F">Felix Lucka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=P%C3%A9rez-Liva%2C+M">Mailyn Pérez-Liva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Treeby%2C+B+E">Bradley E. Treeby</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cox%2C+B+T">Ben T. Cox</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.00755v3-abstract-short" style="display: inline;">
        Ultrasound tomography (UST) scanners allow quantitative images of the human breast&#39;s acoustic properties to be derived with potential <span class="search-hit mathjax">applications</span> in screening, diagnosis and therapy planning. Time domain full waveform inversion (TD-FWI) is a promising UST image formation technique that fits the parameter fields of a wave physics model by gradient-based&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.00755v3-abstract-full').style.display = 'inline'; document.getElementById('2102.00755v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.00755v3-abstract-full" style="display: none;">
        Ultrasound tomography (UST) scanners allow quantitative images of the human breast&#39;s acoustic properties to be derived with potential <span class="search-hit mathjax">applications</span> in screening, diagnosis and therapy planning. Time domain full waveform inversion (TD-FWI) is a promising UST image formation technique that fits the parameter fields of a wave physics model by gradient-based <span class="search-hit mathjax">optimization</span>. For high resolution 3D UST, it holds three key challenges: Firstly, its central building block, the computation of the gradient for a single US measurement, has a restrictively large memory footprint. Secondly, this building block needs to be computed for each of the $10^3-10^4$ measurements, resulting in a massive parallel computation usually <span class="search-hit mathjax">performed</span> on large computational clusters for days. Lastly, the structure of the underlying <span class="search-hit mathjax">optimization</span> problem may result in slow progression of the solver and convergence to a local minimum. In this work, we design and evaluate a comprehensive computational strategy to overcome these challenges: Firstly, we introduce a novel gradient computation based on time reversal that dramatically <span class="search-hit mathjax">reduces</span> the memory footprint at the expense of one additional wave simulation per source. Secondly, we break the dependence on the number of measurements by using source encoding (SE) to compute stochastic gradient estimates. Also we describe a more accurate, TD-specific SE technique with a finer variance control and use a state-of-the-art stochastic LBFGS method. Lastly, we design an <span class="search-hit mathjax">efficient</span> TD multi-grid scheme together with preconditioning to speed up the convergence while avoiding local minima. All components are evaluated in extensive numerical proof-of-concept studies simulating a bowl-shaped 3D UST breast scanner prototype. Finally, we demonstrate that their combination allows us to obtain an accurate 442x442x222 voxel image with a resolution of 0.5mm using Matlab on a single GPU within 24h.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.00755v3-abstract-full').style.display = 'none'; document.getElementById('2102.00755v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.00390">arXiv:2102.00390</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.00390">pdf</a>, <a href="https://arxiv.org/format/2102.00390">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AACP: Model Compression by Accurate and <span class="search-hit mathjax">Automatic</span> Channel Pruning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+L">Lanbo Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yujiu Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Z">Zhenhua Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.00390v1-abstract-short" style="display: inline;">
        &hellip;is formulated as a neural architecture search (NAS) problem recently. However, existing NAS-based methods are challenged by huge computational cost and inflexibility of <span class="search-hit mathjax">applications</span>. How to deal with multiple sparsity constraints simultaneously and speed up NAS-based channel pruning are still open challenges. In this paper, we propose a novel Accurate and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.00390v1-abstract-full').style.display = 'inline'; document.getElementById('2102.00390v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.00390v1-abstract-full" style="display: none;">
        Channel pruning is formulated as a neural architecture search (NAS) problem recently. However, existing NAS-based methods are challenged by huge computational cost and inflexibility of <span class="search-hit mathjax">applications</span>. How to deal with multiple sparsity constraints simultaneously and speed up NAS-based channel pruning are still open challenges. In this paper, we propose a novel Accurate and <span class="search-hit mathjax">Automatic</span> Channel Pruning (AACP) method to address these problems. Firstly, AACP represents the structure of a model as a structure vector and introduces a pruning step vector to control the compressing granularity of each layer. Secondly, AACP utilizes Pruned Structure <span class="search-hit mathjax">Accuracy</span> Estimator (PSAE) to speed up the <span class="search-hit mathjax">performance</span> estimation process. Thirdly, AACP proposes <span class="search-hit mathjax">Improved</span> Differential Evolution (IDE) algorithm to search the <span class="search-hit mathjax">optimal</span> structure vector <span class="search-hit mathjax">effectively</span>. Because of IDE, AACP can deal with FLOPs constraint and model size constraint simultaneously and <span class="search-hit mathjax">efficiently</span>. Our method can be easily applied to various tasks and achieve state of the art <span class="search-hit mathjax">performance</span>. On CIFAR10, our method <span class="search-hit mathjax">reduces</span> $65\%$ FLOPs of ResNet110 with an <span class="search-hit mathjax">improvement</span> of $0.26\%$ top-1 <span class="search-hit mathjax">accuracy</span>. On ImageNet, we <span class="search-hit mathjax">reduce</span> $42\%$ FLOPs of ResNet50 with a small loss of $0.18\%$ top-1 <span class="search-hit mathjax">accuracy</span> and <span class="search-hit mathjax">reduce</span> $30\%$ FLOPs of MobileNetV2 with a small loss of $0.7\%$ top-1 <span class="search-hit mathjax">accuracy</span>. The source <span class="search-hit mathjax">code</span> will be released after publication.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.00390v1-abstract-full').style.display = 'none'; document.getElementById('2102.00390v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.00092">arXiv:2102.00092</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.00092">pdf</a>, <a href="https://arxiv.org/format/2102.00092">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Can Machine Learning Help in Solving Cargo Capacity Management Booking Control Problems?
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dumouchelle%2C+J">Justin Dumouchelle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frejinger%2C+E">Emma Frejinger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lodi%2C+A">Andrea Lodi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.00092v1-abstract-short" style="display: inline;">
        &hellip;(e.g., airlines and railroads). In this paper, we focus on cargo capacity management which has received less attention in the literature than its passenger counterpart. More <span class="search-hit mathjax">precisely</span>, we focus on the problem of controlling booking accept/reject decisions: Given a limited capacity, accept a booking request or reject it to reserve capacity for future bookings&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.00092v1-abstract-full').style.display = 'inline'; document.getElementById('2102.00092v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.00092v1-abstract-full" style="display: none;">
        Revenue management is important for carriers (e.g., airlines and railroads). In this paper, we focus on cargo capacity management which has received less attention in the literature than its passenger counterpart. More <span class="search-hit mathjax">precisely</span>, we focus on the problem of controlling booking accept/reject decisions: Given a limited capacity, accept a booking request or reject it to reserve capacity for future bookings with potentially higher revenue. We formulate the problem as a finite-horizon stochastic dynamic <span class="search-hit mathjax">program</span>. The cost of fulfilling the accepted bookings, incurred at the end of the horizon, depends on the packing and routing of the cargo. This is a computationally challenging aspect as the latter are solutions to an operational decision-making problem, in our <span class="search-hit mathjax">application</span> a vehicle routing problem (VRP). Seeking a balance between online and offline computation, we propose to train a predictor of the solution costs to the VRPs using supervised learning. In turn, we use the predictions online in approximate dynamic <span class="search-hit mathjax">programming</span> and reinforcement learning algorithms to solve the booking control problem. We compare the results to an existing approach in the literature and show that we are able to obtain control policies that provide increased profit at a <span class="search-hit mathjax">reduced</span> evaluation time. This is achieved thanks to accurate approximation of the operational costs and negligible computing time in comparison to solving the VRPs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.00092v1-abstract-full').style.display = 'none'; document.getElementById('2102.00092v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.11118">arXiv:2101.11118</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.11118">pdf</a>, <a href="https://arxiv.org/format/2101.11118">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Can Offline Testing of Deep Neural Networks Replace Their Online Testing?
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Haq%2C+F+U">Fitash Ul Haq</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shin%2C+D">Donghwan Shin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nejati%2C+S">Shiva Nejati</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briand%2C+L">Lionel Briand</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.11118v2-abstract-short" style="display: inline;">
        &hellip;where DNNs are tested as individual units based on test datasets obtained without involving the DNNs under test, and online testing where DNNs are embedded into a specific <span class="search-hit mathjax">application</span> environment and tested in a closed-loop mode in interaction with the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.11118v2-abstract-full').style.display = 'inline'; document.getElementById('2101.11118v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.11118v2-abstract-full" style="display: none;">
        We distinguish two general modes of testing for Deep Neural Networks (DNNs): Offline testing where DNNs are tested as individual units based on test datasets obtained without involving the DNNs under test, and online testing where DNNs are embedded into a specific <span class="search-hit mathjax">application</span> environment and tested in a closed-loop mode in interaction with the <span class="search-hit mathjax">application</span> environment. Typically, DNNs are subjected to both types of testing during their development life cycle where offline testing is applied immediately after DNN training and online testing follows after offline testing and once a DNN is deployed within a specific <span class="search-hit mathjax">application</span> environment. In this paper, we study the relationship between offline and online testing. Our goal is to determine how offline testing and online testing differ or complement one another and if offline testing results can be used to help <span class="search-hit mathjax">reduce</span> the cost of online testing? Though these questions are generally relevant to all autonomous systems, we study them in the context of <span class="search-hit mathjax">automated</span> driving systems where, as study subjects, we use DNNs <span class="search-hit mathjax">automating</span> end-to-end controls of steering functions of self-driving vehicles. Our results show that offline testing is less <span class="search-hit mathjax">effective</span> than online testing as many safety violations identified by online testing could not be identified by offline testing, while large prediction errors generated by offline testing always led to severe safety violations detectable by online testing. Further, we cannot exploit offline testing results to <span class="search-hit mathjax">reduce</span> the cost of online testing in practice since we are not able to identify specific situations where offline testing could be as accurate as online testing in identifying safety requirement violations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.11118v2-abstract-full').style.display = 'none'; document.getElementById('2101.11118v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Journal extension of arXiv:1912.00805; To appear in Empirical <span class="search-hit mathjax">Software</span> Engineering</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.10453">arXiv:2101.10453</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.10453">pdf</a>, <a href="https://arxiv.org/format/2101.10453">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.cosrev.2020.100342">10.1016/j.cosrev.2020.100342 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Nature-Inspired Algorithms for Wireless Sensor Networks: A Comprehensive Survey
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+A">Abhilash Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+S">Sandeep Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+J">Jitenda Singh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.10453v1-abstract-short" style="display: inline;">
        &hellip;solve the critical issues in Wireless Sensor Networks (WSNs), with concern for limited sensor lifetime, nature-inspired algorithms are emerging as a suitable method. Getting <span class="search-hit mathjax">optimal</span> network coverage is one of those challenging issues that need to be examined critically before any network setup.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.10453v1-abstract-full').style.display = 'inline'; document.getElementById('2101.10453v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.10453v1-abstract-full" style="display: none;">
        In order to solve the critical issues in Wireless Sensor Networks (WSNs), with concern for limited sensor lifetime, nature-inspired algorithms are emerging as a suitable method. Getting <span class="search-hit mathjax">optimal</span> network coverage is one of those challenging issues that need to be examined critically before any network setup. <span class="search-hit mathjax">Optimal</span> network coverage not only minimizes the consumption of limited energy of battery-driven sensors but also <span class="search-hit mathjax">reduce</span> the sensing of redundant information. In this paper, we focus on nature-inspired <span class="search-hit mathjax">optimization</span> algorithms concerning the <span class="search-hit mathjax">optimal</span> coverage in WSNs. In the first half of the paper, we have briefly discussed the taxonomy of the <span class="search-hit mathjax">optimization</span> algorithms along with the problem domains in WSNs. In the second half of the paper, we have compared the <span class="search-hit mathjax">performance</span> of two nature-inspired algorithms for getting <span class="search-hit mathjax">optimal</span> coverage in WSNs. The first one is a combined <span class="search-hit mathjax">Improved</span> Genetic Algorithm and Binary Ant Colony Algorithm (IGABACA), and the second one is Lion <span class="search-hit mathjax">Optimization</span> (LO). The simulation results confirm that LO gives better network coverage, and the convergence rate of LO is faster than that of IGA-BACA. Further, we observed that the <span class="search-hit mathjax">optimal</span> coverage is achieved at a lesser number of generations in LO as compared to IGA-BACA. This review will help researchers to explore the <span class="search-hit mathjax">applications</span> in this field as well as beyond this area. Keywords: <span class="search-hit mathjax">Optimal</span> Coverage, Bio-inspired Algorithm, Lion <span class="search-hit mathjax">Optimization</span>, WSNs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.10453v1-abstract-full').style.display = 'none'; document.getElementById('2101.10453v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Computer Science Review (2020) 100342
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.10357">arXiv:2101.10357</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.10357">pdf</a>, <a href="https://arxiv.org/ps/2101.10357">ps</a>, <a href="https://arxiv.org/format/2101.10357">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Regret-<span class="search-hit mathjax">Optimal</span> Filtering
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sabag%2C+O">Oron Sabag</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hassibi%2C+B">Babak Hassibi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.10357v1-abstract-short" style="display: inline;">
        We consider the problem of filtering in linear state-space models (e.g., the Kalman filter setting) through the lens of regret <span class="search-hit mathjax">optimization</span>. Different assumptions on the driving disturbance and the observation noise sequences give rise to different estimators: in the stochastic setting to the celebrated Kalman filter, and in the deterministic setting of boun&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.10357v1-abstract-full').style.display = 'inline'; document.getElementById('2101.10357v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.10357v1-abstract-full" style="display: none;">
        We consider the problem of filtering in linear state-space models (e.g., the Kalman filter setting) through the lens of regret <span class="search-hit mathjax">optimization</span>. Different assumptions on the driving disturbance and the observation noise sequences give rise to different estimators: in the stochastic setting to the celebrated Kalman filter, and in the deterministic setting of bounded energy disturbances to $H_\infty$ estimators. In this work, we formulate a novel criterion for filter design based on the concept of regret between the estimation error energy of a clairvoyant estimator that has access to all future observations (a so-called smoother) and a causal one that only has access to current and past observations. The regret-<span class="search-hit mathjax">optimal</span> estimator is chosen to minimize this worst-case difference across all bounded-energy noise sequences. The resulting estimator is adaptive in the sense that it aims to mimic the behavior of the clairvoyant estimator, irrespective of what the realization of the noise will be and thus interpolates between the stochastic and deterministic approaches. We provide a solution for the regret estimation problem at two different levels. First, we provide a solution at the operator level by <span class="search-hit mathjax">reducing</span> it to the Nehari problem. Second, for state-space models, we explicitly find the estimator that achieves the <span class="search-hit mathjax">optimal</span> regret. From a computational perspective, the regret-<span class="search-hit mathjax">optimal</span> estimator can be easily implemented by solving three Riccati equations and a single Lyapunov equation. For a state-space model of dimension $n$, the regret-<span class="search-hit mathjax">optimal</span> estimator has a state-space structure of dimension $3n$. We demonstrate the <span class="search-hit mathjax">applicability</span> and efficacy of the estimator in a variety of problems and observe that the estimator has average and worst-case <span class="search-hit mathjax">performances</span> that are simultaneously close to their <span class="search-hit mathjax">optimal</span> values. We therefore argue that regret-<span class="search-hit mathjax">optimality</span> is a viable approach to estimator design.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.10357v1-abstract-full').style.display = 'none'; document.getElementById('2101.10357v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to AISTATS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.10209">arXiv:2101.10209</a>
        <span>&nbsp;&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Novel Dynamic Load Balancing Algorithm for Cloud-Based Big Data Analytics
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Aghdashi%2C+A">Arman Aghdashi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mirtaheri%2C+S+L">Seyedeh Leili Mirtaheri</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.10209v2-abstract-short" style="display: inline;">
        Big data analytics in cloud environments introduces challenges such as real-time load balancing besides security, privacy, and energy <span class="search-hit mathjax">efficiency</span>. In this paper, we propose a novel load balancing algorithm in cloud environments that&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.10209v2-abstract-full').style.display = 'inline'; document.getElementById('2101.10209v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.10209v2-abstract-full" style="display: none;">
        Big data analytics in cloud environments introduces challenges such as real-time load balancing besides security, privacy, and energy <span class="search-hit mathjax">efficiency</span>. In this paper, we propose a novel load balancing algorithm in cloud environments that <span class="search-hit mathjax">performs</span> resource allocation and task scheduling <span class="search-hit mathjax">efficiently</span>. The proposed load balancer <span class="search-hit mathjax">reduces</span> the execution response time in big data <span class="search-hit mathjax">applications</span> <span class="search-hit mathjax">performed</span> on clouds. Scheduling, in general, is an NP-hard problem. In our proposed algorithm, we provide solutions to <span class="search-hit mathjax">reduce</span> the search area that leads to <span class="search-hit mathjax">reduced</span> complexity of the load balancing. We recommend two mathematical <span class="search-hit mathjax">optimization</span> models to <span class="search-hit mathjax">perform</span> dynamic resource allocation to virtual machines and task scheduling. The provided solution is based on the hill-climbing algorithm to minimize response time. We evaluate the <span class="search-hit mathjax">performance</span> of proposed algorithms in terms of response time, turnaround time, throughput metrics, and request distribution with some of the existing algorithms that show significant <span class="search-hit mathjax">improvements</span>
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.10209v2-abstract-full').style.display = 'none'; document.getElementById('2101.10209v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 February, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">My supervisor has requested that i withdraw the article</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.09796">arXiv:2101.09796</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.09796">pdf</a>, <a href="https://arxiv.org/format/2101.09796">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Ifs and Buts of the Development Approaches for IoT <span class="search-hit mathjax">Applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Agudelo-Sanabria%2C+S+D">Saitel Daniela Agudelo-Sanabria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jindal%2C+A">Anshul Jindal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.09796v1-abstract-short" style="display: inline;">
        The recent growth of the Internet of Things (IoT) devices has lead to the rise of various complex <span class="search-hit mathjax">applications</span> where these&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.09796v1-abstract-full').style.display = 'inline'; document.getElementById('2101.09796v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.09796v1-abstract-full" style="display: none;">
        The recent growth of the Internet of Things (IoT) devices has lead to the rise of various complex <span class="search-hit mathjax">applications</span> where these <span class="search-hit mathjax">applications</span> involve interactions among large numbers of heterogeneous devices. An important challenge that needs to be addressed is to facilitate the agile development of IoT <span class="search-hit mathjax">applications</span> with minimal effort by the various parties involved in the process. However, IoT <span class="search-hit mathjax">application</span> development is challenging due to the wide variety of hardware and <span class="search-hit mathjax">software</span> technologies that interact in an IoT system. Moreover, it involves dealing with issues that are attributed to different <span class="search-hit mathjax">software</span> life-cycle phases: development, deployment, and progression.
  In this paper, we examine three IoT <span class="search-hit mathjax">application</span> development approaches: Mashup-based development, Model-based development, and Function-as-a-Service based development. The advantages and disadvantages of each approach are discussed from different perspectives, including reliability, deployment expeditiousness, ease of use, and targeted audience. Finally, we propose a simple solution where these techniques are combined to deliver reliable <span class="search-hit mathjax">applications</span> while <span class="search-hit mathjax">reducing</span> costs and time to release.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.09796v1-abstract-full').style.display = 'none'; document.getElementById('2101.09796v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.09671">arXiv:2101.09671</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.09671">pdf</a>, <a href="https://arxiv.org/format/2101.09671">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Pruning and Quantization for Deep Neural Network Acceleration: A Survey
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+T">Tailin Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Glossner%2C+J">John Glossner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Lei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+S">Shaobo Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiaotong Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.09671v3-abstract-short" style="display: inline;">
        Deep neural networks have been applied in many <span class="search-hit mathjax">applications</span> exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.09671v3-abstract-full').style.display = 'inline'; document.getElementById('2101.09671v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.09671v3-abstract-full" style="display: none;">
        Deep neural networks have been applied in many <span class="search-hit mathjax">applications</span> exhibiting extraordinary abilities in the field of computer vision. However, complex network architectures challenge <span class="search-hit mathjax">efficient</span> real-time deployment and require significant computation resources and energy costs. These challenges can be overcome through <span class="search-hit mathjax">optimizations</span> such as network compression. Network compression can often be realized with little loss of <span class="search-hit mathjax">accuracy</span>. In some cases <span class="search-hit mathjax">accuracy</span> may even <span class="search-hit mathjax">improve</span>. This paper provides a survey on two types of network compression: pruning and quantization. Pruning can be categorized as static if it is <span class="search-hit mathjax">performed</span> offline or dynamic if it is <span class="search-hit mathjax">performed</span> at run-time. We compare pruning techniques and describe criteria used to remove redundant computations. We discuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise, layer-wise and even network-wise pruning. Quantization <span class="search-hit mathjax">reduces</span> computations by <span class="search-hit mathjax">reducing</span> the <span class="search-hit mathjax">precision</span> of the datatype. Weights, biases, and activations may be quantized typically to 8-bit integers although lower bit width implementations are also discussed including binary neural networks. Both pruning and quantization can be used independently or combined. We compare current techniques, analyze their strengths and weaknesses, present compressed network <span class="search-hit mathjax">accuracy</span> results on a number of frameworks, and provide practical guidance for compressing networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.09671v3-abstract-full').style.display = 'none'; document.getElementById('2101.09671v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.09359">arXiv:2101.09359</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.09359">pdf</a>, <a href="https://arxiv.org/format/2101.09359">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Load-Balancing for <span class="search-hit mathjax">Improving</span> User Responsiveness on Multicore Embedded Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lim%2C+G">Geunsik Lim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Min%2C+C">Changwoo Min</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Eom%2C+Y">YoungIk Eom</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.09359v1-abstract-short" style="display: inline;">
        Most commercial embedded devices have been deployed with a single processor architecture. The <span class="search-hit mathjax">code</span> size and complexity of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.09359v1-abstract-full').style.display = 'inline'; document.getElementById('2101.09359v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.09359v1-abstract-full" style="display: none;">
        Most commercial embedded devices have been deployed with a single processor architecture. The <span class="search-hit mathjax">code</span> size and complexity of <span class="search-hit mathjax">applications</span> running on embedded devices are rapidly increasing due to the emergence of <span class="search-hit mathjax">application</span> business models such as Google Play Store and Apple App Store. As a result, a high-<span class="search-hit mathjax">performance</span> multicore CPUs have become a major trend in the embedded market as well as in the personal computer market. Due to this trend, many device manufacturers have been able to adopt more attractive user interfaces and high-<span class="search-hit mathjax">performance</span> <span class="search-hit mathjax">applications</span> for better user experiences on the multicore systems. In this paper, we describe how to <span class="search-hit mathjax">improve</span> the real-time <span class="search-hit mathjax">performance</span> by <span class="search-hit mathjax">reducing</span> the user waiting time on multicore systems that use a partitioned per-CPU run queue scheduling technique. Rather than focusing on naive load-balancing scheme for equally balanced CPU usage, our approach tries to minimize the cost of task migration by considering the importance level of running tasks and to <span class="search-hit mathjax">optimize</span> per-CPU utilization on multicore embedded systems. Consequently, our approach <span class="search-hit mathjax">improves</span> the real-time characteristics such as cache <span class="search-hit mathjax">efficiency</span>, user responsiveness, and latency. Experimental results under heavy background stress show that our approach <span class="search-hit mathjax">reduces</span> the average scheduling latency of an urgent task by 2.3 times.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.09359v1-abstract-full').style.display = 'none'; document.getElementById('2101.09359v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.09194">arXiv:2101.09194</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.09194">pdf</a>, <a href="https://arxiv.org/format/2101.09194">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        It Takes Two to Tango: Combining Visual and Textual Information for Detecting Duplicate Video-Based Bug Reports
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cooper%2C+N">Nathan Cooper</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bernal-C%C3%A1rdenas%2C+C">Carlos Bernal-Cárdenas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chaparro%2C+O">Oscar Chaparro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moran%2C+K">Kevin Moran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Poshyvanyk%2C+D">Denys Poshyvanyk</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.09194v2-abstract-short" style="display: inline;">
        When a bug manifests in a user-facing <span class="search-hit mathjax">application</span>, it is likely to be exposed through the graphical user interface (GUI). Given the importance of visual information to the process of identifying and understanding such bugs, users are increasingly making use of screenshots and screen-recordings as a means to report issues to developers. However, when such inf&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.09194v2-abstract-full').style.display = 'inline'; document.getElementById('2101.09194v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.09194v2-abstract-full" style="display: none;">
        When a bug manifests in a user-facing <span class="search-hit mathjax">application</span>, it is likely to be exposed through the graphical user interface (GUI). Given the importance of visual information to the process of identifying and understanding such bugs, users are increasingly making use of screenshots and screen-recordings as a means to report issues to developers. However, when such information is reported en masse, such as during crowd-sourced testing, managing these artifacts can be a time-consuming process. As the reporting of screen-recordings in particular becomes more popular, developers are likely to face challenges related to manually identifying videos that depict duplicate bugs. Due to their graphical nature, screen-recordings present challenges for <span class="search-hit mathjax">automated</span> analysis that preclude the use of current duplicate bug report detection techniques. To overcome these challenges and aid developers in this task, this paper presents Tango, a duplicate detection technique that operates purely on video-based bug reports by leveraging both visual and textual information. Tango combines tailored computer vision techniques, optical character recognition, and text retrieval. We evaluated multiple configurations of Tango in a comprehensive empirical evaluation on 4,860 duplicate detection tasks that involved a total of 180 screen-recordings from six Android apps. Additionally, we conducted a user study investigating the effort required for developers to manually detect duplicate video-based bug reports and compared this to the effort required to use Tango. The results reveal that Tango&#39;s <span class="search-hit mathjax">optimal</span> configuration is highly <span class="search-hit mathjax">effective</span> at detecting duplicate video-based bug reports, accurately ranking target duplicate videos in the top-2 returned results in 83% of the tasks. Additionally, our user study shows that, on average, Tango can <span class="search-hit mathjax">reduce</span> developer effort by over 60%, illustrating its practicality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.09194v2-abstract-full').style.display = 'none'; document.getElementById('2101.09194v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 February, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages and 1 figure. Published at ICSE&#39;21</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.08763">arXiv:2101.08763</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.08763">pdf</a>, <a href="https://arxiv.org/ps/2101.08763">ps</a>, <a href="https://arxiv.org/format/2101.08763">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GPU-Accelerated <span class="search-hit mathjax">Optimizer</span>-Aware Evaluation of Submodular Exemplar Clustering
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Honysz%2C+P">Philipp-Jan Honysz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buschj%C3%A4ger%2C+S">Sebastian Buschjäger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Morik%2C+K">Katharina Morik</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.08763v1-abstract-short" style="display: inline;">
        The <span class="search-hit mathjax">optimization</span> of submodular functions constitutes a viable way to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.08763v1-abstract-full').style.display = 'inline'; document.getElementById('2101.08763v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.08763v1-abstract-full" style="display: none;">
        The <span class="search-hit mathjax">optimization</span> of submodular functions constitutes a viable way to <span class="search-hit mathjax">perform</span> clustering. Strong approximation guarantees and feasible <span class="search-hit mathjax">optimization</span> w.r.t. streaming data make this clustering approach favorable. Technically, submodular functions map subsets of data to real values, which indicate how &#34;representative&#34; a specific subset is. <span class="search-hit mathjax">Optimal</span> sets might then be used to partition the data space and to infer clusters. Exemplar-based clustering is one of the possible submodular functions, but suffers from high computational complexity. However, for practical <span class="search-hit mathjax">applications</span>, the particular real-time or wall-clock run-time is decisive. In this work, we present a novel way to evaluate this particular function on GPUs, which keeps the necessities of <span class="search-hit mathjax">optimizers</span> in mind and <span class="search-hit mathjax">reduces</span> wall-clock run-time. To discuss our GPU algorithm, we investigated both the impact of different run-time critical problem properties, like data dimensionality and the number of data points in a subset, and the influence of required floating-point <span class="search-hit mathjax">precision</span>. In reproducible experiments, our GPU algorithm was able to achieve competitive speedups of up to 72x depending on whether multi-threaded computation on CPUs was used for comparison and the type of floating-point <span class="search-hit mathjax">precision</span> required. Half-<span class="search-hit mathjax">precision</span> GPU computation led to large speedups of up to 452x compared to single-<span class="search-hit mathjax">precision</span>, single-thread CPU computations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.08763v1-abstract-full').style.display = 'none'; document.getElementById('2101.08763v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.08458">arXiv:2101.08458</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.08458">pdf</a>, <a href="https://arxiv.org/format/2101.08458">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/CGO51591.2021.9370330">10.1109/CGO51591.2021.9370330 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        UNIT: Unifying Tensorized Instruction Compilation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Weng%2C+J">Jian Weng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jain%2C+A">Animesh Jain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jie Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Leyuan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yida Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nowatzki%2C+T">Tony Nowatzki</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.08458v3-abstract-short" style="display: inline;">
        Because of the increasing demand for computation in DNN, researchers develope both hardware and <span class="search-hit mathjax">software</span> mechanisms to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.08458v3-abstract-full').style.display = 'inline'; document.getElementById('2101.08458v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.08458v3-abstract-full" style="display: none;">
        Because of the increasing demand for computation in DNN, researchers develope both hardware and <span class="search-hit mathjax">software</span> mechanisms to <span class="search-hit mathjax">reduce</span> the compute and memory burden. A widely adopted approach is to use mixed <span class="search-hit mathjax">precision</span> data types. However, it is hard to leverage mixed <span class="search-hit mathjax">precision</span> without hardware support because of the overhead of data casting. Hardware vendors offer tensorized instructions for mixed-<span class="search-hit mathjax">precision</span> tensor operations, like Intel VNNI, Tensor Core, and ARM-DOT. These instructions involve a computing idiom that <span class="search-hit mathjax">reduces</span> multiple low <span class="search-hit mathjax">precision</span> elements into one high <span class="search-hit mathjax">precision</span> element. The lack of compilation techniques for this makes it hard to utilize these instructions: Using vendor-provided libraries for computationally-intensive kernels is inflexible and prevents further <span class="search-hit mathjax">optimizations</span>, and manually writing hardware intrinsics is error-prone and difficult for programmers. Some prior works address this problem by creating compilers for each instruction. This requires excessive effort when it comes to many tensorized instructions. In this work, we develop a compiler framework to unify the compilation for these instructions -- a unified semantics abstraction eases the integration of new instructions, and reuses the analysis and transformations. Tensorized instructions from different platforms can be compiled via UNIT with moderate effort for favorable <span class="search-hit mathjax">performance</span>. Given a tensorized instruction and a tensor operation, UNIT <span class="search-hit mathjax">automatically</span> detects the <span class="search-hit mathjax">applicability</span>, transforms the loop organization of the operation,and rewrites the loop body to leverage the tensorized instruction. According to our evaluation, UNIT can target various mainstream hardware platforms. The generated end-to-end inference model achieves 1.3x speedup over Intel oneDNN on an x86 CPU, 1.75x speedup over Nvidia cuDNN on an NvidiaGPU, and 1.13x speedup over a carefully tuned TVM solution for ARM DOT on an ARM CPU.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.08458v3-abstract-full').style.display = 'none'; document.getElementById('2101.08458v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 13 figures, and 1 table</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2021 IEEE/ACM International Symposium on <span class="search-hit mathjax">Code</span> Generation and <span class="search-hit mathjax">Optimization</span> (CGO), Seoul, Korea (South), 2021, pp. 77-89
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.07910">arXiv:2101.07910</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.07910">pdf</a>, <a href="https://arxiv.org/format/2101.07910">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Search-Based Testing Framework for Deep Neural Networks of Source <span class="search-hit mathjax">Code</span> Embedding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pour%2C+M+V">Maryam Vahdat Pour</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhuo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+L">Lei Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hemmati%2C+H">Hadi Hemmati</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.07910v1-abstract-short" style="display: inline;">
        Over the past few years, deep neural networks (DNNs) have been continuously expanding their real-world <span class="search-hit mathjax">applications</span> for source&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07910v1-abstract-full').style.display = 'inline'; document.getElementById('2101.07910v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.07910v1-abstract-full" style="display: none;">
        Over the past few years, deep neural networks (DNNs) have been continuously expanding their real-world <span class="search-hit mathjax">applications</span> for source <span class="search-hit mathjax">code</span> processing tasks across the <span class="search-hit mathjax">software</span> engineering domain, e.g., clone detection, <span class="search-hit mathjax">code</span> search, comment generation. Although quite a few recent works have been <span class="search-hit mathjax">performed</span> on testing of DNNs in the context of image and speech processing, limited progress has been achieved so far on DNN testing in the context of source <span class="search-hit mathjax">code</span> processing, that exhibits rather unique characteristics and challenges.
  In this paper, we propose a search-based testing framework for DNNs of source <span class="search-hit mathjax">code</span> embedding and its downstream processing tasks like <span class="search-hit mathjax">Code</span> Search. To generate new test inputs, we adopt popular source <span class="search-hit mathjax">code</span> refactoring tools to generate the semantically equivalent variants. For more <span class="search-hit mathjax">effective</span> testing, we leverage the DNN mutation testing to guide the testing direction. To demonstrate the usefulness of our technique, we <span class="search-hit mathjax">perform</span> a large-scale evaluation on popular DNNs of source <span class="search-hit mathjax">code</span> processing based on multiple state-of-the-art <span class="search-hit mathjax">code</span> embedding methods (i.e., Code2vec, Code2seq and CodeBERT). The testing results show that our generated adversarial samples can on average <span class="search-hit mathjax">reduce</span> the <span class="search-hit mathjax">performance</span> of these DNNs from 5.41% to 9.58%. Through retraining the DNNs with our generated adversarial samples, the robustness of DNN can <span class="search-hit mathjax">improve</span> by 23.05% on average. The evaluation results also show that our adversarial test generation strategy has the least negative impact (median of 3.56%), on the <span class="search-hit mathjax">performance</span> of the DNNs for regular test data, compared to the other methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07910v1-abstract-full').style.display = 'none'; document.getElementById('2101.07910v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICST 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.07690">arXiv:2101.07690</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.07690">pdf</a>, <a href="https://arxiv.org/format/2101.07690">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Efficient</span> Mining of Frequent Subgraphs with Two-Vertex Exploration
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+P">Peng Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Rujia Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+B">Bo Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.07690v2-abstract-short" style="display: inline;">
        Frequent Subgraph Mining (FSM) is the key task in many graph mining and machine learning <span class="search-hit mathjax">applications</span>. Numerous systems have been proposed for FSM in the past decade. Although these systems show good&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07690v2-abstract-full').style.display = 'inline'; document.getElementById('2101.07690v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.07690v2-abstract-full" style="display: none;">
        Frequent Subgraph Mining (FSM) is the key task in many graph mining and machine learning <span class="search-hit mathjax">applications</span>. Numerous systems have been proposed for FSM in the past decade. Although these systems show good <span class="search-hit mathjax">performance</span> for small patterns (with no more than four vertices), we found that they have difficulty in mining larger patterns. In this work, we propose a novel two-vertex exploration strategy to accelerate the mining process. Compared with the single-vertex exploration adopted by previous systems, our two-vertex exploration avoids the large memory consumption issue and significantly <span class="search-hit mathjax">reduces</span> the memory access overhead. We further enhance the <span class="search-hit mathjax">performance</span> through an index-based quick pattern technique that <span class="search-hit mathjax">reduces</span> the overhead of isomorphism checks, and a subgraph sampling technique that mitigates the issue of subgraph explosion. The experimental results show that our system achieves significant speedups against the state-of-the-art graph pattern mining systems and supports larger pattern mining tasks that none of the existing systems can handle.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07690v2-abstract-full').style.display = 'none'; document.getElementById('2101.07690v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 February, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.07430">arXiv:2101.07430</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.07430">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Surrogate-Assisted Variable Grouping Algorithm for General Large Scale Global <span class="search-hit mathjax">Optimization</span> Problems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+A">An Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Z">Zhigang Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+M">Muyi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+Y">Yongsheng Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hanqing Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+W">Wenhao Du</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.07430v1-abstract-short" style="display: inline;">
        Problem decomposition plays a vital role when applying cooperative coevolution (CC) to large scale global <span class="search-hit mathjax">optimization</span> problems. However, most learning-based decomposition algorithms either only apply to additively separable problems or face the issue of false separability detections. Directing against these limitations, this study proposes a novel decomposi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07430v1-abstract-full').style.display = 'inline'; document.getElementById('2101.07430v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.07430v1-abstract-full" style="display: none;">
        Problem decomposition plays a vital role when applying cooperative coevolution (CC) to large scale global <span class="search-hit mathjax">optimization</span> problems. However, most learning-based decomposition algorithms either only apply to additively separable problems or face the issue of false separability detections. Directing against these limitations, this study proposes a novel decomposition algorithm called surrogate-assisted variable grouping (SVG). SVG first designs a general-separability-oriented detection criterion according to whether the optimum of a variable changes with other variables. This criterion is consistent with the separability definition and thus endows SVG with broad <span class="search-hit mathjax">applicability</span> and high <span class="search-hit mathjax">accuracy</span>. To <span class="search-hit mathjax">reduce</span> the fitness evaluation requirement, SVG seeks the optimum of a variable with the help of a surrogate model rather than the original expensive high-dimensional model. Moreover, it converts the variable grouping process into a dynamic-binary-tree search one, which facilitates reutilizing historical separability detection information and thus <span class="search-hit mathjax">reducing</span> detection times. To evaluate the <span class="search-hit mathjax">performance</span> of SVG, a suite of benchmark functions with up to 2000 dimensions, including additively and non-additively separable ones, were designed. Experimental results on these functions indicate that, compared with six state-of-the-art decomposition algorithms, SVG possesses broader <span class="search-hit mathjax">applicability</span> and competitive <span class="search-hit mathjax">efficiency</span>. Furthermore, it can significantly enhance the <span class="search-hit mathjax">optimization</span> <span class="search-hit mathjax">performance</span> of CC.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07430v1-abstract-full').style.display = 'none'; document.getElementById('2101.07430v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.07412">arXiv:2101.07412</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.07412">pdf</a>, <a href="https://arxiv.org/format/2101.07412">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Improved</span> parallel WaveGAN vocoder with perceptually weighted spectrogram loss
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+E">Eunwoo Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yamamoto%2C+R">Ryuichi Yamamoto</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hwang%2C+M">Min-Jae Hwang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Jin-Seob Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kwon%2C+O">Ohsung Kwon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Jae-Min Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.07412v1-abstract-short" style="display: inline;">
        &hellip;model. By employing multi-resolution short-time Fourier transform (MR-STFT) criteria with a generative adversarial network, the light-weight convolutional networks can be <span class="search-hit mathjax">effectively</span> trained without any distillation process. To further&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07412v1-abstract-full').style.display = 'inline'; document.getElementById('2101.07412v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.07412v1-abstract-full" style="display: none;">
        This paper proposes a spectral-domain perceptual weighting technique for Parallel WaveGAN-based text-to-speech (TTS) systems. The recently proposed Parallel WaveGAN vocoder successfully generates waveform sequences using a fast non-autoregressive WaveNet model. By employing multi-resolution short-time Fourier transform (MR-STFT) criteria with a generative adversarial network, the light-weight convolutional networks can be <span class="search-hit mathjax">effectively</span> trained without any distillation process. To further <span class="search-hit mathjax">improve</span> the vocoding <span class="search-hit mathjax">performance</span>, we propose the <span class="search-hit mathjax">application</span> of frequency-dependent weighting to the MR-STFT loss function. The proposed method penalizes perceptually-sensitive errors in the frequency domain; thus, the model is <span class="search-hit mathjax">optimized</span> toward <span class="search-hit mathjax">reducing</span> auditory noise in the synthesized speech. Subjective listening test results demonstrate that our proposed method achieves 4.21 and 4.26 TTS mean opinion scores for female and male Korean speakers, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07412v1-abstract-full').style.display = 'none'; document.getElementById('2101.07412v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in SLT 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.07344">arXiv:2101.07344</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.07344">pdf</a>, <a href="https://arxiv.org/format/2101.07344">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Accelerating Deep Learning Inference via Learned Caches
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Balasubramanian%2C+A">Arjun Balasubramanian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+A">Adarsh Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yuhan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+H">Han Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Venkataraman%2C+S">Shivaram Venkataraman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akella%2C+A">Aditya Akella</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.07344v1-abstract-short" style="display: inline;">
        Deep Neural Networks (DNNs) are witnessing increased adoption in multiple domains owing to their high <span class="search-hit mathjax">accuracy</span> in solving real-world problems. However, this high&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07344v1-abstract-full').style.display = 'inline'; document.getElementById('2101.07344v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.07344v1-abstract-full" style="display: none;">
        Deep Neural Networks (DNNs) are witnessing increased adoption in multiple domains owing to their high <span class="search-hit mathjax">accuracy</span> in solving real-world problems. However, this high <span class="search-hit mathjax">accuracy</span> has been achieved by building deeper networks, posing a fundamental challenge to the low latency inference desired by user-facing <span class="search-hit mathjax">applications</span>. Current low latency solutions trade-off on <span class="search-hit mathjax">accuracy</span> or fail to exploit the inherent temporal locality in prediction serving workloads.
  We observe that caching hidden layer outputs of the DNN can introduce a form of late-binding where inference requests only consume the amount of computation needed. This enables a mechanism for achieving low latencies, coupled with an ability to exploit temporal locality. However, traditional caching approaches incur high memory overheads and lookup latencies, leading us to design learned caches - caches that consist of simple ML models that are continuously updated. We present the design of GATI, an end-to-end prediction serving system that incorporates learned caches for low-latency DNN inference. Results show that GATI can <span class="search-hit mathjax">reduce</span> inference latency by up to 7.69X on realistic workloads.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07344v1-abstract-full').style.display = 'none'; document.getElementById('2101.07344v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.07327">arXiv:2101.07327</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.07327">pdf</a>, <a href="https://arxiv.org/format/2101.07327">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OpenUVR: an Open-Source System Framework for Untethered Virtual Reality <span class="search-hit mathjax">Applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rohloff%2C+A">Alec Rohloff</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Allen%2C+Z">Zackary Allen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+K">Kung-Min Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Okrend%2C+J">Joshua Okrend</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nie%2C+C">Chengyi Nie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yu-Chia Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tseng%2C+H">Hung-Wei Tseng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.07327v1-abstract-short" style="display: inline;">
        Advancements in heterogeneous computing technologies enable the significant potential of virtual reality (VR) <span class="search-hit mathjax">applications</span>. To offer the best user experience (UX), a system should adopt an untethered, wireless-network-based architecture to transfer VR content between the user and the content generator. However, modern wireless network technologies make imple&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07327v1-abstract-full').style.display = 'inline'; document.getElementById('2101.07327v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.07327v1-abstract-full" style="display: none;">
        Advancements in heterogeneous computing technologies enable the significant potential of virtual reality (VR) <span class="search-hit mathjax">applications</span>. To offer the best user experience (UX), a system should adopt an untethered, wireless-network-based architecture to transfer VR content between the user and the content generator. However, modern wireless network technologies make implementing such an architecture challenging, as VR <span class="search-hit mathjax">applications</span> require superior video quality -- with high resolution, high frame rates, and very low latency.
  This paper presents OpenUVR, an open-source framework that uses commodity hardware components to satisfy the demands of interactive, real-time VR <span class="search-hit mathjax">applications</span>. OpenUVR significantly <span class="search-hit mathjax">improves</span> UX through a redesign of the system stack and addresses the most time-sensitive issues associated with redundant memory copying in modern computing systems. OpenUVR presents a cross-layered VR datapath to avoid redundant data operations and computation among system components, OpenUVR customizes the network stack to eliminate unnecessary memory operations incurred by mismatching data formats in each layer, and OpenUVR uses feedback from mobile devices to remove memory buffers.
  Together, these modifications allow OpenUVR to <span class="search-hit mathjax">reduce</span> VR <span class="search-hit mathjax">application</span> delays to 14.32 ms, meeting the 20 ms minimum latency in avoiding motion sickness. As an open-source system that is fully compatible with commodity hardware, OpenUVR offers the research community an opportunity to develop, investigate, and <span class="search-hit mathjax">optimize</span> <span class="search-hit mathjax">applications</span> for untethered, high-<span class="search-hit mathjax">performance</span> VR architectures.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07327v1-abstract-full').style.display = 'none'; document.getElementById('2101.07327v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.07004">arXiv:2101.07004</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.07004">pdf</a>, <a href="https://arxiv.org/format/2101.07004">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Machine Learning-Enabled Joint Antenna Selection and Precoding Design: From Offline Complexity to Online <span class="search-hit mathjax">Performance</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Vu%2C+T+X">Thang X. Vu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatzinotas%2C+S">Symeon Chatzinotas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+V">Van-Dinh Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoang%2C+D+T">Dinh Thai Hoang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+D+N">Diep N. Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Di+Renzo%2C+M">Marco Di Renzo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ottersten%2C+B">Bjorn Ottersten</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.07004v1-abstract-short" style="display: inline;">
        We investigate the <span class="search-hit mathjax">performance</span> of multi-user multiple-antenna downlink systems in which a BS serves multiple users via a shared wireless medium. In order to fully exploit the spatial diversity while minimizing the passive energy consumed by radio frequency (RF) components, the BS is equipped with M RF chains and N antennas, where M &lt; N. Upon receiving pil&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07004v1-abstract-full').style.display = 'inline'; document.getElementById('2101.07004v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.07004v1-abstract-full" style="display: none;">
        We investigate the <span class="search-hit mathjax">performance</span> of multi-user multiple-antenna downlink systems in which a BS serves multiple users via a shared wireless medium. In order to fully exploit the spatial diversity while minimizing the passive energy consumed by radio frequency (RF) components, the BS is equipped with M RF chains and N antennas, where M &lt; N. Upon receiving pilot sequences to obtain the channel state information, the BS determines the best subset of M antennas for serving the users. We propose a joint antenna selection and precoding design (JASPD) algorithm to maximize the system sum rate subject to a transmit power constraint and QoS requirements. The JASPD overcomes the non-convexity of the formulated problem via a doubly iterative algorithm, in which an inner loop successively <span class="search-hit mathjax">optimizes</span> the precoding vectors, followed by an outer loop that tries all valid antenna subsets. Although approaching the (near) global <span class="search-hit mathjax">optimality</span>, the JASPD suffers from a combinatorial complexity, which may limit its <span class="search-hit mathjax">application</span> in real-time network operations. To overcome this limitation, we propose a learning-based antenna selection and precoding design algorithm (L-ASPA), which employs a DNN to establish underlaying relations between the key system parameters and the selected antennas. The proposed L-ASPD is robust against the number of users and their locations, BS&#39;s transmit power, as well as the small-scale channel fading. With a well-trained learning model, it is shown that the L-ASPD significantly outperforms baseline schemes based on the block diagonalization and a learning-assisted solution for broadcasting systems and achieves higher <span class="search-hit mathjax">effective</span> sum rate than that of the JASPA under limited processing time. In addition, we observed that the proposed L-ASPD can <span class="search-hit mathjax">reduce</span> the computation complexity by 95% while retaining more than 95% of the <span class="search-hit mathjax">optimal</span> <span class="search-hit mathjax">performance</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.07004v1-abstract-full').style.display = 'none'; document.getElementById('2101.07004v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted to the IEEE Transactions on Wireless Communications</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.06594">arXiv:2101.06594</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.06594">pdf</a>, <a href="https://arxiv.org/format/2101.06594">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PLUME: <span class="search-hit mathjax">Efficient</span> 3D Object Detection from Stereo Images
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+B">Bin Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+R">Rui Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+M">Ming Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Urtasun%2C+R">Raquel Urtasun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.06594v2-abstract-short" style="display: inline;">
        3D object detection plays a significant role in various robotic <span class="search-hit mathjax">applications</span> including self-driving. While many approaches rely on expensive 3D sensors like LiDAR to produce accurate 3D estimates, stereo-based methods have recently shown promising results at a lower cost. Existing methods tackle the problem in two steps: first depth estimation is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.06594v2-abstract-full').style.display = 'inline'; document.getElementById('2101.06594v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.06594v2-abstract-full" style="display: none;">
        3D object detection plays a significant role in various robotic <span class="search-hit mathjax">applications</span> including self-driving. While many approaches rely on expensive 3D sensors like LiDAR to produce accurate 3D estimates, stereo-based methods have recently shown promising results at a lower cost. Existing methods tackle the problem in two steps: first depth estimation is <span class="search-hit mathjax">performed</span>, a pseudo LiDAR point cloud representation is computed from the depth estimates, and then object detection is <span class="search-hit mathjax">performed</span> in 3D space. However, because the two separate tasks are <span class="search-hit mathjax">optimized</span> in different metric spaces, the depth estimation is biased towards nearby objects and may cause sub-<span class="search-hit mathjax">optimal</span> <span class="search-hit mathjax">performance</span> of 3D detection. In this paper we propose a model that unifies these two tasks in the same metric space. Specifically, our model directly constructs a pseudo LiDAR feature volume (PLUME) in 3D space, which is used to solve both occupancy estimation and object detection tasks. Our approach achieves state-of-the-art <span class="search-hit mathjax">performance</span> on the challenging KITTI benchmark, with significantly <span class="search-hit mathjax">reduced</span> inference time compared with existing methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.06594v2-abstract-full').style.display = 'none'; document.getElementById('2101.06594v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.06371">arXiv:2101.06371</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.06371">pdf</a>, <a href="https://arxiv.org/format/2101.06371">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NNStreamer: <span class="search-hit mathjax">Efficient</span> and Agile Development of On-Device AI Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ham%2C+M">MyungJoo Ham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moon%2C+J">Jijoong Moon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lim%2C+G">Geunsik Lim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jung%2C+J">Jaeyun Jung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ahn%2C+H">Hyoungjoo Ahn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+W">Wook Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Woo%2C+S">Sangjung Woo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kapoor%2C+P">Parichay Kapoor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chae%2C+D">Dongju Chae</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jang%2C+G">Gichan Jang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ahn%2C+Y">Yongjoo Ahn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+J">Jihoon Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.06371v1-abstract-short" style="display: inline;">
        We propose NNStreamer, a <span class="search-hit mathjax">software</span> system that handles neural networks as filters of stream pipelines, applying the stream processing paradigm to deep neural network&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.06371v1-abstract-full').style.display = 'inline'; document.getElementById('2101.06371v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.06371v1-abstract-full" style="display: none;">
        We propose NNStreamer, a <span class="search-hit mathjax">software</span> system that handles neural networks as filters of stream pipelines, applying the stream processing paradigm to deep neural network <span class="search-hit mathjax">applications</span>. A new trend with the wide-spread of deep neural network <span class="search-hit mathjax">applications</span> is on-device AI. It is to process neural networks on mobile devices or edge/IoT devices instead of cloud servers. Emerging privacy issues, data transmission costs, and operational costs signify the need for on-device AI, especially if we deploy a massive number of devices. NNStreamer <span class="search-hit mathjax">efficiently</span> handles neural networks with complex data stream pipelines on devices, significantly <span class="search-hit mathjax">improving</span> the overall <span class="search-hit mathjax">performance</span> with minimal efforts. Besides, NNStreamer simplifies implementations and allows reusing off-the-shelf media filters directly, which <span class="search-hit mathjax">reduces</span> developmental costs significantly. We are already deploying NNStreamer for a wide range of products and platforms, including the Galaxy series and various consumer electronic devices. The experimental results suggest a reduction in developmental costs and enhanced <span class="search-hit mathjax">performance</span> of pipeline architectures and NNStreamer. It is an open-source project incubated by Linux Foundation AI, available to the public and <span class="search-hit mathjax">applicable</span> to various hardware and <span class="search-hit mathjax">software</span> platforms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.06371v1-abstract-full').style.display = 'none'; document.getElementById('2101.06371v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE/ACM ICSE 2021 SEIP (preprint)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.05946">arXiv:2101.05946</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.05946">pdf</a>, <a href="https://arxiv.org/format/2101.05946">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Risk-Sensitive Task Offloading Strategy for Edge Computing in Industrial Internet of Things
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+X">Xiaoyu Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+R">Ruohai Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+T">Tao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Y">Yulin Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+B">Bo Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+Y">Yuhe Qiu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.05946v1-abstract-short" style="display: inline;">
        &hellip;Things in the fifth generation communication systems, and is also a promising technology in the future sixth generation communication systems. In this work, we consider the <span class="search-hit mathjax">application</span> of edge computing to smart factories for mission-critical task offloading through wireless links. In such scenarios, although high end-to-end delays from the generation to com&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.05946v1-abstract-full').style.display = 'inline'; document.getElementById('2101.05946v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.05946v1-abstract-full" style="display: none;">
        Edge computing has become one of the key enablers for ultra-reliable and low-latency communications in the industrial Internet of Things in the fifth generation communication systems, and is also a promising technology in the future sixth generation communication systems. In this work, we consider the <span class="search-hit mathjax">application</span> of edge computing to smart factories for mission-critical task offloading through wireless links. In such scenarios, although high end-to-end delays from the generation to completion of tasks happen with low probability, they may incur severe casualties and property loss, and should be seriously treated. Inspired by the risk management theory widely used in finance, we adopt the Conditional Value at Risk to capture the tail of the delay distribution. An upper bound of the Conditional Value at Risk is derived through analysis of the queues both at the devices and the edge computing servers. We aim to find out the <span class="search-hit mathjax">optimal</span> offloading policy taking into consideration both the average and the worst case delay <span class="search-hit mathjax">performance</span> of the system. Given that the formulated <span class="search-hit mathjax">optimization</span> problem is a non-convex mixed integer non-linear <span class="search-hit mathjax">programming</span> problem, a decomposition into sub-problems is <span class="search-hit mathjax">performed</span> and a two-stage heuristic algorithm is proposed. Simulation results validate our analysis and indicate that the proposed algorithm can <span class="search-hit mathjax">reduce</span> the risk in both the queuing and end-to-end delay.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.05946v1-abstract-full').style.display = 'none'; document.getElementById('2101.05946v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, has been submitted to EURASIP JWCN, major revision</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.05363">arXiv:2101.05363</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.05363">pdf</a>, <a href="https://arxiv.org/format/2101.05363">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NetCut: Real-Time DNN Inference Using Layer Removal
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zandigohar%2C+M">Mehrshad Zandigohar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erdogmus%2C+D">Deniz Erdogmus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schirner%2C+G">Gunar Schirner</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.05363v1-abstract-short" style="display: inline;">
        &hellip;Learning plays a significant role in assisting humans in many aspects of their lives. As these networks tend to get deeper over time, they extract more features to increase <span class="search-hit mathjax">accuracy</span> at the cost of additional inference latency. This&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.05363v1-abstract-full').style.display = 'inline'; document.getElementById('2101.05363v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.05363v1-abstract-full" style="display: none;">
        Deep Learning plays a significant role in assisting humans in many aspects of their lives. As these networks tend to get deeper over time, they extract more features to increase <span class="search-hit mathjax">accuracy</span> at the cost of additional inference latency. This <span class="search-hit mathjax">accuracy</span>-<span class="search-hit mathjax">performance</span> trade-off makes it more challenging for Embedded Systems, as resource-constrained processors with strict deadlines, to deploy them <span class="search-hit mathjax">efficiently</span>. This can lead to selection of networks that can prematurely meet a specified deadline with excess slack time that could have potentially contributed to increased <span class="search-hit mathjax">accuracy</span>.
  In this work, we propose: (i) the concept of layer removal as a means of constructing TRimmed Networks (TRNs) that are based on removing problem-specific features of a pretrained network used in transfer learning, and (ii) NetCut, a methodology based on an empirical or an analytical latency estimator, which only proposes and retrains TRNs that can meet the <span class="search-hit mathjax">application&#39;s</span> deadline, hence <span class="search-hit mathjax">reducing</span> the exploration time significantly.
  We demonstrate that TRNs can expand the Pareto frontier that trades off latency and <span class="search-hit mathjax">accuracy</span> to provide networks that can meet arbitrary deadlines with potential <span class="search-hit mathjax">accuracy</span> <span class="search-hit mathjax">improvement</span> over off-the-shelf networks. Our experimental results show that such utilization of TRNs, while transferring to a simpler dataset, in combination with NetCut, can lead to the proposal of networks that can achieve relative <span class="search-hit mathjax">accuracy</span> <span class="search-hit mathjax">improvement</span> of up to 10.43% among existing off-the-shelf neural architectures while meeting a specific deadline, and 27x speedup in exploration time.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.05363v1-abstract-full').style.display = 'none'; document.getElementById('2101.05363v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.05216">arXiv:2101.05216</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.05216">pdf</a>, <a href="https://arxiv.org/format/2101.05216">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AttentionLite: Towards <span class="search-hit mathjax">Efficient</span> Self-Attention Models for Vision
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kundu%2C+S">Souvik Kundu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sundaresan%2C+S">Sairam Sundaresan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.05216v1-abstract-short" style="display: inline;">
        We propose a novel framework for producing a class of parameter and compute <span class="search-hit mathjax">efficient</span> models called AttentionLitesuitable for resource-constrained&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.05216v1-abstract-full').style.display = 'inline'; document.getElementById('2101.05216v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.05216v1-abstract-full" style="display: none;">
        We propose a novel framework for producing a class of parameter and compute <span class="search-hit mathjax">efficient</span> models called AttentionLitesuitable for resource-constrained <span class="search-hit mathjax">applications</span>. Prior work has primarily focused on <span class="search-hit mathjax">optimizing</span> models either via knowledge distillation or pruning. In addition to fusing these two mechanisms, our joint <span class="search-hit mathjax">optimization</span> framework also leverages recent advances in self-attention as a substitute for convolutions. We can simultaneously distill knowledge from a compute-heavy teacher while also pruning the student model in a single pass of training thereby <span class="search-hit mathjax">reducing</span> training and fine-tuning times considerably. We evaluate the merits of our proposed approach on the CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. Not only do our AttentionLite models significantly outperform their unoptimized counterparts in <span class="search-hit mathjax">accuracy</span>, we find that in some cases, that they <span class="search-hit mathjax">perform</span> almost as well as their compute-heavy teachers while consuming only a fraction of the parameters and FLOPs. Concretely, AttentionLite models can achieve upto30x parameter <span class="search-hit mathjax">efficiency</span> and 2x computation <span class="search-hit mathjax">efficiency</span> with no significant <span class="search-hit mathjax">accuracy</span> drop compared to their teacher.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.05216v1-abstract-full').style.display = 'none'; document.getElementById('2101.05216v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 3 figures, 2 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.04434">arXiv:2101.04434</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.04434">pdf</a>, <a href="https://arxiv.org/format/2101.04434">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Developing an OpenAI Gym-compatible framework and simulation environment for testing Deep Reinforcement Learning agents solving the Ambulance Location Problem
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Allen%2C+M">Michael Allen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pearn%2C+K">Kerry Pearn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Monks%2C+T">Tom Monks</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.04434v2-abstract-short" style="display: inline;">
        Background and motivation: Deep Reinforcement Learning (Deep RL) is a rapidly developing field. Historically most <span class="search-hit mathjax">application</span> has been made to games (such as chess, Atari games, and go). Deep RL is now reaching the stage where it may offer value in real world problems, including optimisation of healthcare systems. One such problem is where to locate ambulanc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.04434v2-abstract-full').style.display = 'inline'; document.getElementById('2101.04434v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.04434v2-abstract-full" style="display: none;">
        Background and motivation: Deep Reinforcement Learning (Deep RL) is a rapidly developing field. Historically most <span class="search-hit mathjax">application</span> has been made to games (such as chess, Atari games, and go). Deep RL is now reaching the stage where it may offer value in real world problems, including optimisation of healthcare systems. One such problem is where to locate ambulances between calls in order to minimise time from emergency call to ambulance on-scene. This is known as the Ambulance Location problem.
  Aim: To develop an OpenAI Gym-compatible framework and simulation environment for testing Deep RL agents.
  Methods: A custom ambulance dispatch simulation environment was developed using OpenAI Gym and SimPy. Deep RL agents were built using PyTorch. The environment is a simplification of the real world, but allows control over the number of clusters of incident locations, number of possible dispatch locations, number of hospitals, and creating incidents that occur at different locations throughout each day.
  Results: A range of Deep RL agents based on Deep Q networks were tested in this custom environment. All <span class="search-hit mathjax">reduced</span> time to respond to emergency calls compared with random allocation to dispatch points. Bagging Noisy Duelling Deep Q networks gave the most consistence <span class="search-hit mathjax">performance</span>. All methods had a tendency to lose <span class="search-hit mathjax">performance</span> if trained for too long, and so agents were saved at their <span class="search-hit mathjax">optimal</span> <span class="search-hit mathjax">performance</span> (and tested on independent simulation runs).
  Conclusions: Deep RL agents, developed using simulated environments, have the potential to offer a novel approach to optimise the Ambulance Location problem. Creating open simulation environments should allow more rapid progress in this field.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.04434v2-abstract-full').style.display = 'none'; document.getElementById('2101.04434v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Fig 1 updated since first version (corrected panel 3 which previously replicated panel 2)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.02594">arXiv:2101.02594</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.02594">pdf</a>, <a href="https://arxiv.org/format/2101.02594">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Formal Languages and Automata Theory">cs.FL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On Satisficing in Quantitative Games
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bansal%2C+S">Suguman Bansal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatterjee%2C+K">Krishnendu Chatterjee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vardi%2C+M+Y">Moshe Y. Vardi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.02594v1-abstract-short" style="display: inline;">
        Several problems in planning and reactive synthesis can be <span class="search-hit mathjax">reduced</span> to the analysis of two-player quantitative graph games. {\em&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.02594v1-abstract-full').style.display = 'inline'; document.getElementById('2101.02594v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.02594v1-abstract-full" style="display: none;">
        Several problems in planning and reactive synthesis can be <span class="search-hit mathjax">reduced</span> to the analysis of two-player quantitative graph games. {\em <span class="search-hit mathjax">Optimization</span>} is one form of analysis. We argue that in many cases it may be better to replace the <span class="search-hit mathjax">optimization</span> problem with the {\em satisficing problem}, where instead of searching for <span class="search-hit mathjax">optimal</span> solutions, the goal is to search for solutions that adhere to a given threshold bound.
  This work defines and investigates the satisficing problem on a two-player graph game with the discounted-sum cost model. We show that while the satisficing problem can be solved using numerical methods just like the <span class="search-hit mathjax">optimization</span> problem, this approach does not render compelling benefits over <span class="search-hit mathjax">optimization</span>. When the discount factor is, however, an integer, we present another approach to satisficing, which is purely based on automata methods. We show that this approach is algorithmically more <span class="search-hit mathjax">performant</span> -- both theoretically and empirically -- and demonstrates the broader <span class="search-hit mathjax">applicability</span> of satisficing overoptimization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.02594v1-abstract-full').style.display = 'none'; document.getElementById('2101.02594v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: text overlap with arXiv:2010.02055</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.02270">arXiv:2101.02270</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.02270">pdf</a>, <a href="https://arxiv.org/format/2101.02270">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.segan.2021.100483">10.1016/j.segan.2021.100483 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fast Parallel Newton-Raphson Power Flow Solver for Large Number of System Calculations with CPU and GPU
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhenqi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berg%2C+S+W">Sebastian Wende-von Berg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Braun%2C+M">Martin Braun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.02270v3-abstract-short" style="display: inline;">
        &hellip;uncertainties of the renewable generation with probabilistic Monte Carlo simulation or in stationary time series simulation, large number of power flow calculations have to be <span class="search-hit mathjax">performed</span>. For the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.02270v3-abstract-full').style.display = 'inline'; document.getElementById('2101.02270v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.02270v3-abstract-full" style="display: none;">
        To analyze large sets of grid states, e.g. when evaluating the impact from the uncertainties of the renewable generation with probabilistic Monte Carlo simulation or in stationary time series simulation, large number of power flow calculations have to be <span class="search-hit mathjax">performed</span>. For the <span class="search-hit mathjax">application</span> in real-time grid operation, grid planning and in further cases when computational time is critical, a novel approach on simultaneous parallelization of many Newton-Raphson power flow calculations on CPU and with GPU-acceleration is proposed. The result shows a speed-up of over x100 comparing to the open-source tool pandapower, when <span class="search-hit mathjax">performing</span> repetitive power flows of system with admittance matrix of the same sparsity pattern on both CPU and GPU. The speed-up relies on the algorithm <span class="search-hit mathjax">improvement</span> and highly <span class="search-hit mathjax">optimized</span> parallelization strategy, which can <span class="search-hit mathjax">reduce</span> the repetitive work and saturate the high hardware computational capability of modern CPUs and GPUs well. This is achieved with the proposed batched sparse matrix operation and batched linear solver based on LU-refactorization. The batched linear solver shows a large <span class="search-hit mathjax">performance</span> <span class="search-hit mathjax">improvement</span> comparing to the state-of-the-art linear system solver KLU library and a better saturation of the GPU <span class="search-hit mathjax">performance</span> with small problem scale. Finally, the method of integrating the proposed solver into pandapower is presented, thus the parallel power flow solver with outstanding <span class="search-hit mathjax">performance</span> can be easily applied in challenging real-life grid operation and innovative researches e.g. data-driven machine learning studies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.02270v3-abstract-full').style.display = 'none'; document.getElementById('2101.02270v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">preprint accepted in Sustainable Energy, Grids and Networks</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.01505">arXiv:2101.01505</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.01505">pdf</a>, <a href="https://arxiv.org/format/2101.01505">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Delayed Projection Techniques for Linearly Constrained Problems: Convergence Rates, Acceleration, and <span class="search-hit mathjax">Applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhihua Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.01505v1-abstract-short" style="display: inline;">
        In this work, we study a novel class of projection-based algorithms for linearly constrained problems (LCPs) which have a lot of <span class="search-hit mathjax">applications</span> in statistics,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.01505v1-abstract-full').style.display = 'inline'; document.getElementById('2101.01505v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.01505v1-abstract-full" style="display: none;">
        In this work, we study a novel class of projection-based algorithms for linearly constrained problems (LCPs) which have a lot of <span class="search-hit mathjax">applications</span> in statistics, <span class="search-hit mathjax">optimization</span>, and machine learning. Conventional primal gradient-based methods for LCPs call a projection after each (stochastic) gradient descent, resulting in that the required number of projections equals that of gradient descents (or total iterations). Motivated by the recent progress in distributed <span class="search-hit mathjax">optimization</span>, we propose the delayed projection technique that calls a projection once for a while, lowering the projection frequency and <span class="search-hit mathjax">improving</span> the projection <span class="search-hit mathjax">efficiency</span>. Accordingly, we devise a series of stochastic methods for LCPs using the technique, including a variance <span class="search-hit mathjax">reduced</span> method and an accelerated one. We theoretically show that it is feasible to <span class="search-hit mathjax">improve</span> projection <span class="search-hit mathjax">efficiency</span> in both strongly convex and generally convex cases. Our analysis is simple and unified and can be easily extended to other methods using delayed projections. When applying our new algorithms to federated <span class="search-hit mathjax">optimization</span>, a newfangled and privacy-preserving subfield in distributed <span class="search-hit mathjax">optimization</span>, we obtain not only a variance <span class="search-hit mathjax">reduced</span> federated algorithm with convergence rates better than previous works, but also the first accelerated method able to handle data heterogeneity inherent in federated <span class="search-hit mathjax">optimization</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.01505v1-abstract-full').style.display = 'none'; document.getElementById('2101.01505v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.01335">arXiv:2101.01335</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.01335">pdf</a>, <a href="https://arxiv.org/format/2101.01335">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Modeling the Linux page cache for accurate simulation of data-intensive <span class="search-hit mathjax">applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Do%2C+H">Hoang-Dung Do</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hayot-Sasson%2C+V">Valerie Hayot-Sasson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=da+Silva%2C+R+F">Rafael Ferreira da Silva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Steele%2C+C">Christopher Steele</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Casanova%2C+H">Henri Casanova</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Glatard%2C+T">Tristan Glatard</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.01335v1-abstract-short" style="display: inline;">
        The emergence of Big Data in recent years has resulted in a growing need for <span class="search-hit mathjax">efficient</span> data processing solutions. While infrastructures with sufficient compute power are available, the I/O bottleneck remains. The Linux page cache is an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.01335v1-abstract-full').style.display = 'inline'; document.getElementById('2101.01335v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.01335v1-abstract-full" style="display: none;">
        The emergence of Big Data in recent years has resulted in a growing need for <span class="search-hit mathjax">efficient</span> data processing solutions. While infrastructures with sufficient compute power are available, the I/O bottleneck remains. The Linux page cache is an <span class="search-hit mathjax">efficient</span> approach to <span class="search-hit mathjax">reduce</span> I/O overheads, but few experimental studies of its interactions with Big Data <span class="search-hit mathjax">applications</span> exist, partly due to limitations of real-world experiments. Simulation is a popular approach to address these issues, however, existing simulation frameworks do not simulate page caching fully, or even at all. As a result, simulation-based <span class="search-hit mathjax">performance</span> studies of data-intensive <span class="search-hit mathjax">applications</span> lead to inaccurate results.
  In this paper, we propose an I/O simulation model that includes the key features of the Linux page cache. We have implemented this model as part of the WRENCH workflow simulation framework, which itself builds on the popular SimGrid distributed systems simulation framework. Our model and its implementation enable the simulation of both single-threaded and multithreaded <span class="search-hit mathjax">applications</span>, and of both writeback and writethrough caches for local or network-based filesystems. We evaluate the <span class="search-hit mathjax">accuracy</span> of our model in different conditions, including sequential and concurrent <span class="search-hit mathjax">applications</span>, as well as local and remote I/Os. We find that our page cache model <span class="search-hit mathjax">reduces</span> the simulation error by up to an order of magnitude when compared to state-of-the-art, cacheless simulations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.01335v1-abstract-full').style.display = 'none'; document.getElementById('2101.01335v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 8 figures, CCGrid</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.01302">arXiv:2101.01302</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.01302">pdf</a>, <a href="https://arxiv.org/ps/2101.01302">ps</a>, <a href="https://arxiv.org/format/2101.01302">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploiting Deep Learning for Secure Transmission in an Underlay Cognitive Radio Network
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Miao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cumanan%2C+K">Kanapathippillai Cumanan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thiyagalingam%2C+J">Jeyarajan Thiyagalingam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Y">Yanqun Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+Z">Zhiguo Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dobre%2C+O+A">Octavia A. Dobre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.01302v1-abstract-short" style="display: inline;">
        &hellip;is the capability to solve the power allocation problem with both perfect and imperfect channel state information. In a conventional setting, two completely different <span class="search-hit mathjax">optimization</span> frameworks have to be designed, namely the robust and non-robust designs. Furthermore, conventional algorithms are often based on iterative techniques, and hence, they require a co&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.01302v1-abstract-full').style.display = 'inline'; document.getElementById('2101.01302v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.01302v1-abstract-full" style="display: none;">
        This paper investigates a machine learning-based power allocation design for secure transmission in a cognitive radio (CR) network. In particular, a neural network (NN)-based approach is proposed to maximize the secrecy rate of the secondary receiver under the constraints of total transmit power of secondary transmitter, and the interference leakage to the primary receiver, within which three different regularization schemes are developed. The key advantage of the proposed algorithm over conventional approaches is the capability to solve the power allocation problem with both perfect and imperfect channel state information. In a conventional setting, two completely different <span class="search-hit mathjax">optimization</span> frameworks have to be designed, namely the robust and non-robust designs. Furthermore, conventional algorithms are often based on iterative techniques, and hence, they require a considerable number of iterations, rendering them less suitable in future wireless networks where there are very stringent delay constraints. To meet the unprecedented requirements of future ultra-reliable low-latency networks, we propose an NN-based approach that can determine the power allocation in a CR network with significantly <span class="search-hit mathjax">reduced</span> computational time and complexity. As this trained NN only requires a small number of linear operations to yield the required power allocations, the approach can also be extended to different delay sensitive <span class="search-hit mathjax">applications</span> and services in future wireless networks. When evaluate the proposed method versus conventional approaches, using a suitable test set, the proposed approach can achieve more than 94% of the secrecy rate <span class="search-hit mathjax">performance</span> with less than 1% computation time and more than 93% satisfaction of interference leakage constraints. These results are obtained with significant reduction in computational time, which we believe that it is suitable for future real-time wireless <span class="search-hit mathjax">applications</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.01302v1-abstract-full').style.display = 'none'; document.getElementById('2101.01302v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.00958">arXiv:2101.00958</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.00958">pdf</a>, <a href="https://arxiv.org/format/2101.00958">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scalable Online Conformance Checking Using Incremental Prefix-Alignment Computation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Schuster%2C+D">Daniel Schuster</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kolhof%2C+G+J">Gero J. Kolhof</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.00958v1-abstract-short" style="display: inline;">
        &hellip;We have, therefore, recently introduced a novel approach to compute exact conformance checking results in an online environment. In this paper, we focus on the practical <span class="search-hit mathjax">application</span> and present a scalable, distributed implementation of the proposed online conformance checking approach. Moreover, we present two extensions to said approach to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00958v1-abstract-full').style.display = 'inline'; document.getElementById('2101.00958v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.00958v1-abstract-full" style="display: none;">
        Conformance checking techniques aim to collate observed process behavior with normative/modeled process models. The majority of existing approaches focuses on completed process executions, i.e., offline conformance checking. Recently, novel approaches have been designed to monitor ongoing processes, i.e., online conformance checking. Such techniques detect deviations of an ongoing process execution from a normative process model at the moment they occur. Thereby, countermeasures can be taken immediately to prevent a process deviation from causing further, undesired consequences. Most online approaches only allow to detect approximations of deviations. This causes the problem of falsely detected deviations, i.e., detected deviations that are actually no deviations. We have, therefore, recently introduced a novel approach to compute exact conformance checking results in an online environment. In this paper, we focus on the practical <span class="search-hit mathjax">application</span> and present a scalable, distributed implementation of the proposed online conformance checking approach. Moreover, we present two extensions to said approach to <span class="search-hit mathjax">reduce</span> its computational effort and its practical <span class="search-hit mathjax">applicability</span>. We evaluate our implementation using data sets capturing the execution of real processes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00958v1-abstract-full').style.display = 'none'; document.getElementById('2101.00958v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.00745">arXiv:2101.00745</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.00745">pdf</a>, <a href="https://arxiv.org/format/2101.00745">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DSXplore: <span class="search-hit mathjax">Optimizing</span> Convolutional Neural Networks via Sliding-Channel Convolutions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuke Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+B">Boyuan Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+Y">Yufei Ding</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.00745v1-abstract-short" style="display: inline;">
        As the key advancement of the convolutional neural networks (CNNs), depthwise separable convolutions (DSCs) are becoming one of the most popular techniques to <span class="search-hit mathjax">reduce</span> the computations and parameters size of CNNs meanwhile maintaining the model&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00745v1-abstract-full').style.display = 'inline'; document.getElementById('2101.00745v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.00745v1-abstract-full" style="display: none;">
        As the key advancement of the convolutional neural networks (CNNs), depthwise separable convolutions (DSCs) are becoming one of the most popular techniques to <span class="search-hit mathjax">reduce</span> the computations and parameters size of CNNs meanwhile maintaining the model <span class="search-hit mathjax">accuracy</span>. It also brings profound impact to <span class="search-hit mathjax">improve</span> the <span class="search-hit mathjax">applicability</span> of the compute- and memory-intensive CNNs to a broad range of <span class="search-hit mathjax">applications</span>, such as mobile devices, which are generally short of computation power and memory. However, previous research in DSCs are largely focusing on compositing the limited existing DSC designs, thus, missing the opportunities to explore more potential designs that can achieve better <span class="search-hit mathjax">accuracy</span> and higher computation/parameter reduction. Besides, the off-the-shelf convolution implementations offer limited computing schemes, therefore, lacking support for DSCs with different convolution patterns.
  To this end, we introduce, DSXplore, the first <span class="search-hit mathjax">optimized</span> design for exploring DSCs on CNNs. Specifically, at the algorithm level, DSXplore incorporates a novel factorized kernel -- sliding-channel convolution (SCC), featured with input-channel overlapping to balance the <span class="search-hit mathjax">accuracy</span> <span class="search-hit mathjax">performance</span> and the reduction of computation and memory cost. SCC also offers enormous space for design exploration by introducing adjustable kernel parameters. Further, at the implementation level, we carry out an <span class="search-hit mathjax">optimized</span> GPU-implementation tailored for SCC by leveraging several key techniques, such as the input-centric backward design and the channel-cyclic <span class="search-hit mathjax">optimization</span>. Intensive experiments on different datasets across mainstream CNNs show the advantages of DSXplore in balancing <span class="search-hit mathjax">accuracy</span> and computation/parameter reduction over the standard convolution and the existing DSCs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00745v1-abstract-full').style.display = 'none'; document.getElementById('2101.00745v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.00256">arXiv:2101.00256</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.00256">pdf</a>, <a href="https://arxiv.org/format/2101.00256">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        5G MEC Computation Handoff for Mobile Augmented Reality
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+P">Pengyuan Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Finley%2C+B">Benjamin Finley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xuebing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tarkoma%2C+S">Sasu Tarkoma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kangasharju%2C+J">Jussi Kangasharju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ammar%2C+M">Mostafa Ammar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hui%2C+P">Pan Hui</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.00256v1-abstract-short" style="display: inline;">
        The combination of 5G and Multi-access Edge Computing (MEC) can significantly <span class="search-hit mathjax">reduce</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00256v1-abstract-full').style.display = 'inline'; document.getElementById('2101.00256v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.00256v1-abstract-full" style="display: none;">
        The combination of 5G and Multi-access Edge Computing (MEC) can significantly <span class="search-hit mathjax">reduce</span> <span class="search-hit mathjax">application</span> delay by lowering transmission delay and bringing computational capabilities closer to the end user. Therefore, 5G MEC could enable excellent user experience in <span class="search-hit mathjax">applications</span> like Mobile Augmented Reality (MAR), which are computation-intensive, and delay and jitter-sensitive. However, existing 5G handoff algorithms often do not consider the computational load of MEC servers, are too complex for real-time execution, or do not integrate easily with the standard protocol stack. Thus they can impair the <span class="search-hit mathjax">performance</span> of 5G MEC. To address this gap, we propose Comp-HO, a handoff algorithm that finds a local solution to the joint problem of <span class="search-hit mathjax">optimizing</span> signal strength and computational load. Additionally, Comp-HO can easily be integrated into current LTE and 5G base stations thanks to its simplicity and standard-friendly deployability. Specifically, we evaluate Comp-HO through a custom NS-3 simulator which we calibrate via MAR prototype measurements from a real-world 5G testbed. We simulate both Comp-HO and several classic handoff algorithms. The results show that, even without a global optimum, the proposed algorithm still significantly <span class="search-hit mathjax">reduces</span> the number of large delays, caused by congestion at MECs, at the expense of a small increase in transmission delay.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00256v1-abstract-full').style.display = 'none'; document.getElementById('2101.00256v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to Mobihoc&#39;21</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.00236">arXiv:2101.00236</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.00236">pdf</a>, <a href="https://arxiv.org/format/2101.00236">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On Stochastic Variance <span class="search-hit mathjax">Reduced</span> Gradient Method for Semidefinite <span class="search-hit mathjax">Optimization</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+J">Jinshan Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zha%2C+Y">Yixuan Zha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+K">Ke Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yuan Yao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.00236v1-abstract-short" style="display: inline;">
        The low-rank stochastic semidefinite <span class="search-hit mathjax">optimization</span> has attracted rising attention due to its wide range of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00236v1-abstract-full').style.display = 'inline'; document.getElementById('2101.00236v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.00236v1-abstract-full" style="display: none;">
        The low-rank stochastic semidefinite <span class="search-hit mathjax">optimization</span> has attracted rising attention due to its wide range of <span class="search-hit mathjax">applications</span>. The nonconvex reformulation based on the low-rank factorization, significantly <span class="search-hit mathjax">improves</span> the computational <span class="search-hit mathjax">efficiency</span> but brings some new challenge to the analysis. The stochastic variance <span class="search-hit mathjax">reduced</span> gradient (SVRG) method has been regarded as one of the most <span class="search-hit mathjax">effective</span> methods. SVRG in general consists of two loops, where a reference full gradient is first evaluated in the outer loop and then used to yield a variance <span class="search-hit mathjax">reduced</span> estimate of the current gradient in the inner loop. Two options have been suggested to yield the output of the inner loop, where Option I sets the output as its last iterate, and Option II yields the output via random sampling from all the iterates in the inner loop. However, there is a significant gap between the theory and practice of SVRG when adapted to the stochastic semidefinite <span class="search-hit mathjax">programming</span> (SDP). SVRG practically works better with Option I, while most of existing theoretical results focus on Option II. In this paper, we fill this gap via exploiting a new semi-stochastic variant of the original SVRG with Option I adapted to the semidefinite <span class="search-hit mathjax">optimization</span>. Equipped with this, we establish the global linear submanifold convergence (i.e., converging exponentially fast to a submanifold of a global minimum under the orthogonal group action) of the proposed SVRG method, given a provable initialization scheme and under certain smoothness and restricted strongly convex assumptions. Our analysis includes the <span class="search-hit mathjax">effects</span> of the mini-batch size and update frequency in the inner loop as well as two practical step size strategies, the fixed and stabilized Barzilai-Borwein step sizes. Some numerical results in matrix sensing demonstrate the <span class="search-hit mathjax">efficiency</span> of proposed SVRG method outperforming Option II counterpart as well as others.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00236v1-abstract-full').style.display = 'none'; document.getElementById('2101.00236v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">27 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.00090">arXiv:2101.00090</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.00090">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PHP <span class="search-hit mathjax">code</span> smells in web apps: survival and anomalies
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rio%2C+A">Américo Rio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abreu%2C+F+B+e">Fernando Brito e Abreu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.00090v1-abstract-short" style="display: inline;">
        Context: <span class="search-hit mathjax">Code</span> smells are considered symptoms of poor design, leading to future problems, such as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00090v1-abstract-full').style.display = 'inline'; document.getElementById('2101.00090v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.00090v1-abstract-full" style="display: none;">
        Context: <span class="search-hit mathjax">Code</span> smells are considered symptoms of poor design, leading to future problems, such as <span class="search-hit mathjax">reduced</span> maintainability. Except for anecdotal cases (e. g. <span class="search-hit mathjax">code</span> dropout), a <span class="search-hit mathjax">code</span> smell survives until it gets explicitly refactored or removed. This paper presents a longitudinal study on the survival of <span class="search-hit mathjax">code</span> smells for web apps built with PHP.
  Objectives: RQ: (i) <span class="search-hit mathjax">code</span> smells survival depends on their scope? (ii) practitioners attitudes towards <span class="search-hit mathjax">code</span> smells removal in web apps have changed throughout time? (iii) how long <span class="search-hit mathjax">code</span> smells survive in web <span class="search-hit mathjax">applications</span>? (iv) are there sudden variations (anomalies) in the density of <span class="search-hit mathjax">code</span> smells through the evolution of web apps?
  Method: We analyze the evolution of 6 <span class="search-hit mathjax">code</span> smells in 8 web <span class="search-hit mathjax">applications</span> written in PHP at the server side, across several years, using the survival analysis technique. We classify <span class="search-hit mathjax">code</span> smells according to scope in two categories: scattered and localized. Scattered <span class="search-hit mathjax">code</span> smells are expected to be more harmful since their influence is not circumscribed as in localized <span class="search-hit mathjax">code</span> smells. We split the observations for each web app into two equal and consecutive timeframes, to test the hypothesis that <span class="search-hit mathjax">code</span> smells awareness has increased throughout time. As for the anomalies, we standardize their detection criteria.
  Results: We present some evidence that <span class="search-hit mathjax">code</span> smells survival depends on their scope: the average survival rate decreases in some of them, while the opposite is observed for the remainder. The survival of localized <span class="search-hit mathjax">code</span> smells is around 4 years, while the scattered ones live around 5 years. Around 60% of the smells are removed, and some live through all the <span class="search-hit mathjax">application</span> life. We also show how a graphical representation of anomalies found in the evolution of <span class="search-hit mathjax">code</span> smells allows unveiling the story of a development project and make managers aware of the need for enforcing regular refactoring practices.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00090v1-abstract-full').style.display = 'none'; document.getElementById('2101.00090v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          D.2
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.14968">arXiv:2012.14968</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.14968">pdf</a>, <a href="https://arxiv.org/format/2012.14968">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Optimizing</span> IoT and Web Traffic Using Selective Edge Compression
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Melissaris%2C+T">Themis Melissaris</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shaw%2C+K">Kelly Shaw</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martonosi%2C+M">Margaret Martonosi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.14968v1-abstract-short" style="display: inline;">
        Internet of Things (IoT) devices and <span class="search-hit mathjax">applications</span> are generating and communicating vast quantities of data, and the rate of data collection is increasing rapidly. These high communication volumes are challenging for energy-constrained, data-capped, wireless mobile devices and networked sensors. Compression is commonly used to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.14968v1-abstract-full').style.display = 'inline'; document.getElementById('2012.14968v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.14968v1-abstract-full" style="display: none;">
        Internet of Things (IoT) devices and <span class="search-hit mathjax">applications</span> are generating and communicating vast quantities of data, and the rate of data collection is increasing rapidly. These high communication volumes are challenging for energy-constrained, data-capped, wireless mobile devices and networked sensors. Compression is commonly used to <span class="search-hit mathjax">reduce</span> web traffic, to save energy, and to make network transfers faster. If not used judiciously, however, compression can hurt <span class="search-hit mathjax">performance</span>. This work proposes and evaluates mechanisms that employ selective compression at the network&#39;s edge, based on data characteristics and network conditions. This approach (i) <span class="search-hit mathjax">improves</span> the <span class="search-hit mathjax">performance</span> of network transfers in IoT environments, while (ii) providing significant data savings. We demonstrate that our library speeds up web transfers by an average of 2.18x and 2.03x under fixed and dynamically changing network conditions respectively. Furthermore, it also provides consistent data savings, compacting data down to 19% of the original data size.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.14968v1-abstract-full').style.display = 'none'; document.getElementById('2012.14968v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.14635">arXiv:2012.14635</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.14635">pdf</a>, <a href="https://arxiv.org/format/2012.14635">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sensifi: A Wireless Sensing System for Ultra-High-Rate <span class="search-hit mathjax">Applications</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chia-Chi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramanna%2C+V+K">Vikram K. Ramanna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Webber%2C+D">Daniel Webber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hunter%2C+C">Cole Hunter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hack%2C+T">Tyler Hack</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dezfouli%2C+B">Behnam Dezfouli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.14635v2-abstract-short" style="display: inline;">
        Wireless Sensor Networks (WSNs) are being used in various <span class="search-hit mathjax">applications</span> such as structural health monitoring and industrial control. Since energy&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.14635v2-abstract-full').style.display = 'inline'; document.getElementById('2012.14635v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.14635v2-abstract-full" style="display: none;">
        Wireless Sensor Networks (WSNs) are being used in various <span class="search-hit mathjax">applications</span> such as structural health monitoring and industrial control. Since energy <span class="search-hit mathjax">efficiency</span> is one of the major design factors, the existing WSNs primarily rely on low-power, low-rate wireless technologies such as 802.15.4 and Bluetooth. In this paper, we strive to tackle the challenges of developing ultra-high-rate WSNs based on 802.11 (WiFi) standard by proposing Sensifi. As an illustrative <span class="search-hit mathjax">application</span> of this system, we consider vibration test monitoring of spacecraft and identify system design requirements and challenges. Our main contributions are as follows. First, we propose packet encoding methods to <span class="search-hit mathjax">reduce</span> the overhead of assigning accurate timestamps to samples. Second, we propose energy <span class="search-hit mathjax">efficiency</span> methods to enhance the system&#39;s lifetime. Third, we <span class="search-hit mathjax">reduce</span> the overhead of processing outgoing packets through network stack to enhance sampling rate and mitigate sampling rate instability. Fourth, we study and <span class="search-hit mathjax">reduce</span> the delay of processing incoming packets through network stack to enhance the <span class="search-hit mathjax">accuracy</span> of time synchronization among nodes. Fifth, we propose a low-power node design for ultra-high-rate <span class="search-hit mathjax">applications</span>. Sixth, we use our node design to empirically evaluate the system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.14635v2-abstract-full').style.display = 'none'; document.getElementById('2012.14635v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 December, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          SIOTLAB-2020-12-DEC
        

        

        
      </p>
    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=200"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=200"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=400"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=600"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%28code+OR+program+OR+software+OR+application%29+AND+%28optimize+OR+optimizing+OR+optimization+OR+improve+OR+improving+OR+improvement+OR+automated+OR+automatically+OR+reduce+OR+reducing%29+AND+%28performance+OR+efficient+OR+efficiency+OR+effective+OR+effectiveness+OR+accuracy+OR+precision%29&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=200&amp;order=-announced_date_first&amp;start=800"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>