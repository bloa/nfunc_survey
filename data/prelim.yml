
- title: Implementing Quicksort programs
  url: https://dl.acm.org/doi/10.1145/359619.359631
  abstract: "This paper is a practical study of how to implement the Quicksort sorting algorithm and its best variants on real computers, including how to apply various code optimization techniques. A detailed implementation combining the most effective improvements to Quicksort is given, along with a discussion of how to implement it in assembly language. Analytic results describing the performance of the programs are summarized. A variety of special situations are considered from a practical standpoint to illustrate Quicksort's wide applicability as an internal sorting method which requires negligible extra storage."

- title: Predicting the effects of optimization on a procedure body
  url: https://dl.acm.org/doi/10.1145/800229.806972
  abstract: "Practical application of procedure integration (inline expansion) as a program optimization requires some method for estimating the effects of subsequent optimization on the integrated procedure. A technique is described for predicting the code improvement that can be expected due to constant folding and test elision when a procedure call involving constant actual parameters is integrated. The technique is based on information collected during a single data flow analysis of each procedure body, and on execution frequency statistics for the procedure. This information can then be used to estimate the cost in code size and the benefit in execution speed of integrating each call to the procedure. The algorithm uses a syntax directed analysis technique suitable for structured programming languages without gotos."

- title: Program optimization for a pipelined machine a case study
  url: https://dl.acm.org/doi/10.1145/1031382.809316
  abstract: "The Amdahl 580 processor is a pipelined processor whose performance can be affected by characteristics of the instructions it executes. This paper describes certain optimizations made to a set of system software routines during their development. The optimization effort was driven by the execution frequencies of common paths through the programs in question, and by the execution characteristics of those paths, as shown by a processor simulator. Path optimization itself was done with both general program optimization techniques and with techniques specific to the particular characteristics of the 580's pipeline. Overall, the average execution time for these routines was reduced by over 50%."

- title: Restructuring Lisp programs for concurrent execution
  url: https://dl.acm.org/doi/10.1145/62115.62126
  abstract: "This paper describes the techniques that the program transformation system CURARE uses to restructure Lisp programs for concurrent execution in multiprocessor Lisp systems and discusses the problems inherent in producing concurrent programs in a flexible and dynamic programming language such as Lisp.

CURARE's overall organization is similar to other program restructuring systems: it detects potential conflicts between statements in a program, then transforms the program to improve its concurrent performance, and finally inserts synchronization to ensure the program's concurrent behavior. However, the language and programs that CURARE transforms are very different from the FORTRAN programs that are the traditional targets of program restructuring and so CURARE requires new algorithms and approaches, which are described in this paper."

- title: Vectorization techniques for prolog
  url: https://dl.acm.org/doi/10.1145/55364.55417
  abstract: "Several techniques for running Prolog programs on pipelined vector processors, such as the Hitachi S-820 or the Cray-2, are developed. This paper presents an automatic program transformation (vectorization) method of Prolog, which enables a type of or-parallel execution of Prolog programs using vector operations. Performance is evaluated on the Hitachi S-810 using the Eight-Queens problem. Its vector execution speed is 4.5 MLIPS (18 ms). This is eight or nine times faster than scalar execution. This result confirms the effectiveness of vectorization techniques and applicability of vector processors to Prolog execution and to symbol processing applications."

- title: The effect of compiler-flag tuning on SPEC benchmark performance
  url: https://dl.acm.org/doi/10.1145/190787.190807
  abstract: "The SPEC CINT92 and CFP92 benchmark suites are application-based system benchmarks primarily intended for workstation-class system performance measurements. The SPEC CPU benchmark results are widely disseminated by system vendors and as such have become the de-facto standard for comparing system performance. Recently, many observers have expressed concerns about the suitability of published SPEC benchmark results in representing application performance on typical systems. The most outspoken concern is that there is too much freedom permitted in the manipulation of compiler flags. This has resulted in revisions to the SPEC reporting procedure.This paper presents and discusses many of the issues concerning the tuning of benchmarks through manipulation of compiler flags. We attempt to quantify the impact of these procedures through controlled experiments. Baseline performance results, using a set of uniform, common optimizations are compared to published data. Further experiments measure the performance of the SPEC benchmarks in the other common usage scenarios. These are a centralized file storage configuration and a system using common binaries among several implementations of the same architecture. Despite the great concern over the use of compiler flags in the SPEC community, our experiments show only a modest impact on performance. The more significant performance differential shown in the other experiments draws into question the utility of current SPEC data to many users."

- title: Source code optimization and profiling of energy consumption in embedded systems
  url: https://ieeexplore.ieee.org/document/874049
  abstract: "This paper presents a source code optimization methodology and a profiling tool that have been developed to help designers in optimizing software performance and energy in embedded systems. Code optimizations are applied at three levels of abstraction: algorithmic, data and instruction-level. The profiler exploits a cycle-accurate energy consumption simulator (Simunic et al., 1999) to relate the embedded system energy consumption and performance to the source code. Thus, it can be used for analysis (i.e., to find energy-critical sections of the code), and for validation (i.e., to assess the impact of each code optimization). Code optimizations and the profiling tool are used to optimize and tune the implementation of an MPEG Layer III (MP3) audio decoder for the SmartBadge (Maguire et al., 1998) portable embedded system. We show that using our methodology and tool we can quickly and easily re-design the MP3 audio decoder software to run in real time with low energy consumption decrease of 77% (over the original executable specification) has been achieved for MP3 audio decoding on the SmartBadge."

- title: Source code transformation based on software cost analysis
  url: https://dl.acm.org/doi/10.1145/500001.500036
  abstract: "This paper presents a model and a strategy for source-code transformation applied to software application programs to reduce their energy cost. We propose a flexible performance and energy model for a processor-memory system. The benefit of the model is generality (it is not tied to a single memory and processor architecture) and effectiveness of evaluation. With this model, we first estimate the effects of source-code transformations (called transformation cost), representing the improvement ratios of processor cycles, I-cache misses, and D-cache misses. Next, we combine the transformation cost model with hardware parameters to estimate the actual effect of a transformation on performance and energy. The model can be used to guide software transformation selection for power and performance. The experimental results show that the proposed approach finds the optimal transformation in 95% of the cases, and that the penalty when the non-optimal transformation is selected is within 5%."

- title: Evolutionary programming to optimize an assembly program
  url: https://ieeexplore.ieee.org/document/1004533
  abstract: "Evolutionary programming was used to attempt to optimize a program written in the pseudo-assembly language Redcode, invented by A.K. Dewdney. Corewars is the game under which Redcode programs compete. Since 1994, the last standardization of Redcode, many complicated, effective Redcode programs have been written by people, but intense study is required to learn the nuances of the language and perfect programs. Since this is such a difficult task, evolutionary techniques may outperform humans. Multiple point, variable length crossover and change, insert, and delete mutations were the operators used. Relative fitnesses were calculated within a subset of the population on remote client computers. A food model was used to select the most fit programs. Current results are preliminary, but already one of the resulting programs wins 38% and ties 29% against a common type of human-written program. The best performance is 151 wins, 49 losses, and 0 ties against a typical human program."

- title: "Classbox/J: controlling the scope of change in Java"
  url: https://dl.acm.org/doi/10.1145/1103845.1094826
  abstract: "Unanticipated changes to complex software systems can introduce anomalies such as duplicated code, suboptimal inheritance relationships and a proliferation of run-time downcasts. Refactoring to eliminate these anomalies may not be an option, at least in certain stages of software evolution. Classboxes are modules that restrict the visibility of changes to selected clients only, thereby offering more freedom in the way unanticipated changes may be implemented, and thus reducing the need for convoluted design anomalies. In this paper we demonstrate how classboxes can be implemented in statically-typed languages like Java. We also present an extended case study of Swing, a Java GUI package built on top of AWT, and we document the ensuing anomalies that Swing introduces. We show how Classbox/J, a prototype implementation of classboxes for Java, is used to provide a cleaner implementation of Swing using local refinement rather than subclassing."

- title: On the impact of data input sets on statistical compiler tuning
  url: https://ieeexplore.ieee.org/document/1639724/
  abstract: "In recent years, several approaches have been proposed to use profile information in compiler optimization. This profile information can be used at the source level to guide loop transformations as well as in the backend to guide low level optimizations. At the same time, profile guided library generators have been proposed also, like Atlas, Spiral, or FFTW, that tune their routines for the underlying hardware. These approaches have led to excellent performance improvements. However, a possible drawback of these approaches is that applications are optimized using a single or a limited set of data inputs. It is well known that programs can exhibit vastly differing behaviors for different inputs. Therefore, it is not clear whether the performance numbers reported are still valid for other input than the input used to optimize the program. In this paper, we address this problem for a specific statistical compiler tuning method. We use three different platforms and several SPECint2000 benchmarks. We show that when we tune the compiler using train data, we obtain a compiler setting that still performs well for reference data. These results suggest that profile guided optimization may be more stable than is sometimes believed and that a limited number of train data sets are sufficient to obtain a well optimized program for all inputs"

- title: "VISTA: VPO interactive system for tuning applications"
  url: https://dl.acm.org/doi/10.1145/1196636.1196640
  abstract: "Software designers face many challenges when developing applications for embedded systems. One major challenge is meeting the conflicting constraints of speed, code size, and power consumption. Embedded application developers often resort to hand-coded assembly language to meet these constraints since traditional optimizing compiler technology is usually of little help in addressing this challenge. The results are software systems that are not portable, less robust, and more costly to develop and maintain. Another limitation is that compilers traditionally apply the optimizations to a program in a fixed order. However, it has long been known that a single ordering of optimization phases will not produce the best code for every application. In fact, the smallest unit of compilation in most compilers is typically a function and the programmer has no control over the code improvement process other than setting flags to enable or disable certain optimization phases. This paper describes a new code improvement paradigm implemented in a system called VISTA that can help achieve the cost/performance trade-offs that embedded applications demand. The VISTA system opens the code improvement process and gives the application programmer, when necessary, the ability to finely control it. VISTA also provides support for finding effective sequences of optimization phases. This support includes the ability to interactively get static and dynamic performance information, which can be used by the developer to steer the code improvement process. This performance information is also internally used by VISTA for automatically selecting the best optimization sequence from several attempted. One such feature is the use of a genetic algorithm to search for the most efficient sequence based on specified fitness criteria. We include a number of experimental results that evaluate the effectiveness of using a genetic algorithm in VISTA to find effective optimization phase sequences."

- title: Evolving a CUDA Kernel from an nVidia Template
  url: https://ieeexplore.ieee.org/document/5585922
  abstract: "Rather than attempting to evolve a complete program from scratch we demonstrate genetic interface programming (GIP) by automatically generating a parallel CUDA kernel with identical functionality to existing highly optimised ancient sequential C code (gzip). Generic GPGPU nVidia kernel C++ code is converted into a BNF grammar. Strongly typed genetic programming uses the BNF to generate compilable and executable graphics card kernels. Their fitness is given by running the population on a GPU with randomised subsets of training data itself derived from gzip's SIR test suite. Back-to-back validation uses the original code as a test oracle."

- title: Automated just-in-time compiler tuning
  url: https://dl.acm.org/doi/10.1145/1772954.1772965
  abstract: "Managed runtime systems, such as a Java virtual machine (JVM), are complex pieces of software with many interacting components. The Just-In-Time (JIT) compiler is at the core of the virtual machine, however, tuning the compiler for optimum performance is a challenging task. There are (i) many compiler optimizations and options, (ii) there may be multiple optimization levels (e.g., -O0, -O1, -O2), each with a specific optimization plan consisting of a collection of optimizations, (iii) the Adaptive Optimization System (AOS) that decides which method to optimize to which optimization level requires fine-tuning, and (iv) the effectiveness of the optimizations depends on the application as well as on the hardware platform. Current practice is to manually tune the JIT compiler which is both tedious and very time-consuming, and in addition may lead to suboptimal performance.
This paper proposes automated tuning of the JIT compiler through multi-objective evolutionary search. The proposed framework (i) identifies optimization plans that are Pareto-optimal in terms of compilation time and code quality, (ii) assigns these plans to optimization levels, and (iii) fine-tunes the AOS accordingly. The key benefit of our framework is that it automates the entire exploration process, which enables tuning the JIT compiler for a given hardware platform and/or application at very low cost.
By automatically tuning Jikes RVM using our framework for average performance across the DaCapo and SPECjvm98 benchmark suites, we achieve similar performance to the hand-tuned default Jikes RVM. When optimizing the JIT compiler for individual benchmarks, we achieve statistically significant speedups for most benchmarks, up to 40% for startup and up to 19% for steady-state performance. We also show that tuning the JIT compiler for a new hardware platform can yield significantly better performance compared to using a JIT compiler that was tuned for another platform."

- title: Evolutionary Improvement of Programs
  url: https://ieeexplore.ieee.org/document/5688317
  abstract: "Most applications of genetic programming (GP) involve the creation of an entirely new function, program or expression to solve a specific problem. In this paper, we propose a new approach that applies GP to improve existing software by optimizing its non-functional properties such as execution time, memory usage, or power consumption. In general, satisfying non-functional requirements is a difficult task and often achieved in part by optimizing compilers. However, modern compilers are in general not always able to produce semantically equivalent alternatives that optimize non-functional properties, even if such alternatives are known to exist: this is usually due to the limited local nature of such optimizations. In this paper, we discuss how best to combine and extend the existing evolutionary methods of GP, multiobjective optimization, and coevolution in order to improve existing software. Given as input the implementation of a function, we attempt to evolve a semantically equivalent version, in this case optimized to reduce execution time subject to a given probability distribution of inputs. We demonstrate that our framework is able to produce non-obvious optimizations that compilers are not yet able to generate on eight example functions. We employ a coevolved population of test cases to encourage the preservation of the function's semantics. We exploit the original program both through seeding of the population in order to focus the search, and as an oracle for testing purposes. As well as discussing the issues that arise when attempting to improve software, we employ rigorous experimental method to provide interesting and practical insights to suggest how to address these issues."

- title: "High Performance I/O"
  url: https://ieeexplore.ieee.org/document/5739034
  abstract: "Parallelisation, serial optimisation, compiler tuning, and many more techniques are used to optimise and improve the performance scaling of parallel programs. One area which is frequently not optimised is file I/O. This is because it is often not considered to be key to the performance of a program and also because it is traditionally difficult to optimise and very machine specific. However, in the current era of Peta- and Exascale computing it is no longer possible to ignore I/O performance as it can significantly limit the scaling of many codes when executing on very large numbers of processors or cores. Furthermore, as producing data is the main purpose of most simulation codes any work that can be undertaken to provide improved performance of I/0 can be applicable to a very large range of simulation codes, and provide them with improved functionality (i.e. the ability to produce more data).This paper describes some of the issues surrounding I/O, the technology that is commonly deployed to provide I/O on HPC machines and the software libraries available to programmers to undertake I/O. The performance of all these aspects of I/O on a range of HPC systems were investigated by the authors and a represented in this paper to motivate the discussions in the paper."

- title: Effects of loop unrolling and use of instruction buffer on processor energy consumption
  url: https://ieeexplore.ieee.org/document/6089224
  abstract: "In the area of Embedded Systems, instruction memories are one of the critical components consuming significant amounts of energy. Existence of a relation between size of the compiled program, and consequently required size of the instruction memory, and the compiler optimization flags is well-known. In particular, loop transformations such as loop unrolling, while having potential to increase performance dramatically, often cause unreasonable growth in the size of the required instruction memory, causing loss of benefit of lower cycle count from overall system energy point of view. One method how to decrease energy consumption of the memories is use of instruction buffers. Often executed loops are stored in the buffer and executed from there, while main memory is not read. In this paper, we show how the compiler flag, controlling loop unrolling, influences the structure of the loops in the program. While unrolling improves performance, unrolled loops can disappear from the program completely, or grow to unreasonable size where use of instruction buffer brings no benefits from the energy point of view."

- title: An Optimized Tuning of Genetic Algorithm Parameters in Compiler Flag Selection Based on Compilation and Execution Duration
  url: https://link.springer.com/chapter/10.1007%2F978-81-322-0491-6_55
  abstract: "Compiler flags exist to provide option for the software developer to dictate certain parameter to the compiler. Such parameters provide hints to the compiler on how to handle certain portion of the source code. In the realm of optimization, compiler flags provide the fastest way to speed up a program. The right combination of flags will provide significant enhancement in speed without compromising the integrity of the output. However, the main challenge is choosing that particular right set of flags. Many a times, developers work around this issue by dictating the optimization level. In that way, the compiler imposes a package of flags. This process may lead to degradation of performance in terms of execution speed and also significant increase in program size. In this work, we are studying the usage of Genetic Algorithm as a way to select the optimization flags that could produce codes which compile and execute fast."

- title: Performance profile of some hybrid heuristic search techniques using compiler flag selection as a seed example
  url: https://ieeexplore.ieee.org/document/6256576
  abstract: "The availability of different flavor of processor architecture coupled with computer codes of various nature poses a discreet challenge to the programmers in forms of code optimization. Programmers need to contemplate on optimization during pre and post implementation to take advantage of the hardware given for a specific nature of the code. To compliment this requirement, the evolution of compiler technology has resulted in built in optimization functionality called compiler flags. Like a switch the flag turns on or off for a particular optimization behavior. The existence of various flags in turn causes confusion as to which flag or combination of flags to be utilized since misuse has detrimental effect on performance. In this work we are performing a comparative study on the utilization of Genetic Algorithm and Simulated Annealing in finding the best compiler flag combination respectively and finally proposing a hybrid algorithm that produces better flag combination in comparison to the former two."

- title: Exploiting slicing and patterns for RTSJ immortal memory optimization
  url: https://dl.acm.org/doi/abs/10.1145/2500828.2500845
  abstract: "The Real-Time Specification for Java (RTSJ) introduces a new memory management model which avoids interfering with the garbage collection process. Two types of memory areas are provided - immortal and scoped. Using this memory management model is not straightforward, since many issues need to be considered. A developer has to decide on the objects that will be allocated in immortal or scoped memory areas and on the memory space required at runtime. On the other hand, reference checks between memory areas constrain the design of the real-time applications and increase the complexity of the development process. In previous work, a new RTSJ case study was implemented and discussed using different scoped memory models. The case study showed the complexity of using the new RTSJ memory model and the space overhead incurred by immortal memory. In this paper, dynamic code slicing is employed as a debugging technique to explore constant increases in immortal memory. Two programming design patterns are presented for decreasing immortal memory overheads generated by specific data structures. Experimental results showed a significant decrease in immortal memory consumption at runtime. The work therefore provides a new approach for assisting developers in debugging and optimising scoped and immortal memory implementation"

- title: Applying Genetic Improvement to MiniSAT
  url: https://link.springer.com/chapter/10.1007/978-3-642-39742-4_21
  abstract: "Genetic Programming (GP) has long been applied to several SBSE problems. Recently there has been much interest in using GP and its variants to solve demanding problems in which the code evolved by GP is intended for deployment. This paper investigates the application of genetic improvement to a challenging problem of improving a well-studied system: a Boolean satisfiability (SAT) solver called MiniSAT. Many programmers have tried to make this very popular solver even faster and a separate SAT competition track has been created to facilitate this goal. Thus genetically improving MiniSAT poses a great challenge. Moreover, due to a wide range of applications of SAT solving technologies any improvement could have a great impact. Our initial results show that there is some room for improvement. However, a significantly more efficient version of MiniSAT is yet to be discovered."

- title: Improving 3D medical image registration CUDA software with genetic programming
  url: https://dl.acm.org/doi/10.1145/2576768.2598244
  abstract: "Genetic Improvement (GI) is shown to optimise, in some cases by more than 35percent, a critical component of healthcare industry software across a diverse range of six nVidia graphics processing units (GPUs). GP and other search based software engineering techniques can automatically optimise the current rate limiting CUDA parallel function in the NiftyReg open source C++ project used to align or register high resolution nuclear magnetic resonance NMRI and other diagnostic NIfTI images. Future Neurosurgery techniques will require hardware acceleration, such as GPGPU, to enable real time comparison of three dimensional in theatre images with earlier patient images and reference data. With millimetre resolution brain scan measurements comprising more than ten million voxels the modified kernel can process in excess of 3 billion active voxels per second."

- title: How do code refactorings affect energy usage?
  url: https://dl.acm.org/doi/10.1145/2652524.2652538
  abstract: "Context: Code refactoring's benefits to understandability, maintainability and extensibility are well known enough that automated support for refactoring is now common in IDEs. However, the decision to apply such transformations is currently performed without regard to the impacts of the refactorings on energy consumption. This is primarily due to a lack of information and tools to provide such relevant information to developers. Unfortunately, concerns about energy efficiency are rapidly becoming a high priority concern in many environments, including embedded systems, laptops, mobile devices, and data centers.
Goal: We aim to address the lack of information about the energy efficiency impacts of code refactorings.
Method: We conducted an empirical study to investigate the energy impacts of 197 applications of 6 commonly-used refactorings.
Results: We found that refactorings can not only impact energy usage but can also increase and decrease the amount of energy used by an application. In addition, we also show that metrics commonly believed to correlate with energy usage are unlikely to be able to fully predict the impact of applying a refactoring.
Conclusion: The results from this and similar studies could be used to augment IDEs to help software developers build more energy efficient software."

- title: Playing regex golf with genetic programming
  url: https://dl.acm.org/doi/10.1145/2576768.2598333
  abstract: "Regex golf has recently emerged as a specific kind of code golf, i.e., unstructured and informal programming competitions aimed at writing the shortest code solving a particular problem. A problem in regex golf consists in writing the shortest regular expression which matches all the strings in a given list and does not match any of the strings in another given list. The regular expression is expected to follow the syntax of a specified programming language, e.g., Javascript or PHP. In this paper, we propose a regex golf player internally based on Genetic Programming. We generate a population of candidate regular expressions represented as trees and evolve such population based on a multi-objective fitness which minimizes the errors and the length of the regular expression. We assess experimentally our player on a popular regex golf challenge consisting of 16 problems and compare our results against those of a recently proposed algorithm---the only one we are aware of.Our player obtains scores which improve over the baseline and are highly competitive also with respect to human players. The time for generating a solution is usually in the order of tens minutes, which is arguably comparable to the time required by human players."

- title: Refactoring Java Concurrent Programs Based on Synchronization Requirement Analysis
  url: https://ieeexplore.ieee.org/document/6976102/
  abstract: "Writing high quality concurrent programs is challenging. A concurrent program that is not well-written may suffer from coarse synchronization problems, e.g., overly-large critical sections, overly-coarse locks, and etc. These coarse synchronizations may introduce unnecessary lock contention and thereby affect the parallel execution of running threads. To optimize them, people suggest use refactorings, e.g., Split Lock refactoring and Split Critical Section refactoring, to gradually evolve the synchronization code for better parallelism. However, manually identifying the refactoring opportunities is difficult and by-hand code transformations are error-prone. To reduce the manual efforts, this paper proposes an automated refactoring approach for Java concurrent programs based on synchronization requirement analysis. It can automatically analyze the existing synchronization code to identify synchronization requirements. Bases on these requirements, we can find Split Lock, Split Critical Section, and Convert to Atomic refactoring opportunities and then make proper code transformation for each of them. Our experiment shows that the approach does find effective refactoring opportunities in real projects and can transform the refactorable code correctly. This indicates the approach could be helpful for concurrent program evolution."

- title: Exploiting GPU Hardware Saturation for Fast Compiler Optimization
  url: https://dl.acm.org/doi/10.1145/2588768.2576791
  abstract: "Graphics Processing Units (GPUs) are efficient devices capable of delivering high performance for general purpose computation. Realizing their full performance potential often requires extensive compiler tuning. This process is particularly expensive since it has to be repeated for each target program and platform.
In this paper we study the utilization of GPU hardware resources across multiple input sizes and compiler options. In this context we introduce the notion of hardware saturation. Saturation is reached when an application is executed with a number of threads large enough to fully utilize the available hardware resources. We give experimental evidence of hardware saturation and describe its properties using 16 OpenCL kernels on 3 GPUs from Nvidia and AMD. We show that input sizes that saturates the GPU show performance stability across compiler transformations.
Using the thread-coarsening transformation as an example, we show that compiler settings maintain their relative performance across input sizes within the saturation region. Leveraging these hardware and software properties we propose a technique to identify the input size at the lower bound of the saturation zone, we call it Minimum Saturation Point (MSP). By performing iterative compilation on the MSP input size we obtain results effectively applicable for much large input problems reducing the overhead of tuning by an order of magnitude on average."

- title: Execution profile driven speedup estimation for porting sequential code to GPU
  url: https://dl.acm.org/doi/10.1145/2675744.2675767
  abstract: "Parallelization of an existing sequential application to achieve a good speed-up on a data-parallel infrastructure is quite difficult and time consuming effort. One of the important steps towards this is to assess whether the existing application in its current form can be parallelized to get the desired speedup. In this paper, we propose a method of analyzing an existing sequential source code that contains data-parallel loops, and give a reasonably accurate prediction of the extent of speedup possible from this algorithm. The proposed method performs static and dynamic analysis of the sequential source code to determine the time required by various portions of the code, including the data-parallel portions. Subsequently, it uses a set of novel invariants to calculate various bottlenecks that exists if the program is to be transferred to a GPGPU platform and predicts the extent of parallelization necessary by the GPU in order to achieve the desired end-to-end speedup. Our approach does not require creation of GPU code skeletons of the data parallel portions in the sequential code, thereby reducing the performance prediction effort. We observed a reasonably accurate speedup prediction when we tested our approach on multiple well-known Rodinia benchmark applications, a popular matrix multiplication program and a fast Walsh transform program."

- title: Using Genetic Improvement and Code Transplants to Specialise a C++ Program to a Problem Class
  url: https://link.springer.com/chapter/10.1007/978-3-662-44303-3_12
  abstract: "Genetic Improvement (GI) is a form of Genetic Programming that improves an existing program. We use GI to evolve a faster version of a C++ program, a Boolean satisfiability (SAT) solver called MiniSAT, specialising it for a particular problem class, namely Combinatorial Interaction Testing (CIT), using automated code transplantation. Our GI-evolved solver achieves overall 17% improvement, making it comparable with average expert human performance. Additionally, this automatically evolved solver is faster than any of the human-improved solvers for the CIT problem."

- title: Reducing Energy Consumption Using Genetic Improvement
  url: https://dl.acm.org/doi/10.1145/2739480.2754752
  abstract: "Genetic Improvement (GI) is an area of Search Based Software Engineering which seeks to improve software's non-functional properties by treating program code as if it were genetic material which is then evolved to produce more optimal solutions. Hitherto, the majority of focus has been on optimising program's execution time which, though important, is only one of many non-functional targets. The growth in mobile computing, cloud computing infrastructure, and ecological concerns are forcing developers to focus on the energy their software consumes. We report on investigations into using GI to automatically find more energy efficient versions of the MiniSAT Boolean satisfiability solver when specialising for three downstream applications. Our results find that GI can successfully be used to reduce energy consumption by up to 25%"

- title: "locoGP: Improving Performance by Genetic Programming Java Source Code"
  url: https://dl.acm.org/doi/10.1145/2739482.2768419
  abstract: "We present locoGP, a Genetic Programming (GP) system written in Java for evolving Java source code. locoGP was designed to improve the performance of programs as measured in the number of operations executed. Variable test cases are used to maintain functional correctness during evolution. The operation of locoGP is demonstrated on a number of typically constructed \"off-the-shelf\" hand-written implementations of sort and prefix-code programs. locoGP was able to find improvement opportunities in all test problems."

- title: Optimizing energy of HTTP requests in Android applications
  url: https://dl.acm.org/doi/10.1145/2804345.2804351
  abstract: "Energy is important for mobile apps. Among all operations of mobile apps, making HTTP requests is one of the most energy consuming. However, there is not sufficient work in optimizing the energy consumption of HTTP requests in mobile apps. In our previous study, we found that making small HTTP requests was not energy efficient. Yet, we did not study how to optimize the energy of HTTP requests. In this paper, we make a preliminary study to bundle sequential HTTP requests with a proxy server. With our technique, we had a 50% energy saving for HTTP requests in two market Android apps. This result indicates that our technique is promising and we will build on the result in our future work."

- title: Improving CUDA DNA Analysis Software with Genetic Programming
  url: https://dl.acm.org/doi/10.1145/2739480.2754652
  abstract: "We genetically improve BarraCUDA using a BNF grammar incorporating C scoping rules with GP. Barracuda maps next generation DNA sequences to the human genome using the Burrows-Wheeler algorithm (BWA) on nVidia Tesla parallel graphics hardware (GPUs). GI using phenotypic tabu search with manually grown code can graft new features giving more than 100 fold speed up on a performance critical kernel without loss of accuracy."

- title: Grow and Graft a Better CUDA pknotsRG for RNA Pseudoknot Free Energy Calculation
  url: https://dl.acm.org/doi/abs/10.1145/2739482.2768418
  abstract: "Grow and graft genetic programming greatly improves GPGPU dynamic programming software for predicting the minimum binding energy for folding of RNA molecules. The parallel code inserted into the existing CUDA version of pknots was grown using a BNF grammar. On an nVidia Tesla K40 GPU GGGP gives a speed up of up to 10000 times."

- title: Embedding Adaptivity in Software Systems using the ECSELR framework
  url: https://dl.acm.org/doi/10.1145/2739482.2768425
  abstract: "ECSELR is an ecologically-inspired approach to software evolution that enables environmentally driven evolution at runtime in extant software systems without relying on any offline components or management. ECSELR embeds adaptation and evolution inside the target software system enabling the system to transform itself via darwinian evolutionary mechanisms and adapt in a self contained manner. This allows the software system to benefit autonomously from the useful emergent byproducts of evolution like adaptivity and bio-diversity, avoiding the problems involved in engineering and maintaining such properties. ECSELR enables software systems to address changing environments at runtime, ensuring benefits like mitigation of attacks and memory-optimization among others while avoiding time consuming and costly maintenance and downtime. ECSELR differs from existing work in that, 1) adaptation is embedded in the target system, 2) evolution and adaptation happens online(i.e. in-situ at runtime) and 3) ECSELR is able to embed adaptation inside systems that have already been started and are in the midst of execution. We demonstrate the use of ECSELR and present results on using the ECSELR framework to slim a software system."

- title: Deep Parameter Optimisation
  url: https://dl.acm.org/doi/10.1145/2739480.2754648
  abstract: "We introduce a mutation-based approach to automatically discover and expose 'deep' (previously unavailable) parameters that affect a program's runtime costs. These discovered parameters, together with existing ('shallow') parameters, form a search space that we tune using search-based optimisation in a bi-objective formulation that optimises both time and memory consumption. We implemented our approach and evaluated it on four real-world programs. The results show that we can improve execution time by 12% or achieve a 21% memory consumption reduction in the best cases. In three subjects, our deep parameter tuning results in a significant improvement over the baseline of shallow parameter tuning, demonstrating the potential value of our deep parameter extraction approach."

- title: Effects of source-code optimizations on GPU performance and energy consumption
  url: https://dl.acm.org/doi/10.1145/2716282.2716292
  abstract: "This paper studies the effects of source-code optimizations on the performance, power draw, and energy consumption of a modern compute GPU. We evaluate 128 versions of two n-body codes: a compute-bound regular implementation and a memory-bound irregular implementation. Both programs include six optimizations that can be individually enabled or disabled. We measured the active runtime and the power consumption of each code version on three inputs, various GPU clock frequencies, two arithmetic precisions, and with and without ECC. This paper investigates which optimizations primarily improve energy efficiency, which ones mainly boost performance, and which ones help both aspects. Some optimizations also have the added benefit of reducing the power draw. Our analysis shows that individual and combinations of optimizations can alter the performance and energy consumption of a GPU kernel by up to a factor of five."

- title: Optimizing Existing Software With Genetic Programming
  url: https://ieeexplore.ieee.org/document/6733370
  abstract: "We show that the genetic improvement of programs (GIP) can scale by evolving increased performance in a widely-used and highly complex 50000 line system. Genetic improvement of software for multiple objective exploration (GISMOE) found code that is 70 times faster (on average) and yet is at least as good functionally. Indeed, it even gives a small semantic gain."

- title: Object-Oriented Genetic Improvement for Improved Energy Consumption in Google Guava
  url: https://link.springer.com/chapter/10.1007/978-3-319-22183-0_20
  abstract: "In this work we use metaheuristic search to improve Google’s Guava library, finding a semantically equivalent version of com.google.common.collect.ImmutableMultimap with reduced energy consumption. Semantics-preserving transformations are found in the source code, using the principle of subtype polymorphism. We introduce a new tool, Opacitor, to deterministically measure the energy consumption, and find that a statistically significant reduction to Guava’s energy consumption is possible. We corroborate these results using Jalen, and evaluate the performance of the metaheuristic search compared to an exhaustive search—finding that the same result is achieved while requiring almost 200 times fewer fitness evaluations. Finally, we compare the metaheuristic search to an independent exhaustive search at each variation point, finding that the metaheuristic has superior performance."

- title: "Genetic Programming: From Design to Improved Implementation"
  url: https://dl.acm.org/doi/10.1145/2908961.2931693
  abstract: "Genetic programming (GP) is an evolutionary-based search paradigm that is well suited to automatically solve difficult design problems. The general principles of GP have been used to evolve mathematical functions, models, image operators, programs, and even antennas and lenses. Since GP evolves the syntax and structure of a solution, the evolutionary process can be carried out in one environment and the solution can then be ported to another. However, given the nature of GP it is common that the evolved designs are unorthodox compared to traditional approaches used in the problem domain. Therefore, efficiently porting, improving or optimizing an evolved design might not be a trivial task. In this work we argue that the same GP principles used to evolve the solution can then be used to optimize a particular new implementation of the design, following the Genetic Improvement approach. In particular, this paper presents a case study where evolved image operators are ported from Matlab to OpenCV, and then the source code is optimized an improved using Genetic Improvement of Software for Multiple Objectives (GISMOE). In the example we show that functional behavior is maintained (output image) while improving non-functional properties (computation time). Despite the fact that this first example is a simple case, it clearly illustrates the possibilities of using GP principles in two distinct stages of the software development process, from design to improved implementation."

- title: Evolutionary Optimization of Compiler Flag Selection by Learning and Exploiting Flags Interactions
  url: https://dl.acm.org/doi/10.1145/2908961.2931696
  abstract: "Compiler flag selection can be an effective way to increase the quality of executable code according to different code quality criteria. Evolutionary algorithms have been successfully applied to this optimization problem. However, previous approaches have only partially addressed the question of capturing and exploiting the interactions between compilation options to improve the search. In this paper we deal with this question comparing estimation of distribution algorithms (EDAs) and a traditional genetic algorithm approach. We show that EDAs that learn bivariate interactions can improve the results of GAs for some of the programs considered. We also show that the probabilistic models generated as a result of the search for optimal flag combinations can be used to unveil the (problem-dependent) interactions between the flags, allowing the user a more informed choice of compilation options."

- title: "COBAYN: Compiler Autotuning Framework Using Bayesian Networks"
  url: https://dl.acm.org/doi/10.1145/2928270
  abstract: "The variety of today’s architectures forces programmers to spend a great deal of time porting and tuning application codes across different platforms. Compilers themselves need additional tuning, which has considerable complexity as the standard optimization levels, usually designed for the average case and the specific target architecture, often fail to bring the best results.
This article proposes COBAYN: Compiler autotuning framework using BAYesian Networks, an approach for a compiler autotuning methodology using machine learning to speed up application performance and to reduce the cost of the compiler optimization phases. The proposed framework is based on the application characterization done dynamically by using independent microarchitecture features and Bayesian networks. The article also presents an evaluation based on using static analysis and hybrid feature collection approaches. In addition, the article compares Bayesian networks with respect to several state-of-the-art machine-learning models.
Experiments were carried out on an ARM embedded platform and GCC compiler by considering two benchmark suites with 39 applications. The set of compiler configurations, selected by the model (less than 7% of the search space), demonstrated an application performance speedup of up to 4.6 × on Polybench (1.85 × on average) and 3.1 × on cBench (1.54 × on average) with respect to standard optimization levels. Moreover, the comparison of the proposed technique with (i) random iterative compilation, (ii) machine learning--based iterative compilation, and (iii) noniterative predictive modeling techniques shows, on average, 1.2 × , 1.37 × , and 1.48 × speedup, respectively. Finally, the proposed method demonstrates 4 × and 3 × speedup, respectively, on cBench and Polybench in terms of exploration efficiency given the same quality of the solutions generated by the random iterative compilation model."

- title: Comparative study of the impact of processor architecture on compiler tuning
  url: https://ieeexplore.ieee.org/document/7732109
  abstract: "Deciding the nearly optimal optimization options and selecting the right values for compiler parameter set is a combinatorial problem. In order to obtain the maximal performance, sophisticated tuning strategies were employed by many researchers. Impact of tuning and thereby quality of the Compiler generated code often depend on many factors like the optimization infrastructure of the compiler and its maturity level, optimization objective, application source and also the target processor architecture. In order to understand the impact of processor architecture, we have conducted an empirical study on X86 and ARM platforms and compared the results. We have employed Genetic Algorithm based tuning techniques on SPEC benchmark programs for both the compiler optimization option selection and also the parameter tuning problems independently as well as together. Results demonstrate that there is a significant impact of processor architecture on compiler tuning."

- title: API-Constrained Genetic Improvement
  url: https://link.springer.com/chapter/10.1007/978-3-319-47106-8_16
  abstract: "ACGI respects the Application Programming Interface whilst using genetic programming to optimise the implementation of the API. It reduces the scope for improvement but it may smooth the path to GI acceptance because the programmer’s code remains unaffected; only library code is modified. We applied ACGI to C++ software for the state-of-the-art OpenCV SEEDS superPixels image segmentation algorithm, obtaining a speed-up of up to 13.2 % (  ±1.3%±1.3% ) to the $50 K Challenge winner announced at CVPR 2015."

- title: A General-Purpose Framework for Genetic Improvement
  url: https://link.springer.com/chapter/10.1007/978-3-319-45823-6_32
  abstract: "Genetic Improvement is an evolutionary-based technique. Despite its relatively recent introduction, several successful applications have been already reported in the scientific literature: it has been demonstrated able to modify the code complex programs without modifying their intended behavior; to increase performance with regards to speed, energy consumption or memory use. Some results suggest that it could be also used to correct bugs, restoring the software’s intended functionalities. Given the novelty of the technique, however, instances of Genetic Improvement so far rely upon ad-hoc, language-specific implementations. In this paper, we propose a general framework based on the software engineering’s idea of mutation testing coupled with Genetic Programming, that can be easily adapted to different programming languages and objective. In a preliminary evaluation, the framework efficiently optimizes the code of the md5 hash function in C, Java, and Python."

- title: "MO-ParamILS: A Multi-objective Automatic Algorithm Configuration Framework"
  url: https://link.springer.com/chapter/10.1007%2F978-3-319-50349-3_3
  abstract: "Automated algorithm configuration procedures play an increasingly important role in the development and application of algorithms for a wide range of computationally challenging problems. Until very recently, these configuration procedures were limited to optimising a single performance objective, such as the running time or solution quality achieved by the algorithm being configured. However, in many applications there is more than one performance objective of interest. This gives rise to the multi-objective automatic algorithm configuration problem, which involves finding a Pareto set of configurations of a given target algorithm that characterises trade-offs between multiple performance objectives. In this work, we introduce MO-ParamILS, a multi-objective extension of the state-of-the-art single-objective algorithm configuration framework ParamILS, and demonstrate that it produces good results on several challenging bi-objective algorithm configuration scenarios compared to a base-line obtained from using a state-of-the-art single-objective algorithm configurator."

- title: Genetic improvement of runtime and its fitness landscape in a bioinformatics application
  url: https://dl.acm.org/doi/10.1145/3067695.3082526
  abstract: "We present a Genetic Improvement (GI) experiment on ProbAbel, a piece of bioinformatics software for Genome Wide Association (GWA) studies. The GI framework used here has previously been successfully used on Python programs and can, with minimal adaptation, be used on source code written in other languages. We achieve improvements in execution time without the loss of accuracy in output while also exploring the vast fitness landscape that the GI framework has to search. The runtime improvements achieved on smaller data set scale up for larger data sets. Our findings are that for ProbAbel, the GI's execution time landscape is noisy but flat. We also confirm that human written code is robust with respect to small edits to the source code."

- title: Genetic improvement of GPU software
  url: https://link.springer.com/article/10.1007/s10710-016-9273-9
  abstract: "We survey genetic improvement (GI) of general purpose computing on graphics cards. We summarise several experiments which demonstrate four themes. Experiments with the gzip program show that genetic programming can automatically port sequential C code to parallel code. Experiments with the StereoCamera program show that GI can upgrade legacy parallel code for new hardware and software. Experiments with NiftyReg and BarraCUDA show that GI can make substantial improvements to current parallel CUDA applications. Finally, experiments with the pknotsRG program show that with semi-automated approaches, enormous speed ups can sometimes be had by growing and grafting new code with genetic programming in combination with human input."

- title: Online Genetic Improvement on the java virtual machine with ECSELR
  url: https://link.springer.com/article/10.1007/s10710-016-9278-4
  abstract: "Online Genetic Improvement embeds the ability to evolve and adapt inside a target software system enabling it to improve at runtime without any external dependencies or human intervention. We recently developed a general purpose tool enabling Online Genetic Improvement in software systems running on the java virtual machine. This tool, dubbed ECSELR, is embedded inside extant software systems at runtime, enabling such systems to self-improve and adapt autonomously online. We present this tool, describing its architecture and focusing on its design choices and possible uses."

- title: A search for improved performance in regular expressions
  url: https://dl.acm.org/doi/10.1145/3071178.3071196
  abstract: "The primary aim of automated performance improvement is to reduce the running time of programs while maintaining (or improving on) functionality. In this paper, Genetic Programming is used to find performance improvements in regular expressions for an array of target programs, representing the first application of automated software improvement for run-time performance in the Regular Expression language. This particular problem is interesting as there may be many possible alternative regular expressions which perform the same task while exhibiting subtle differences in performance. A benchmark suite of candidate regular expressions is proposed for improvement. We show that the application of Genetic Programming techniques can result in performance improvements in all cases.
As we start evolution from a known good regular expression, diversity is critical in escaping the local optima of the seed expression. In order to understand diversity during evolution we compare an initial population consisting of only seed programs with a population initialised using a combination of a single seed individual with individuals generated using PI Grow and Ramped-half-and-half initialisation mechanisms."

- title: "Using algorithm configuration tools to optimize genetic programming parameters: a case study"
  url: https://dl.acm.org/doi/10.1145/3067695.3076097
  abstract: "We use Sequential Model-based Algorithm Configuration (SMAC) to optimize a group of parameters for PushGP, a stack-based genetic programming system, for several software synthesis problems. Applying SMAC to one particular problem leads to marked improvements in the success rate and the speed with which a solution was found for that problem. Applying these \"tuned\" parameters to four additional problems, however, only improved performance on one, and substantially reduced performance on another. This suggests that SMAC is \"overfitting\", tuning the parameters in ways that are highly problem specific, and raises doubts about the value of using these \"tuned\" parameters on previously unsolved problems. Efforts to use SMAC to optimize PushGP parameters on other problems have been less successful due to a combination of long PushGP run times and low success rates, which make it hard for SMAC to acquire enough information in a reasonable amount of time."

- title: Improving SSE parallel code with grow and graft genetic programming
  url: https://dl.acm.org/doi/10.1145/3067695.3082524
  abstract: "RNAfold predicts the secondary structure of RNA molecules from their base sequence. We apply a mixture of manual and automated genetic improvements to its C source. GI gives a 1.6% improvement to parallel SSE4.1 code. The automatic programming evolutionary system has access to Intel library code and previous revisions. On 4 666 curated structures from RNA_STRAND, GGGP gives a combined speed up of 31.9%, with no loss of accuracy (GI code run 1.4 1011 times)."

- title: The use of predictive models in dynamic treatment planning
  url: https://ieeexplore.ieee.org/document/8024536
  abstract: "With the expanding load on healthcare and consequent strain on budget, the demand for tools to increase efficiency in treatments is rising. The use of prediction models throughout the treatment to identify risk factors might be a solution. In this paper we present a novel implementation of a prediction tool and the first use of a dynamic predictor in vocational rehabilitation practice. The tool is periodically updated and improved with Genetic Improvement of software. The predictor has been in use for 10 months and is evaluated on predictions made during that time by comparing them with actual treatment outcome. The results show that the predictions have been consistently accurate throughout the patients' treatment. After approximately 3 week learning phase, the predictor classified patients with 100% accuracy and precision on previously unseen data. The predictor is currently being successfully used in a complex live system where specialists have used it to make informed decisions."

- title: Performance localisation
  url: https://dl.acm.org/doi/10.1145/3194810.3194815
  abstract: "Profiling techniques highlight where performance issues manifest and provide a starting point for tracing cause back through a program. While people diagnose and understand the cause of performance to guide formulation of a performance improvement, we seek automated techniques for highlighting performance improvement opportunities to guide search algorithms.
We investigate mutation-based approaches for highlighting where a performance improvement is likely to exist. For all modification locations in a program, we make all possible modifications and analyse how often modifications reduce execution count. We compare the resulting code location rankings against rankings derived using a profiler and find that mutation analysis provides the higher accuracy in highlighting performance improvement locations in a set of benchmark problems, though at a much higher execution cost.
We see both approaches as complimentary and consider how they may be used to further guide Genetic Programming in finding performance improvements."

- title: Novelty search for software improvement of a SLAM system
  url: https://dl.acm.org/doi/10.1145/3205651.3208237
  abstract: "Genetic Improvement (GI) performs a search at the level of source code to find the best variant of a baseline system that improves non-functional properties while maintaining functionality with noticeable results in several domains. There a many aspects of this general approach that are currently being explored. In particular, this work deals to the way in which the search is guided to efficiently explore the search space of possible software versions in which GI operates. The proposal is to integrate Novelty Search (NS) within the GISMOE GI framework to improve KinectFusion, which is a vision-based Simultaneous Localization and Mapping (SLAM) system that is used for augmented reality, autonomous vehicle navigation, and many other real-world applications. This is one of a small set of works that have successfully combined NS with a GP system, and the first time that it has been used for software improvement. To achieve this, we propose a new behaviour descriptor for SLAM algorithms, based on state-of-the-art benchmarking and present results that show that NS can produce significant improvement gains in a GI setting, when considering execution time and trajectory estimation as the main performance criteria."

- title: Specialising Software for Different Downstream Applications Using Genetic Improvement and Code Transplantation
  url: https://ieeexplore.ieee.org/abstract/document/7962212
  abstract: "Genetic improvement uses automated search to find improved versions of existing software. Genetic improvement has previously been concerned with improving a system with respect to all possible usage scenarios. In this paper, we show how genetic improvement can also be used to achieve specialisation to a specific set of usage scenarios. We use genetic improvement to evolve faster versions of a C++ program, a Boolean satisfiability solver called MiniSAT, specialising it for three different applications, each with their own characteristics. Our specialised solvers achieve between 4 and 36 percent execution time improvement, which is commensurate with efficiency gains achievable using human expert optimisation for the general solver. We also use genetic improvement to evolve faster versions of an image processing tool called ImageMagick, utilising code from GraphicsMagick, another image processing tool which was forked from it. We specialise the format conversion functionality to greyscale images and colour images only. Our specialised versions achieve up to 3 percent execution time improvement."

- title: Darwinian data structure selection
  url: https://dl.acm.org/doi/10.1145/3236024.3236043
  abstract: "Data structure selection and tuning is laborious but can vastly improve an application’s performance and memory footprint. Some data structures share a common interface and enjoy multiple implementations. We call them Darwinian Data Structures (DDS), since we can subject their implementations to survival of the fittest. We introduce ARTEMIS a multi-objective, cloud-based search-based optimisation framework that automatically finds optimal, tuned DDS modulo a test suite, then changes an application to use that DDS. ARTEMIS achieves substantial performance improvements for every project in 5 Java projects from DaCapo benchmark, 8 popular projects and 30 uniformly sampled projects from GitHub. For execution time, CPU usage, and memory consumption, ARTEMIS finds at least one solution that improves all measures for 86% (37/43) of the projects. The median improvement across the best solutions is 4.8%, 10.1%, 5.1% for runtime, memory and CPU usage.
These aggregate results understate ARTEMIS’s potential impact. Some of the benchmarks it improves are libraries or utility functions. Two examples are gson, a ubiquitous Java serialization framework, and xalan, Apache’s XML transformation tool. ARTEMIS improves gson by 16.5%, 1% and 2.2% for memory, runtime, and CPU; ARTEMIS improves xalan’s memory consumption by 23.5%. Every client of these projects will benefit from these performance improvements."

- title: "In-vivo and offline optimisation of energy use in the presence of small energy signals: A case study on a popular Android library"
  url: https://dl.acm.org/doi/10.1145/3286978.3287014
  abstract: "Energy demands of applications on mobile platforms are increasing. As a result, there has been a growing interest in optimising their energy efficiency. As mobile platforms are fast-changing, diverse and complex, the optimisation of energy use is a non-trivial task.
To date, most energy optimisation methods either use models or external meters to estimate energy use. Unfortunately, it becomes hard to build widely applicable energy models, and external meters are neither cheap nor easy to set up. To address this issue, we run application variants in-vivo on the phone and use a precise internal battery monitor to measure energy use. We describe a methodology for optimising a target application in-vivo and with application-specific models derived from the device's own internal meter based on jiffies and lines of code. We demonstrate that this process produces a significant improvement in energy efficiency with limited loss of accuracy."

- title: Effective Program Debloating via Reinforcement Learning
  url: https://dl.acm.org/doi/10.1145/3243734.3243838
  abstract: "Prevalent software engineering practices such as code reuse and the \"one-size-fits-all\" methodology have contributed to significant and widespread increases in the size and complexity of software. The resulting software bloat has led to decreased performance and increased security vulnerabilities. We propose a system called Chisel to enable programmers to effectively customize and debloat programs. Chisel takes as input a program to be debloated and a high-level specification of its desired functionality. The output is a reduced version of the program that is correct with respect to the specification. Chisel significantly improves upon existing program reduction systems by using a novel reinforcement learning-based approach to accelerate the search for the reduced program and scale to large programs. Our evaluation on a suite of 10 widely used UNIX utility programs each comprising 13-90 KLOC of C source code demonstrates that Chisel is able to successfully remove all unwanted functionalities and reduce attack surfaces. Compared to two state-of-the-art program reducers C-Reduce and Perses, which time out on 6 programs and 2 programs espectively in 12 hours, Chisel runs up to 7.1x and 3.7x faster and finishes on all programs."

- title: "Trimmer: Application specialization for code debloating"
  url: https://dl.acm.org/doi/10.1145/3238147.3238160
  abstract: "With the proliferation of new hardware architectures and ever-evolving user requirements, the software stack is becoming increasingly bloated. In practice, only a limited subset of the supported functionality is utilized in a particular usage context, thereby presenting an opportunity to eliminate unused features. In the past, program specialization has been proposed as a mechanism for enabling automatic software debloating. In this work, we show how existing program specialization techniques lack the analyses required for providing code simplification for real-world programs. We present an approach that uses stronger analysis techniques to take advantage of constant configuration data, thereby enabling more effective debloating. We developed Trimmer, an application specialization tool that leverages user-provided configuration data to specialize an application to its deployment context. The specialization process attempts to eliminate the application functionality that is unused in the user-defined context. Our evaluation demonstrates Trimmer can effectively reduce code bloat. For 13 applications spanning various domains, we observe a mean binary size reduction of 21% and a maximum reduction of 75%. We also show specialization reduces the surface for code-reuse attacks by reducing the number of exploitable gadgets. For the evaluated programs, we observe a 20% mean reduction in the total gadget count and a maximum reduction of 87%."

- title: Approximate Oracles and Synergy in Software Energy Search Spaces
  url: https://ieeexplore.ieee.org/document/8338061
  abstract: "Reducing the energy consumption of software systems through optimisation techniques such as genetic improvement is gaining interest. However, efficient and effective improvement of software systems requires a better understanding of the code-change search space. One important choice practitioners have is whether to preserve the system's original output or permit approximation, with each scenario having its own search space characteristics. When output preservation is a hard constraint, we report that the maximum energy reduction achievable by the modification operators is 2.69 percent (0.76 percent on average). By contrast, this figure increases dramatically to 95.60 percent (33.90 percent on average) when approximation is permitted, indicating the critical importance of approximate output quality assessment for code optimisation. We investigate synergy, a phenomenon that occurs when simultaneously applied source code modifications produce an effect greater than their individual sum. Our results reveal that 12.0 percent of all joint code modifications produced such a synergistic effect, though 38.5 percent produce an antagonistic interaction in which simultaneously applied modifications are less effective than when applied individually. This highlights the need for more advanced search-based techniques."

- title: Challenges on applying genetic improvement in JavaScript using a high-performance computer
  url: https://link.springer.com/article/10.1186%2Fs40411-018-0056-2
  abstract: "Genetic Improvement is an area of Search Based Software Engineering that aims to apply evolutionary computing operators to the software source code to improve it according to one or more quality metrics. This article describes challenges related to experimental studies using Genetic Improvement in JavaScript (an interpreted and non-typed language). It describes our experience on performing a study with fifteen projects submitted to genetic improvement with the use of a supercomputer. The construction of specific software infrastructure to support such an experimentation environment reveals peculiarities (parallelization problems, management of threads, etc.) that must be carefully considered to avoid future research threats to validity such as dead-ends, which make it impossible to observe relevant phenomena (code transformation) to the understanding of software improvements and evolution."

- title: Genetic improvement of GPU code
  url: https://dl.acm.org/doi/10.1109/GI.2019.00014
  abstract: "As the programming stack and tool support for GPU have matured, GPUs have become accessible to programmers who often lack domain-specific knowledge of the underlying architecture and fail to fully leverage the GPU's computation power. This paper presents GEVO (Gpu EVOlution), a tool for automatically tuning the performance of GPU kernels in the LLVM representation to meet desired criteria. GEVO uses population-based search to find edits to programs compiled to LLVM-IR that improve performance on desired criteria and retain required functionality. GEVO extends earlier GI work by operating directly on the LLVM-IR without custom representations or other manual interventions. We demonstrate that GEVO improves runtime on NVIDIA Tesla P100 for many programs in the Rodinia benchmark suite and a supervised machine learning code, ThunderSVM. For the Rodinia benchmark, GEVO improves GPU kernel runtime performance by an average of 13.87% and as much as 43% over the fully compiler-optimized baseline. If the kernel output accuracy is relaxed to tolerate 1% error, GEVO can find kernel variants that outperform the baseline version by an average of 15.47%. For ThunderSVM, GEVO reduces entire model training time by 50% and 24.8%, for MNIST handwriting recognition dataset and a9a income prediction, where the accuracy of trained model are improved by 0.17% and 0.04% respectively."

- title: Genetic improvement of data gives double precision invsqrt
  url: https://dl.acm.org/doi/10.1145/3319619.3326800
  abstract: "CMA-ES plus manual code changes rapidly transforms 512 Newton-Raphson start points from a GNU C library table driven version of sqrt into a double precision reciprocal square root function. The GI x-1/2 is far more accurate than Quake's InvSqrt, Quare root."

- title: "PyGGI 2.0: language independent genetic improvement framework"
  url: https://dl.acm.org/doi/10.1145/3338906.3341184
  abstract: "PyGGI is a research tool for Genetic Improvement (GI), that is designed to be versatile and easy to use. We present version 2.0 of PyGGI, the main feature of which is an XML-based intermediate program representation. It allows users to easily define GI operators and algorithms that can be reused with multiple target languages. Using the new version of PyGGI, we present two case studies. First, we conduct an Automated Program Repair (APR) experiment with the QuixBugs benchmark, one that contains defective programs in both Python and Java. Second, we replicate an existing work on runtime improvement through program specialisation for the MiniSAT satisfiability solver. PyGGI 2.0 was able to generate a patch for a bug not previously fixed by any APR tool. It was also able to achieve 14% runtime improvement in the case of MiniSAT. The presented results show the applicability and the expressiveness of the new version of PyGGI. A video of the tool demo is at: https://youtu.be/PxRUdlRDS40."


- title: Recommending energy-efficient Java collections
  url: https://dl.acm.org/doi/10.1109/MSR.2019.00033
  abstract: "Over the last years, increasing attention has been given to creating energy-efficient software systems. However, developers still lack the knowledge and the tools to support them in that task. In this work, we explore our vision that energy consumption non-specialists can build software that consumes less energy by alternating, at development time, between third-party, readily available, diversely-designed pieces of software, without increasing the development complexity. To support our vision, we propose an approach for energy-aware development that combines the construction of application-independent energy profiles of Java collections and static analysis to produce an estimate of in which ways and how intensively a system employs these collections. By combining these two pieces of information, it is possible to produce energy-saving recommendations for alternative collection implementations to be used in different parts of the system. We implement this approach in a tool named CT+ that works with both desktop and mobile Java systems, and is capable of analyzing 40 different collection implementations of lists, maps, and sets. We applied CT+ to twelve software systems: two mobile-based, seven desktop-based, and three that can run in both environments. Our evaluation infrastructure involved a high-end server, a notebook, and three mobile devices. When applying the (mostly trivial) recommendations, we achieved up to 17.34% reduction in energy consumption just by replacing collection implementations. Even for a real world, mature, highly-optimized system such as Xalan, CT+ could achieve a 5.81% reduction in energy consumption. Our results indicate that some widely used collections, e.g., ArrayList, HashMap, and HashTable, are not energy-efficient and sometimes should be avoided when energy consumption is a major concern."

- title: Evolving sqrt into 1/x via software data maintenance
  url: https://dl.acm.org/doi/10.1145/3377929.3398110
  abstract: "While most software automation research concentrates on programs' code, we have started investigating if Genetic Improvement (GI) of data can assist developers by automating aspects of the maintenance of parameters embedded in source code. We extend recent GI work on optimising compile time constants to give new functionality and describe the transformation of a GNU C library square root function into the double precision reciprocal function, drcp. Multiplying by 1/x (drcp) allows division free division without requiring the hardware to support division. The evolution (6 seconds) and indeed the GI dp division (7.14 ± 0.012 nS) are both surprisingly fast."

- title: Genetic improvement of data gives binary logarithm from sqrt
  url: https://dl.acm.org/doi/10.1145/3319619.3321954
  abstract: "Automated search in the form of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), plus manual code changes, transforms 512 Newton-Raphson floating point start numbers from an open source GNU C library, glibc, table driven square root function to create a new bespoke custom mathematical implementation of double precision binary logarithm log2 for C in seconds."

- title: Safe automated refactoring for intelligent parallelization of Java 8 streams
  url: https://dl.acm.org/doi/10.1109/GI.2019.00014
  abstract: "As the programming stack and tool support for GPU have matured, GPUs have become accessible to programmers who often lack domain-specific knowledge of the underlying architecture and fail to fully leverage the GPU's computation power. This paper presents GEVO (Gpu EVOlution), a tool for automatically tuning the performance of GPU kernels in the LLVM representation to meet desired criteria. GEVO uses population-based search to find edits to programs compiled to LLVM-IR that improve performance on desired criteria and retain required functionality. GEVO extends earlier GI work by operating directly on the LLVM-IR without custom representations or other manual interventions. We demonstrate that GEVO improves runtime on NVIDIA Tesla P100 for many programs in the Rodinia benchmark suite and a supervised machine learning code, ThunderSVM. For the Rodinia benchmark, GEVO improves GPU kernel runtime performance by an average of 13.87% and as much as 43% over the fully compiler-optimized baseline. If the kernel output accuracy is relaxed to tolerate 1% error, GEVO can find kernel variants that outperform the baseline version by an average of 15.47%. For ThunderSVM, GEVO reduces entire model training time by 50% and 24.8%, for MNIST handwriting recognition dataset and a9a income prediction, where the accuracy of trained model are improved by 0.17% and 0.04% respectively."

- title: A methodology correlating code optimizations with data memory accesses, execution time and energy consumption
  url: https://link.springer.com/article/10.1007%2Fs11227-019-02880-z
  abstract: "The advent of data proliferation and electronic devices gets low execution time and energy consumption software in the spotlight. The key to optimizing software is the correct choice, order as well as parameters of optimization transformations that has remained an open problem in compilation research for decades for various reasons. First, most of the transformations are interdependent and thus addressing them separately is not effective. Second, it is very hard to couple the transformation parameters to the processor architecture (e.g., cache size) and algorithm characteristics (e.g., data reuse); therefore, compiler designers and researchers either do not take them into account at all or do it partly. Third, the exploration space, i.e., the set of all optimization configurations that have to be explored, is huge and thus searching is impractical. In this paper, the above problems are addressed for data-dominant affine loop kernels, delivering significant contributions. A novel methodology is presented reducing the exploration space of six code optimizations by many orders of magnitude. The objective can be execution time (ET), energy consumption (E) or the number of L1, L2 and main memory accesses. The exploration space is reduced in two phases: firstly, by applying a novel register blocking algorithm and a novel loop tiling algorithm and secondly, by computing the maximum and minimum ET/E values for each optimization set. The proposed methodology has been evaluated for both embedded and general-purpose CPUs and for seven well-known algorithms, achieving high memory access, speedup and energy consumption gain values (from 1.17 up to 40) over gcc compiler, hand-written optimized code and Polly. The exploration space from which the near-optimum parameters are selected is reduced from 17 up to 30 orders of magnitude."

- title: "Mossad: defeating software plagiarism detection"
  url: https://dl.acm.org/doi/abs/10.1145/3428206
  abstract: "Automatic software plagiarism detection tools are widely used in educational settings to ensure that submitted work was not copied. These tools have grown in use together with the rise in enrollments in computer science programs and the widespread availability of code on-line. Educators rely on the robustness of plagiarism detection tools; the working assumption is that the effort required to evade detection is as high as that required to actually do the assigned work.
This paper shows this is not the case. It presents an entirely automatic program transformation approach, MOSSAD, that defeats popular software plagiarism detection tools. MOSSAD comprises a framework that couples techniques inspired by genetic programming with domain-specific knowledge to effectively undermine plagiarism detectors. MOSSAD is effective at defeating four plagiarism detectors, including Moss and JPlag. MOSSAD is both fast and effective: it can, in minutes, generate modified versions of programs that are likely to escape detection. More insidiously, because of its non-deterministic approach, MOSSAD can, from a single program, generate dozens of variants, which are classified as no more suspicious than legitimate assignments. A detailed study of MOSSAD across a corpus of real student assignments demonstrates its efficacy at evading detection. A user study shows that graduate student assistants consistently rate MOSSAD-generated code as just as readable as authentic student code. This work motivates the need for both research on more robust plagiarism detection tools and greater integration of naturally plagiarism-resistant methodologies like code review into computer science education."

- title: "GEVO: GPU Code Optimization Using Evolutionary Computation"
  url: https://dl.acm.org/doi/10.1145/3418055
  abstract: "GPUs are a key enabler of the revolution in machine learning and high-performance computing, functioning as de facto co-processors to accelerate large-scale computation. As the programming stack and tool support have matured, GPUs have also become accessible to programmers, who may lack detailed knowledge of the underlying architecture and fail to fully leverage the GPU’s computation power. GEVO (Gpu optimization using EVOlutionary computation) is a tool for automatically discovering optimization opportunities and tuning the performance of GPU kernels in the LLVM representation. GEVO uses population-based search to find edits to GPU code compiled to LLVM-IR and improves performance on desired criteria while retaining required functionality. We demonstrate that GEVO improves the execution time of general-purpose GPU programs and machine learning (ML) models on NVIDIA Tesla P100. For the Rodinia benchmarks, GEVO improves GPU kernel runtime performance by an average of 49.48% and by as much as 412% over the fully compiler-optimized baseline. If kernel output accuracy is relaxed to tolerate up to 1% error, GEVO can find kernel variants that outperform the baseline by an average of 51.08%. For the ML workloads, GEVO achieves kernel performance improvement for SVM on the MNIST handwriting recognition (3.24×) and the a9a income prediction (2.93×) datasets with no loss of model accuracy. GEVO achieves 1.79× kernel performance improvement on image classification using ResNet18/CIFAR-10, with less than 1% model accuracy reduction."

- title: Tuning genetic algorithm parameters using design of experiments
  url: https://dl.acm.org/doi/10.1145/3377929.3398136
  abstract: "Tuning evolutionary algorithms is a persistent challenge in the field of evolutionary computing. The efficiency of an evolutionary algorithm relates to the coding of the algorithm, the design of the evolutionary operators and the parameter settings for evolution. In this paper, we explore the effect of tuning the operators and parameters of a genetic algorithm for solving the Traveling Salesman Problem using Design of Experiments theory. Small scale problems are solved with specific settings of parameters including population size, crossover rate, mutation rate and the extent of elitism. Good values of the parameters suggested by the experiments are used to solve large scale problems. Computational tests show that the parameters selected by this process result in improved performance both in the quality of results obtained and the convergence rate when compared with untuned parameter settings."

- title: Injecting Shortcuts for Faster Running Java Code
  url: https://ieeexplore.ieee.org/document/9185708/
  abstract: "Genetic Improvement of software applies search methods to existing software to improve the target program in some way. Impressive results have been achieved, including substantial speedups, using simple operations that replace, swap and delete lines or statements within the code. Often this is achieved by specialising code, removing parts that are unnecessary for particular use-cases. Previous work has shown that there is a great deal of potential in targeting more specialised operations that modify the code to achieve the same functionality in a different way. We propose six new edit types for Genetic Improvement of Java software, based on the insertion of break, continue and return statements. The idea is to add shortcuts that allow parts of the program to be skipped in order to speed it up. 10000 randomlygenerated instances of each edit were applied to three opensource applications taken from GitHub. The key findings are: (1) compilation rates for inserted statements without surrounding “if” statements are 1.3-18.3%; (2) edits where the inserted statement is embedded within an “if” have compilation rates of 3.2-55.8%; (3) of those that compiled, all 6 edits have a high rate of passing tests (Neutral Variant Rate), >60% in all but one case, and so have the potential to be performance improving edits. Finally, a preliminary experiment based on local search shows how these edits might be used in practice."

- title: Comparing Genetic Programming Approaches for Non-functional Genetic Improvement
  url: https://link.springer.com/chapter/10.1007/978-3-030-44094-7_5
  abstract: "Genetic improvement (GI) uses automated search to find improved versions of existing software. While most GI work use genetic programming (GP) as the underlying search process, focus is usually given to the target software only. As a result, specifics of GP algorithms for GI are not well understood and rarely compared to one another. In this work, we propose a robust experimental protocol to compare different GI search processes and investigate several variants of GP- and random-based approaches. Through repeated experiments, we report a comparative analysis of these approaches, using one of the previously used GI scenarios: improvement of runtime of the MiniSAT satisfiability solver. We conclude that the test suites used have the most significant impact on the GI results. Both random and GP-based approaches are able to find improved software, even though the percentage of viable software variants is significantly smaller in the random case (  14.5%14.5%  vs.   80.1%80.1% ). We also report that GI produces MiniSAT variants up to twice as fast as the original on sets of previously unseen instances from the same application domain."

- title: "JShrink: in-depth investigation into debloating modern Java applications"
  url: https://dl.acm.org/doi/10.1145/3368089.3409738
  abstract: "Modern software is bloated. Demand for new functionality has led developers to include more and more features, many of which become unneeded or unused as software evolves. This phenomenon, known as software bloat, results in software consuming more resources than it otherwise needs to. How to effectively and automatically debloat software is a long-standing problem in software engineering. Various debloating techniques have been proposed since the late 1990s. However, many of these techniques are built upon pure static analysis and have yet to be extended and evaluated in the context of modern Java applications where dynamic language features are prevalent.
To this end, we develop an end-to-end bytecode debloating framework called JShrink. It augments traditional static reachability analysis with dynamic profiling and type dependency analysis and renovates existing bytecode transformations to account for new language features in modern Java. We highlight several nuanced technical challenges that must be handled properly and examine behavior preservation of debloated software via regression testing. We find that (1) JShrink is able to debloat our real-world Java benchmark suite by up to 47% (14% on average); (2) accounting for dynamic language features is indeed crucial to ensure behavior preservation---reducing 98% of test failures incurred by a purely static equivalent, Jax, and 84% for ProGuard; and (3) compared with purely dynamic approaches, integrating static analysis with dynamic profiling makes the debloated software more robust to unseen test executions---in 22 out of 26 projects, the debloated software ran successfully under new tests."

- title: "WebJShrink: a web service for debloating Java bytecode"
  url: https://dl.acm.org/doi/10.1145/3368089.3417934
  abstract: "As software projects grow in complexity, they come packaged with under-utilized libraries and therefore become bloated. Though several software debloating tools exist, none of them help developers gain insights into how under-utilized those libraries are nor help developers build confidence in the behavior preservation of software after debloating. To bridge this gap, we developed WebJShrink, a visual analytics tool for analyzing and pruning bloated software projects. WebJShrink is built on JShrink which uses static and dynamic reachability analysis to determine the extent of software bloat. WebJShrink provides rich visualizations of the bloat lurking within a target project's internal structure. It then removes unused features, and returns a safer, slimmer variant of the software project. To illustrate the target project's behavior preservation, WebJShrink examines the debloated software with its JUnit tests and visualizes the test results. In evaluating WebJShrink against 26 real world systems, we found WebJShrink could reduce software size by up to 42%, 11% on average, while still passing 100% of unit tests after debloating. We provide a video demonstrating WebJShrink at https://youtu.be/yzVzcd-MJ1w."

- title: Transformations towards clean functional code
  url: https://dl.acm.org/doi/10.1145/3406085.3409010
  abstract: "The programming style has an impact on the readability and comprehensibility of the source code, and it may also affect run-time performance. This statement also holds for functional languages when the functional style is mixed with imperative design. In this paper, we present a couple of methods that can refactor imperatively styled Erlang source-code into a more functionally styled one. This can be done by transforming unnecessary calls to length, hd and tl into pattern matching or by lifting particular nested expressions. The results of our investigations indicate that these refactorings can not only shorten the length of the source code but also affect the complexity/readability. In this paper, we present some refactorings; moreover, real-life examples and data for its validation."

- title: Optimization of Java Virtual Machine Flags using Feature Model and Genetic Algorithm
  url: https://dl.acm.org/doi/10.1145/3447545.3451177
  abstract: "Optimizing the Java Virtual Machine (JVM) options in order to get the best performance out of a program for production is a challenging and time-consuming task. HotSpot, the Oracle's open-source Java VM implementation offers more than 500 options, called flags, that can be used to tune the JVM's compiler, garbage collector (GC), heap size and much more. In addition to being numerous, these flags are sometimes poorly documented and create a need of benchmarking to ensure that the flags and their associated values deliver the best performance and stability for a particular program to execute.
Auto-tuning approaches have already been proposed in order to mitigate this burden. However, in spite of increasingly sophisticated search techniques allowing for powerful optimizations, these approaches take little account of the underlying complexities of JVM flags. Indeed, dependencies and incompatibilities between flags are non-trivial to express, which if not taken into account may lead to invalid or spurious flag configurations that should not be considered by the auto-tuner.
In this paper, we propose a novel model, inspired by the feature model used in Software Product Line, which takes the complexity of JVM's flags into account. We then demonstrate the usefulness of this model, using it as an input of a Genetic Algorithm (GA) to optimize the execution times of DaCapo Benchmarks."

- title: Efficient Auto-Tuning of Parallel Programs with Interdependent Tuning Parameters via Auto-Tuning Framework (ATF)
  url: https://dl.acm.org/doi/10.1145/3427093
  abstract: "Auto-tuning is a popular approach to program optimization: it automatically finds good configurations of a program’s so-called tuning parameters whose values are crucial for achieving high performance for a particular parallel architecture and characteristics of input/output data. We present three new contributions of the Auto-Tuning Framework (ATF), which enable a key advantage in general-purpose auto-tuning: efficiently optimizing programs whose tuning parameters have interdependencies among them. We make the following contributions to the three main phases of general-purpose auto-tuning: (1) ATF generates the search space of interdependent tuning parameters with high performance by efficiently exploiting parameter constraints; (2) ATF stores such search spaces efficiently in memory, based on a novel chain-of-trees search space structure; (3) ATF explores these search spaces faster, by employing a multi-dimensional search strategy on its chain-of-trees search space representation. Our experiments demonstrate that, compared to the state-of-the-art, general-purpose auto-tuning frameworks, ATF substantially improves generating, storing, and exploring the search space of interdependent tuning parameters, thereby enabling an efficient overall auto-tuning process for important applications from popular domains, including stencil computations, linear algebra routines, quantum chemistry computations, and data mining algorithms."

- title: Empirical Comparison of Search Heuristics for Genetic Improvement of Software
  url: https://ieeexplore.ieee.org/document/9392013
  abstract: "Genetic improvement uses automated search to improve existing software. It has been successfully used to optimise various program properties, such as runtime or energy consumption, as well as for the purpose of bug fixing. Genetic improvement typically navigates a space of thousands of patches in search for the program mutation that best improves the desired software property. While genetic programming has been dominantly used as the search strategy, more recently other search strategies, such as local search, have been tried. It is, however, still unclear which strategy is the most effective and efficient. In this paper, we conduct an in-depth empirical comparison of a total of 18 search processes using a set of 8 improvement scenarios. Additionally, we also provide new genetic improvement benchmarks and we report on new software patches found. Our results show that, overall, local search approaches achieve better effectiveness and efficiency than genetic programming approaches. Moreover, improvements were found in all scenarios (between 15% and 68%). A replication package can be found online: https://github.com/bloa/tevc _2020 artefact."

- title: Enhancing Genetic Improvement of Software with Regression Test Selection
  url: https://ieeexplore.ieee.org/document/9401972/
  abstract: "Genetic improvement uses artificial intelligence to automatically improve software with respect to non-functional properties (AI for SE). In this paper, we propose the use of existing software engineering best practice to enhance Genetic Improvement (SE for AI). We conjecture that existing Regression Test Selection (RTS) techniques (which have been proven to be efficient and effective) can and should be used as a core component of the GI search process for maximising its effectiveness. To assess our idea, we have carried out a thorough empirical study assessing the use of both dynamic and static RTS techniques with GI to improve seven real-world software programs. The results of our empirical evaluation show that incorporation of RTS within GI significantly speeds up the whole GI process, making it up to 78% faster on our benchmark set, being still able to produce valid software improvements. Our findings are significant in that they can save hours to days of computational time, and can facilitate the uptake of GI in an industrial setting, by significantly reducing the time for the developer to receive feedback from such an automated technique. Therefore, we recommend the use of RTS in future test-based automated software improvement work. Finally, we hope this successful application of SE for AI will encourage other researchers to investigate further applications in this area."

- title: Efficient Compiler Autotuning via Bayesian Optimization
  url: https://ieeexplore.ieee.org/document/9401979/
  abstract: "A typical compiler such as GCC supports hundreds of optimizations controlled by compilation flags for improving the runtime performance of the compiled program. Due to the large number of compilation flags and the exponential number of flag combinations, it is impossible for compiler users to manually tune these optimization flags in order to achieve the required runtime performance of the compiled programs. Over the years, many compiler autotuning approaches have been proposed to automatically tune optimization flags, but they still suffer from the efficiency problem due to the huge search space. In this paper, we propose the first Bayesian optimization based approach, called BOCA, for efficient compiler autotuning. In BOCA, we leverage a tree-based model for approximating the objective function in order to make Bayesian optimization scalable to a large number of optimization flags. Moreover, we design a novel searching strategy to improve the efficiency of Bayesian optimization by incorporating the impact of each optimization flag measured by the tree-based model and a decay function to strike a balance between exploitation and exploration. We conduct extensive experiments to investigate the effectiveness of BOCA on two most popular C compilers (i.e., GCC and LLVM) and two widely-used C benchmarks (i.e., cBench and PolyBench). The results show that BOCA significantly outperforms the state-of-the-art compiler autotuning approaches and Bayesion optimization methods in terms of the time spent on achieving specified speedups, demonstrating the effectiveness of BOCA."

- title: "Code Transformation Impact on Compiler-based Optimization: A Case Study in the CMSSW"
  url: https://iopscience.iop.org/article/10.1088/1742-6596/1936/1/012023
  abstract: "In this paper, we study the benefit of applying loop transformations to a part of module in the CMS software. Particularly, we focus at the effect of loop transformations in term of performance improvement from the optimization process of compilers. Loop optimizations have been considered at low-level phase, such as loop unrolling using compiler directive. For high-level code transformations such as index set splitting and loop reordering, we adopt the polyhedral model to simplify the transformations. In this study, our loop optimization has been evaluated quantitatively. We study the impact on loops execution speed up and its instruction executed. Our observation shows that high-level loop optimizations can reduce both execution time and the number of instruction. This behavior suggested that simple loop transformations can trigger other optimizations. Moreover, we not only improve the overall performance, but also reduce the number of instruction. The results show that loop optimizations yield the speed up between 1.5 and 1.7."

- title: Inter-loop optimization in RAJA using loop chains
  url: https://dl.acm.org/doi/10.1145/3447818.3461665
  abstract: "Typical parallelization approaches such as OpenMP and CUDA provide constructs for parallelizing and blocking for data locality for individual loops. By focusing on each loop separately, these approaches fail to leverage sources of data locality possible due to inter-loop data reuse. The loop chain abstraction provides a framework for reasoning about and applying inter-loop optimizations. In this work, we incorporate the loop chain abstraction into RAJA, a performance portability layer for high-performance computing applications. Using the loop-chain-extended RAJA, or RAJALC, developers can have the RAJA library apply loop transformations like loop fusion and overlapped tiling while maintaining the original structure of their programs. By introducing targeted symbolic execution capabilities, we can collect and cache data access information required to verify loop transformations. We evaluate the performance improvement and refactoring costs of our extension. Overall, our results demonstrate 85-98% of the performance improvements of hand-optimized kernels with dramatically fewer code changes."

- title: "CodeMason: Binary-Level Profile-Guided Optimization"
  url: https://dl.acm.org/doi/10.1145/3338502.3359763
  abstract: "Optimizing a program for a specific machine or a specific workload is possible with today's compilers, but infrequently used, despite significant performance gains. We implement workload specialization, or Profile-Guided Optimization (PGO), at the binary level. Our system CodeMason runs on x86_64 Linux and is based on a binary rewriting platform called Egalito. CodeMason performs static binary rewriting to obtain program profiles, then adjusts function ordering, alignment, and other binary-level details to achieve faster performance (particularly on the given workload). We obtain 1.98% average performance speedup on SPEC CPU 2006, and 11.8% speedup in the best case. These substantial performance improvements suggest that binary-level PGO may be widely useful when compiler-based PGO is impossible because the source code is inaccessible."

- title: "Does the Introduction of Lambda Expressions Improve the Comprehension of Java Programs?"
  url: https://dl.acm.org/doi/10.1145/3350768.3350791
  abstract: "Background: The Java programming language version eighth introduced a number of features that encourage the functional style of programming, including the support for lambda expressions and the Stream API. Currently, there is a common wisdom that refactoring a legacy code to introduce lambda expressions, besides other potential benefits, simplifies the code and improves program comprehension. Aims: The purpose of this paper is to investigate this belief, conducting an in depth study to evaluate the effect of introducing lambda expressions on program comprehension. Method: We conduct this research using a mixed-method study. First, we quantitatively analyze 66 pairs of real code snippets, where each pair corresponds to the body of a method before and after the introduction of lambda expressions. We computed two metrics related to source code complexity (number of lines of code and cyclomatic complexity) and two metrics that estimate the readability of the source code. Second, we conduct a survey with practitioners to collect their perceptions about the benefits on program comprehension, with the introduction of lambda expressions. The practitioners evaluate a number between three and six pairs of code snippets, to answer questions about possible improvements. Results: We found contradictory results in our research. Based on the quantitative assessment, we could not find evidences that the introduction of lambda expressions improves software readability--one of the components of program comprehension. Differently, our findings of the qualitative assessment suggest that the introduction of lambda expression improves program comprehension. Implications: We argue in this paper that one can improve program comprehension when she applies particular transformations to introduce lambda expressions (e.g., replacing anonymous inner classes by lambda expressions). In addition, the opinion of the participants shine the opportunities in which a transformation for introducing lambda might be advantageous. This might support the implementation of effective tools for automatic program transformations. Finally, our results suggest that state-of-the-art models for estimating program readability are not helpful to capture the benefits of a program transformation to introduce lambda expressions."

- title: "Slimming javascript applications: An approach for removing unused functions from javascript libraries"
  url: https://www.sciencedirect.com/science/article/pii/S0950584918302210
  abstract: "Context: A common practice in JavaScript development is to ship and deploy an application as a large file, called bundle, which is the result of combining the application code along with the code of all the libraries the application depends on. Despite the benefits of having a single bundle per application, this approach leads to applications being shipped with significant portions of code that are actually not used, which unnecessarily inflates the JavaScript bundles and could slow down website loading because of the extra unused code. Although some static analysis techniques exist for removing unused code, our investigations suggest that there is still room for improvements. Objective: The goal of this paper is to address the problem of reducing the size of bundle files in JavaScript applications. Method: In this context, we define the notion of Unused Foreign Function (UFF) to denote a JavaScript function contained in dependent libraries that is not needed at runtime. Furthermore, we propose an approach based on dynamic analysis that assists developers to identify and remove UFFs from JavaScript bundles. Results: We report on a case-study performed over 22 JavaScript applications, showing evidence that our approach can produce size reductions of 26% on average (with reductions going up to 66% in some applications). Conclusion: It is concluded that removing unused foreign functions from JavaScript bundles helps reduce their size, and thus, it can boost the results of existing static analysis techniques."

- title: "Subdomain-based generality-aware debloating"
  url: https://dl.acm.org/doi/10.1145/3324884.3416644
  abstract: "Programs are becoming increasingly complex and typically contain an abundance of unneeded features, which can degrade the performance and security of the software. Recently, we have witnessed a surge of debloating techniques that aim to create a reduced version of a program by eliminating the unneeded features therein. To debloat a program, most existing techniques require a usage profile of the program, typically provided as a set of inputs I. Unfortunately, these techniques tend to generate a reduced program that is over-fitted to I and thus fails to behave correctly for other inputs. To address this limitation, we propose DomGad, which has two main advantages over existing debloating approaches. First, it produces a reduced program that is guaranteed to work for subdomains, rather than for specific inputs. Second, it uses stochastic optimization to generate reduced programs that achieve a close-to-optimal tradeoff between reduction and generality (i.e., the extent to which the reduced program is able to correctly handle inputs in its whole domain). To assess the effectiveness of DomGad, we applied our approach to a benchmark of ten Unix utility programs. Our results are promising, as they show that DomGad could produce debloated programs that achieve, on average, 50% code reduction and 95% generality. Our results also show that DomGad performs well when compared with two state-of-the-art debloating approaches."

- title: "Nibbler: Debloating binary shared libraries"
  url: https://dl.acm.org/doi/10.1145/3359789.3359823
  abstract: "Developers today have access to an arsenal of toolkits and libraries for rapid application prototyping. However, when an application loads a library, the entirety of that library's code is mapped into the address space, even if only a single function is actually needed. The unused portion is bloat that can negatively impact software defenses by unnecessarily inflating their overhead or increasing their attack surface. Recent work has explored debloating as a way of alleviating the above problems, when source code is available. In this paper, we investigate whether debloating is possible and practical at the binary level. To this end, we present Nibbler: a system that identifies and erases unused functions within shared libraries. Nibbler works in tandem with defenses like continuous code re-randomization and control-flow integrity, enhancing them without incurring additional run-time overhead. We developed and tested a prototype of Nibbler on x86-64 Linux; Nibbler reduces the size of shared libraries and the number of available functions, for real-world binaries and the SPEC CINT2006 suite, by up to 56% and 82%, respectively. We also demonstrate that Nibbler benefits defenses by showing that: (i) it improves the deployability of a continuous re-randomization system for binaries, namely Shuffler, by increasing its efficiency by 20%, and (ii) it improves certain fast, but coarse and context-insensitive control-flow integrity schemes by reducing the number of gadgets reachable through returns and indirect calls by 75% and 49% on average."

- title: "Bloat Factors and Binary Specialization"
  url: https://dl.acm.org/doi/10.1145/3338502.3359765
  abstract: "Code bloating in software has been proven to be pervasive in recent research. However, each study provides a different approach to measure bloat. In this paper, we propose a system of metrics to effectively quantify bloat in binaries called bloat factors. Subsequently, we conducted an extensive study to calculate bloat factors for over 3000 Linux applications and 896 shared libraries. Using these metrics as pointers, we introduce a static approach to perform debloating for closed-source binaries by creating corresponding specialized versions to cater for a specific program requirements. We evaluated our debloating technique on large programs and achieved a maximum code reduction of 19.7%."

- title: "Applying genetic improvement to a genetic programming library in C++"
  url: https://link.springer.com/article/10.1007%2Fs00500-018-03705-6
  abstract: "A young subfield of evolutionary computing that has gained the attention of many researchers in recent years is genetic improvement. It uses an automated search method that directly modifies the source code or binaries of a software system to find improved versions based on some given criteria. Genetic improvement has achieved notable results and the acceptance of several research communities, namely software engineering and evolutionary computation. Over the past 10 years there have been core publications on the subject; however, we have identified, to the best of our knowledge, that there is no work on applying genetic improvement to a meta-heuristic system. In this work we apply the GI framework called GISMO to the Beagle Puppy library version 0.1 in C++, a genetic programming system configured to perform symbolic regression on several benchmark and real-world problems. The objective is to improve the processing time while maintaining a similar or better test fitness of the best individual produced by the unmodified genetic programming search. Results show that GISMO can generate individuals that present an improvement on those two key aspects over some problems, while also reducing the effects of bloat, one of the main issues in genetic programming."

- title: A fuzzy genetic automatic refactoring approach to improve software maintainability and flexibility
  url: https://link.springer.com/article/10.1007%2Fs00500-020-05443-0
  abstract: "The creation of high-quality software is of great importance in the current state of the enterprise systems. High-quality software should contain certain features including flexibility, maintainability, and a well-designed structure. Correctly adhering to the object-oriented principles is a primary approach to make the code more flexible. Developers usually try to leverage these principles, but many times neglecting them due to the lack of time and the extra costs involved. Therefore, sometimes they create confusing, complex, and problematic structures in code known as code smells. Code smells have specific and well-known anti-patterns that can be corrected after their identification with the help of the refactoring techniques. This process can be performed either manually by the developers or automatically. In this paper, an automated method for identifying and refactoring a series of code smells in the Java programs is introduced. The primary mechanism used for performing such automated refactoring is by leveraging a fuzzy genetic method. Besides, a graph model is used as the core representation scheme along with the corresponding measures such as betweenness, load, in-degree, out-degree, and closeness centrality, to identify the code smells in the programs. Then, the applied fuzzy approach is combined with the genetic algorithm to refactor the code using the graph-related features. The proposed method is evaluated using the Freemind, Jag, JGraph, and JUnit as sample projects and compared the results against the Fontana dataset which contains results from IPlasma, FluidTool, Anti-patternScanner, PMD, and Maeinescu. It is shown that the proposed approach can identify on average 68.92% of the bad classes similar to the Fontana dataset and also refactor 77% of the classes correctly with respect to the coupling measures. This is a noteworthy result among the currently existing refactoring mechanisms and also among the studies that consider both the identification and the refactoring of the bad smells."

- title: "ParamILS: An Automatic Algorithm Configuration Framework"
  url: https://jair.org/index.php/jair/article/view/10628
  abstract: "The identification of performance-optimizing parameter settings is an important part of the development and application of algorithms. We describe an automatic framework for this algorithm configuration problem. More formally, we provide methods for optimizing a target algorithms performance on a given class of problem instances by varying a set of ordinal and/or categorical parameters. We review a family of local-search-based algorithm configuration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations. We describe the results of a comprehensive experimental evaluation of our methods, based on the configuration of prominent complete and incomplete algorithms for SAT. We also present what is, to our knowledge, the first published work on automatically configuring the CPLEX mixed integer programming solver. All the algorithms we considered had default parameter settings that were manually identified with considerable effort. Nevertheless, using our automated algorithm configuration procedures, we achieved substantial and consistent performance improvements."

- title: "Sequential Model-Based Optimization for General Algorithm Configuration"
  url: https://link.springer.com/chapter/10.1007%2F978-3-642-25566-3_40
  abstract: "State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach."

- title: "Tuning applications for efficient GPU offloading to in-memory processing"
  url: https://dl.acm.org/doi/10.1145/3392717.3392760
  abstract: "Data movement between processors and main memory is a critical bottleneck for data-intensive applications. This problem is more severe with Graphics Processing Units (GPUs) applications due to their massive parallel data processing characteristics. Recent research has shown that in-memory processing can greatly alleviate this data movement bottleneck by reducing traffic between GPUs and memory devices. It offloads execution to in-memory processors, and avoids transferring enormous data between memory devices and processors. However, while in-memory processing is promising, to fully take advantage of such architecture, we need to solve several issues. For example, the conventional GPU application code that is highly optimized for the locality to execute efficiently in GPU does not necessarily have good locality for in-memory processing. As such, the GPU may mistakenly offload application routines that cannot gain benefit from in-memory processing. Additionally, workload balancing cannot simply treat in-memory processors as GPU processors since its data transfer time can be significantly reduced. Finally, how to offload application routines that access the shared memory inside GPUs is still an unsolved issue.
In this paper, we explore four optimizations for GPU applications to take advantage of in-memory processors. Specifically, we propose four optimizations: application restructuring, run-time adaptation, aggressive loop offloading, and shared-memory transfer on-demand to mitigate the four unsolved issues in the GPU in-memory processing system. From our experimental evaluations with 13 applications, our approach can achieve 2.23x offloading performance improvement."

- title: "Grover: Looking for Performance Improvement by Disabling Local Memory Usage in OpenCL Kernels"
  url: https://ieeexplore.ieee.org/document/6957225
  abstract: "Due to the diversity of processor architectures and application memory access patterns, the performance impact of using local memory in OpenCL kernels has become unpredictable. For example, enabling the use of local memory for an OpenCL kernel can be beneficial for the execution on a GPU, but can lead to performance losses when running on a CPU. To address this unpredictability, we propose an empirical approach: by disabling the use of local memory in OpenCL kernels, we enable users to compare the kernel versions with and without local memory, and further choose the best performing version for a given platform. To this end, we have designed Grover, a method to automatically remove local memory usage from OpenCL kernels. In particular, we create a correspondence between the global and local memory spaces, which is used to replace local memory accesses by global memory accesses. We have implemented this scheme in the LLVM framework as a compiling pass, which automatically transforms an OpenCL kernel with local memory to a version without it. We have validated Grover with 11 applications, and found that it can successfully disable local memory usage for all of them. We have compared the kernels with and without local memory on three different processors, and found performance improvements for more than a third of the test cases after Grover disabled local memory usage. We conclude that such a compiler pass can be beneficial for performance, and, because it is fully automated, it can be used as an auto-tuning step for OpenCL kernels."

- title: "Effects of Refactoring upon Efficiency of an NP-Hard Task Assignment Problem: A case study"
  url: https://ieeexplore.ieee.org/document/9055956
  abstract: "The goal of this paper is to analyze the effects of refactoring on time complexity of an algorithm. For this purpose a problem in which time complexity is highly sensitive, is chosen for studying. As it is known by computer scientists, they use refactoring in order to improve quality of design while preserving external behavior (functional properties). Sustainability of nonfunctional properties are not guaranteed. Hence, for learning its effects on non-functional properties such as time, a multiobjective task assignment problem is selected. The chosen problem has been implemented through an Evolutionary Genetic Algorithm. The problem chosen is an NP -hard problem because of being time sensitive. Initially, code smells are detected & refactoring is applied. In order to observe the improvement in design of code, several metrics of quality such as cohesion, coupling, complexity & inheritance, are calculated and compared before & after applying refactoring. Also, computation time of the improved code is compared with the original code, in order to analyze effects of refactoring on computation time. For problems that are time sensitive, refactoring may not be a good choice depending upon the requirements. Results of the experimentation nullify the approach that refactoring improves the computational cost of the software. Increase in the length of code eventually may prove as a tradeoff in terms of memory consumption."

- title: "Automated selection of software refactorings that improve performance"
  url: https://www.scitepress.org/PublicationsDetail.aspx?ID=Xi8Fg71oxAc=
  abstract: "Performance is a critical property of a program. While there exist refactorings that have the potential to significantly increase the performance of a program, it is hard to decide which refactorings effectively yield improvements. In this paper, we present a novel approach for the automated detection and selection of refactorings that are promising candidates to improve performance. Our key idea is to provide a heuristics that utilizes software properties determined by both static code analyses and dynamic software analyses to compile a list of concrete refactorings sorted by their assessed potential to improve performance. The expected performance improvement of a concrete refactoring depends on two factors: the execution frequency of the respective piece of code, and the effectiveness of the refactoring itself. To assess the latter, namely the general effectiveness of a given set of refactorings, we have implemented a set of micro benchmarks and measured the effect of each refactor ing on computation time and memory consumption. We demonstrate the practical applicability of our overall approach with experimental results."

- title: "Tuning floating-point precision using dynamic program information and temporal locality"
  url: https://ieeexplore.ieee.org/document/9355251
  abstract: "We present a methodology for precision tuning of full applications. These techniques must select a search space composed of either variables or instructions and provide a scalable search strategy. In full application settings one cannot assume compiler support for practical reasons. Thus, an additional important challenge is enabling code refactoring. We argue for an instruction-based search space and we show: 1) how to exploit dynamic program information based on call stacks; and 2) how to exploit the iterative nature of scientific codes, combined with temporal locality. We applied the methodology to tune the implementation of scientific codes written in a combination of Python, CUDA, C++ and Fortran, tuning calls to math exp library functions. The iterative search refinement always reduces the search complexity and the number of steps to solution. Dynamic program information increases search efficacy. Using this approach, we obtain application runtime performance improvements up to 27%."

- title: "Refactoring the FreeBSD Kernel with Checked C"
  url: https://ieeexplore.ieee.org/document/9229980
  abstract: "Most modern operating system kernels are written in C, making them vulnerable to buffer overflow and buffer over-read attacks. Microsoft has developed an extension to the C language named Checked C which provides new source language constructs that allow the compiler to prevent NULL pointer dereferences and spatial memory safety errors through static analysis and run-time check insertion. We evaluate the use of Checked C on operating system kernel code by refactoring parts of the FreeBSD kernel to use Checked C extensions. We describe our experience refactoring the code that implements system calls and UDP and IP networking. We then evaluate the refactoring effort and the performance of the refactored kernel. It took two undergraduate students approximately three months to refactor the system calls, the network packet (mbuf) utility routines, and parts of the IP and UDP processing code. Our experiments show that using Checked C incurred no performance or code size overheads."
